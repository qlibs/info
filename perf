// <!--
//
// Copyright (c) 2024-2025 Kris Jusiak <kris@jusiak.net>
//
// | license        | namespace      | guard            |
// |----------------|----------------|------------------|
// | MIT*           | `perf::*`      | -                |
// | Apache2:LLVM** | `perf::mca::*` | `PERF_LLVM == 1` |
//
// The MIT License (MIT)*
// Permission is hereby granted, free of charge, to any person obtaining a copy
// of this software and associated documentation files (the "Software"), to deal
// in the Software without restriction, including without limitation the rights
// to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
// copies of the Software, and to permit persons to whom the Software is
// furnished to do so, subject to the following conditions:
//
// The above copyright notice and this permission notice shall be included in all
// copies or substantial portions of the Software.
//
// THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
// IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
// FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
// AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
// LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
// OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
// SOFTWARE.
// See https://opensource.org/license/mit for license information.
//
// Apache License v2.0 with LLVM exceptions**
// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.
// See https://llvm.org/LICENSE.txt for license information.
// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception
//
#if 0
// -->
[Overview](#Overview) / [Examples](#Examples) / [API](#API) / [FAQ](#FAQ) / [Resources](#Resources)

> [!WARNING]
> `WORK IN PROGRESS`

## PERF: C++23 Performance library

[![License](https://img.shields.io/badge/license-blue.svg)](LICENSE)
[![Version](https://img.shields.io/github/v/release/qlibs/perf)](https://github.com/qlibs/perf/releases)
[![Build](https://github.com/qlibs/perf/actions/workflows/linux.yml/badge.svg)](https://github.com/qlibs/perf/actions/workflows/linux.yml)
[![Try it online](https://img.shields.io/badge/try%20it-online-blue.svg)](https://godbolt.org/z/MKxTbj87z)

> [`Performance`](https://en.wikipedia.org/wiki/Computer_performance) is not a number!

[](#Overview)

### Use cases

  > Benchmarking, Profiling, Analysis

### Features

  > <details>
  >   <summary>Supports <strong>timing</strong>, <strong>profiling</strong>, <strong>analyzing</strong>, <strong>plotting</strong>, <strong>testing</strong></summary>
  >
  >   - info - cxx, cpu, memory, sys, bin, proc - see info (linux)
  >   - time - steady_clock, tsc (x86_64), cpu, thread, real, monotonic (linux)
  >   - stat - performance counters
  >     - includeing top-down method analysis - see top_down
  >   - record - sampling performance counters
  >   - trace - processor trace (intel_pt)
  >   - mc - disassemble (latency, uops, encoding, ...) (llvm)
  >     - Supports labeling code for assembly
  >       - disassemling sections, functions - see mc
  >   - mca - timeline, resource_pressure, bottleneck (llvm.mca)
  >   - external
  >     - linux-perf
  >     - llvm-xray
  >     - callgrind
  >     - gperftools
  >     - intel-vtune
  >   - plot - hist, bar, box, line, ecdf (gnuplot/sixel) - see plot
  >   - testing (compare assembly, results) - see testing
  >     - https://github.com/qlibs/ut for unit testing - see ut
  >       - Tests selection and reporting
  >       - Results verfification
  >       - Optimized assembly comparison
  >
  > </details>

  > <details>
  >   <summary>Strives for <strong>accuracy</strong>, <strong>flexibility</strong> and <strong>simplicity</strong></summary>
  >
  >   - Single header - https://github.com/qlibs/perf/blob/main/perf
  >   - C++20 module - https://github.com/qlibs/perf/blob/main/perf.cppm
  >
  >   - Any part can be used in isolation - see standalone
  >   - Integration - see #FAQ
  >
  >   - Realsitic scenarios
  >     - Uses generators for input data distribution - see branch_prediction
  >   - Single instruction cycle accurate - see how_it_works
  >     - [uefi](https://gist.github.com/qlibs-dev/f03502d97cdc857832052ef772184a68)
  >       - Access to more hw features (disabling/enabling cache)
  >       - No os noise
  >   - Can measure latency and/or throughput (including multi-threading) - see bench
  >   - Statistical based benchmarking (z-score, t-score) - see how_it_works
  >   - Uses statistics for reports (median, mean, percentile, coefficient_of_varation, ...) - see report
  >   - Focuses on analyzing and profiling (performance is not a number!)
  >   - What, How to benchmark and how to show - see custom_bench
  >   - Custom metrics (ipc, l1_icache_miss_rate, ...) - see metric
  >   - Benchmarking results/data is just a std::vector, current views just iterate over it
  >
  >     ```cpp
  >     runner bench{what and how...}; // latency<cold>, throughput<warm>, throughput<cold>{.threads = 16} ...
  >
  >     bench(lambda or function, parameters...); // similar to std::invoke
  >
  >     report(bench[metrics...], stats...); // per benchmark
  >     annotate(bench[metrics...]); // perf instruction
  >     plot(bench[metrics...]) // hist, bar, box, line, ecdf
  >     ```
  >
  >   - Deduces what to bench based on usage (type safe)
  >     ```cpp
  >     report(bench[stat::cycles, time::cpu]);
  >     annotate(bench[record::cycles]);
  >     plot(bench[stat::ipc]);
  >     ```
  >     - `time::cpu, stat::instructions (ipc), stat::cycles` will be benchmarked
  >
  > </details>

  > <details>
  >   <summary>Enables <strong>dev</strong>, <strong>reaserch</strong> workflows</summary>
  >
  >   - dev workflow
  >     - edit code in the editor compile and run as usual (plot and analyze from c++)
  >
  >     - pros
  >       - easy editing and integration with exising tooling
  >       - output to the console (charts with sixel)
  >       - easy to share via gh gist create # github can render markdown
  >       - can assert and verify expectation
  >       - can be executed in headless mode on the server without ui
  >
  >     - cons
  >      - harder analysis than in python
  >      - single compiler workflow
  >
  >   - research workflow
  >     - run from jupter notebook (produce json) analyze using python - see notebook
  >
  >     - pros
  >       - infite way of data analysis with great matpotlib suppport
  >       - can run different compilers/options
  >       - easy to share via gh gist create # github can render ipynb
  >       - can be used on kaggle (avx512) or google colab
  >
  >     - cons
  >       - editing C++ is not that well supported by notebooks but runnig is fine
  >       - not that great to analyze assembly output
  >       - usually would require running through a browser (there are extensions for vim/emacs/vscode)
  >       - might be slow with a lot of data
  >
  >   - mix workflow
  >     - dev workflow for basic info and research workflow for further analysis, perfetto for flame graphs
  >
  > </details>

  > <details>
  >   <summary>Performs <strong>self-verification</strong> upon usage</summary>
  >
  >   - Runs `compile-time`  tests upon include/import (enabled by default)
  >     - if used with C++20 modules (recommended) compile-time tests will be compiled/executed once during module compilation
  >
  >   - `Sanity check` tests can be executed via
  >     ```cpp
  >     perf::test::run({.verbose = true});
  >     ```
  >
  >   - Tests can be removed by defining `-DNTEST` (not recommended)
  >
  > </details>

### Setup

  > <details open>
  >   <summary>Try it!</summary>
  >
  >   ```sh
  >   docker build -t perf .
  >   docker run --rm --privileged -v ${PWD}:${PWD} -w ${PWD} \
  >     docker clang++-20 -std=c++23 -O3 --precompile perf.cppm
  >   gh gist create -w hello_world.cpp *.md *.svg
  >   ```
  >
  > </details>

  > <details>
  >   <summary>Build & Test</summary>
  >
  >   - Requirements ([docker](https://github.com/qlibs/perf/blob/main/.github/workflows/Dockerfile))
  >
  >     <details open>
  >       <summary>Minimal</summary>
  >
  >       - Clang-16+, GCC-12+ / C++23+
  >
  >     </details>
  >
  >     <details open>
  >       <summary>Optimal (recommended)</summary>
  >
  >       - Clang-19+, GCC-13+ / C++23+
  >         - [`llvm-dev-19+`](https://llvm.org) - `apt-get install llvm-dev`
  >       - Intel-12th+ with PEBS, Intel Processor Trace support
  >         - [`intel_pt`](https://github.com/intel/libipt) - `apt-get install libipt-dev`
  >       - Linux-6.x+
  >         - [`perf_event_open`](https://perf.wiki.kernel.org) - `apt-get install linux-tools-generic`
  >       - Terminal with [Sixel](https://www.arewesixelyet.com) support
  >         - [`gnuplot`](http://www.gnuplot.info) - `apt-get install gnuplot`
  >
  >     </details>
  >
  >     <details open>
  >       <summary>Complementary</summary>
  >
  >       - Sharing results via [https://gist.github.com](https://gist.github.com)
  >         - [gh](https://cli.github.com) - `apt-get install gh`
  >
  >       - Data analysis (research workflow)
  >         - [jupyter notebook](https://jupyter.org) - `apt-get install jupyter`
  >
  >       - Tooling (`prof::tool`)
  >         - [linux-perf](https://perf.wiki.kernel.org) - `apt get install linux-tools-generic`
  >         - [llvm-xray](https://llvm.org/docs/XRay.html) - `apt-get install llvm`
  >         - [gperftools](https://github.com/gperftools/gperftools) - `apt get install google-perftools`
  >         - [intel-vtune](https://www.intel.com/content/www/us/en/docs/vtune-profiler) - `apt get install intel-oneapi-vtune`
  >         - [callgrind](https://valgrind.org/docs/manual/cl-manual.html) - `apt-get install valgrind`
  >
  >     </details>
  >
  >   - Locally (no docker)
  >
  >     - `build dependencies` (optional)
  >       ```sh
  >       apt-get install linux-tools-generic
  >       apt-get install llvm-dev
  >       apt-get install libipt-dev
  >       apt-get install gnuplot
  >       pip3 install pyperf
  >       ```
  >
  >     - `enable linux tracing` (optional)
  >       ```sh
  >       sudo mount -o remount,mode=755 /sys/kernel/debug
  >       sudo mount -o remount,mode=755 /sys/kernel/debug/tracing
  >       sudo chown `whoami` /sys/kernel/debug/tracing/uprobe_events
  >       sudo chmod a+rw /sys/kernel/debug/tracing/uprobe_events
  >       echo 0 | sudo tee /proc/sys/kernel/kptr_restrict
  >       echo -1 | sudo tee /proc/sys/kernel/perf_event_paranoid
  >       ```
  >
  >     - `tune machine for benchmarking` (recommended) # see #FAQ fore more noise reduction details)
  >       ```sh
  >       pyperf system tune
  >       ```
  >
  >     - `include/import` perf
  >       ```sh
  >       cat <<EOF > perf.cpp
  >         import perf;
  >         int main() { }
  >       EOF
  >       ```
  >
  >     - `build` # perf tests are running at compile-time upon inclusion/import and run-time on start-up
  >       ```cpp
  >       clang++-20 \
  >         -std=c++23
  >         -O3 \
  >         -I. \
  >         -I/usr/lib/llvm-20/include \
  >         -lLLVM-20 \
  >         -lipt \
  >         -o perf \
  >         perf.cpp && \
  >       ./perf # runs run-time tests and sanity checks
  >       ```
  >
  >  - Via `docker` (https://github.com/qlibs/perf/blob/main/.github/workflows/Dockerfile)
  >
  >    - `build` perf image
  >       ```sh
  >       wget https://raw.githubusercontent.com/qlibs/perf/refs/heads/main/.github/workflows/Dockerfile
  >       docker build -t perf .
  >       ```
  >
  >     - `include/import` perf
  >       ```sh
  >       cat <<EOF > perf.cpp
  >         import perf;
  >         int main() { }
  >       EOF
  >       ```
  >
  >    - `build` # perf tests are running at compile-time upon inclusion/import and run-time on start-up
  >       ```sh
  >       docker run \
  >         --rm \
  >         --privileged \
  >         -v ${PWD}:${PWD} \
  >         -w ${PWD} \
  >         perf \
  >         clang++-20 \
  >           -std=c++23
  >           -O3 \
  >           -I. \
  >           -I/usr/lib/llvm-20/include \
  >           -lLLVM-20 \
  >           -lipt \
  >           -o perf \
  >           perf.cpp && \
  >         ./perf
  >       ```
  >
  > </details>

### Examples

  > `Benchmarking`
  > - [`hello world`](https://gist.github.com/qlibs-dev/8705ca87416855324b5a09b23425b3ce) <br/>
  > - [`latency` vs `throughput`](https://gist.github.com/qlibs-dev/1ff0f4f5c22e61d0c3a9208002336f71) <br/>
  > - [`report` / `annotate` / `plot`]() <br/>
  > - [`debug` & `test`]() <br/>
  > - [`jupyter notebook`](https:://link_to_kaggle_notebook_with_avx512) <br/>

  > `Profiling` / `Analysis`
  > - [`info` /  `core`](https://godbolt.org/z/Ya1addsr6) <br/>
  > - [`disassemble` vs `trace`]() <br/>
  > - [`profile` vs `analyze`]() <br/>

  > `Studies`
  > - [`max_ipc`]()

### API

  > [`Synopsis`](https://github.com/qlibs/perf/blob/main/perf.cppm#L39)

### FAQ

  > <details>
  >   <summary>Perf[ormance]</summary>
  >
  >   - How does it work?
  >     - unroll - show run
  >     - views - gnuplot via sixel
  >     - function - invoke
  >     - sampler, counter - perf_event_open
  >     - mca - llvm-dev.mca
  >
  >   - How to configure?
  >     ```cpp
  >     // version # https://semver.org
  >     #define PERF (MAJOR, MINOR, PATCH) // ex. (1, 0, 0)
  >
  >     // {1: gnu compatible compiler, 0: otherwise}
  >     // default: deduced based on `__GNUC__`
  >     #define PERF_GNU 0/1
  >
  >     // {1: linux based os, 0: otherwise}
  >     // default: deduced based on available headers
  >     #define PERF_LINUX 0/1
  >
  >     // {1: llvm-dev supported, 0: otherwise}
  >     // default: deduced based on available headers
  >     #define PERF_LLVM 0/1
  >
  >     // {1: intel_pt supported, 0: othrewise}
  >     //  default: deduced based on whether `<intel_pt.h>` is available
  >     #define PERF_INTEL 0/1
  >
  >     // {defined: disables all compile-time, run-time tests, 0, 1, 2}
  >     // default: not defined # tests are running by default
  >     #define NTEST 0/1/2
  >     ```
  >
  >   - Enviornment variables
  >     ```sh
  >     // gnuplot terminal # see `gnuplot -> set terminal`
  >     // default: 'sixel'
  >     // - 'sixel'                  # console image # https://www.arewesixelyet.com
  >     // - 'wxt'                    # popup window
  >     // - 'dumb size 150,25 ansi'  # console with colors
  >     // - 'dumb size 80,25'        # console
  >     ENV:PERF_PLOT_TERM
  >
  >     // { 'light', 'dark' }
  >     // default: 'dark'
  >     ENV:PERF_PLOT_STYLE
  >     ```
  >
  >   - How to share results?
  >
  >     - [export.sh](https://gist.github.com/qlibs-dev/b29ceeb7d2fb94547b593d24f6e89848)
  >     - [gist.sh](https://gist.github.com/qlibs-dev/80b43c6c3c4872951c73503d87bfa355)
  >
  >     - via `https://gist.github.com`
  >
  >       ```
  >       apt-get install gh
  >       ```
  >
  >       ```cpp
  >       gh gist create perf.html --public | awk '{print "https://htmlpreview.github.io/?perf.html/raw"}'
  >       ```
  >
  >   - What is `prevent_elision` and when it`s needed?
  >     - optimizing compiler may elide your code completely if it`s not required (doesn`t have side effects).
  >     `prevent_elision` will prevent that.
  >
  >       ```cpp
  >       verify(perf::compiler::is_elided([] { }));
  >       verify(perf::compiler::is_elided([] { auto value = 4 + 2; }));
  >       verify(perf::compiler::is_elided([] { int i{}; i++; }));
  >       verify(not perf::compiler::is_elided([&] { i++; }));
  >       verify(not perf::compiler::is_elided([] { static int i; i++; }));
  >       verify(not perf::compiler::is_elided([=] {
  >         int i{};
  >         perf::compiler::prevent_elision(i++);
  >       }));
  >       ```
  >
  >   - How to change assembly syntax from intel to at&t?
  >     ```cpp
  >     perf::llvm llvm{.syntax = perf::arch::syntax::att };
  >     ```
  >
  >     ```cpp
  >     perf::bench::runner bench{
  >       {.syntax = perf::arch::syntax::att},
  >       perf::bench::latency,
  >     };
  >     ```
  >
  >   - How to disassemble for different platform?
  >     ```cpp
  >     perf::llvm llvm{.triple = "x86_64-pc-linux-gnu" }; // see `llvm-llc` for details
  >     ```
  >
  >   - How to integration with testing framework?
  >     - perf can be intergrated with any unit testing framework - https://github.com/qlibs/ut
  >       ```cpp
  >       import perf;
  >       import ut;
  >
  >       int main() {
  >         "benchmark"_test = [] {
  >           // ...
  >         };
  >       }
  >       ```
  >
  >   - How to use `perf` as a C++20 module?
  >     ```sh
  >     clang++-20 -std=c++23 -O3 -I. --precompile perf.cppm
  >     clang++-20 -std=c++23 -O3 -fprebuilt-module-path=. perf.pcm perf.cpp -lLLVM-18 -lipt
  >     ```
  >
  >   - What is required to display images on the terminal?
  >     - terminal with sixel support - https://www.arewesixelyet.com
  >     note: Visual Studio Code supports images on terminal (requires enabling Terminal -> Enable images option)
  >
  >   - How to plot on the server without sixel?
  >     ```PERF_TERM=ascii ./a.out``` # see gnuplot - set terminal
  >
  >   - How to write gnuplot charts?
  >     `sudo apt install gnuplot`
  >     - gnuplot documentation - http://www.gnuplot.info/documentation.html
  >     - gnuplot demos - http://www.gnuplot.info/demo
  >     - online gnuplot - https://gnuplot.io
  >
  >   - how to write custom profiler?
  >     ```cpp
  >     struct my_profiler {
  >       constexpr auto start() { }
  >       constexpr auto stop() { }
  >       [[nodiscard]] constexpr auto *operator() { }
  >     };
  >     ```
  >
  >     ```cpp
  >     static_assert(perf::prof_like<your_profiler>);
  >     ```
  >
  >   - How to pollute cache, heap when benchmarking?
  >     - `perf::memory::pollute_heap`
  >     - `perf::memory::flush_cacheline`
  >
  >   - What are similar projects?
  >     - google-benchmark - https://github.com/google/benchmark
  >     - nanobench - https://github.com/martinus/nanobench
  >     - celero - https://github.com/DigitalInBlue/Celero
  >     - nanobench - https://github.com/andreas-abel/nanoBench
  >     - uarch-bench - https://github.com/travisdowns/uarch-bench
  >     - llvm-exegesis - https://llvm.org/docs/CommandGuide/llvm-exegesis.html
  >
  >   - How `perf` compares to `google.benchmark`, `nanobench`, `celero`?
  >     - Firstly, google.benchmark, nanobench, celero are great and established libraries.
  >
  >     - `perf` philosophy is more about the fact that performance is not a number which leads to the following
  >       - data driven inputs (to avoid branch prediction overfitting)
  >       - statistical, data driven inputs
  >       - instruction level and function level
  >       - statistical, don`t avoid branch prediction
  >       - latency and throughput
  >       - analysis, profiling and plotting
  >
  >   - Setup [docker](https://www.docker.com) ([Dockerfile](https://github.com/qlibs/prof/blob/main/.github/workflows/Dockerfile))
  >
  >     ```sh
  >     docker build -t perf .
  >     ```
  >
  >     ```sh
  >     docker run \
  >       -it \
  >       --privileged \
  >       --network=host \
  >       -e DISPLAY=${DISPLAY} \
  >       -v ${PWD}:${PWD} \
  >       -w ${PWD} \
  >       perf
  >     ```
  >
  >   - Setup [linux-perf](https://perf.wiki.kernel.org)
  >
  >     ```sh
  >     sudo mount -o remount,mode=755 /sys/kernel/debug
  >     sudo mount -o remount,mode=755 /sys/kernel/debug/tracing
  >     sudo chown `whoami` /sys/kernel/debug/tracing/uprobe_events
  >     sudo chmod a+rw /sys/kernel/debug/tracing/uprobe_events
  >     echo 0 | sudo tee /proc/sys/kernel/kptr_restrict
  >     echo -1 | sudo tee /proc/sys/kernel/perf_event_paranoid
  >     echo 1000 | sudo tee /proc/sys/kernel/perf_event_max_sample_rate
  >     ```
  >
  >   - Instrumentation with [llvm-xray](https://llvm.org/docs/XRay.html)
  >
  >     ```cpp
  >     [[clang::xray_always_instrument]]
  >     void always_profile();
  >
  >     [[clang::xray_always_instrument, clang::xray_log_args(1)]]
  >     void always_profile_and_log_i(int i);
  >
  >     [[clang::xray_never_instrument]]
  >     void never_profile();
  >     ```
  >
  >     ```cpp
  >     # profiling threshold
  >     -fxray-instruction-threshold=1 # default 200 instructions
  >     ```
  >
  >     ```cpp
  >     # instrumentation info
  >     llvm-xray extract ./a.out --symbolize
  >     ```
  >
  >     > https://godbolt.org/z/WhsEYf9cc
  >
  >   - Conditional profiling with [callgrind](https://valgrind.org/docs/manual/cl-manual.html)
  >
  >     ```cpp
  >     prof::callgrind profiler{"example"};
  >
  >     while (true) {
  >       profiler.start(); // resets profile
  >
  >       if (should_trigger()) {
  >         trigger();
  >         profiler.stop();
  >         proflier.flush(); // dumps `example` profile
  >       }
  >     }
  >     ```
  >
  >     ```sh
  >     kcachegrind callgrind.* # opens all profiles combined
  >     ```
  > </details>

  > <details>
  >   <summary>Benchmarking / Profiling / Analysis</summary>
  >
  >   - Latency and Throughput?
  >     - latency is the time it takes for a single operation to complete (ns)
  >     - throughput is the total number of operations or tasks completed in a given amount of time (op/s)
  >
  >     note: In single threaded execution if the algo can`t be parallilzed (where throughput would be important) you likely care about the latency.
  >
  >   - Common pitfalls?
  >     - Undersand
  >       - what (latency, throughput),
  >       - state (cold, warm, data distribution)
  >       - how (timing, counters, profiling) to measure
  >       - hardware (what env, os, setup, ...)
  >       - ensure proper analysis (avoid premature conclusions)
  >       - assert your expectations
  >       - verify in the production-like use case/env
  >     - Ensure the machine is tuned for micro-benchmarking (see #tuning)
  >     - Use structured and diligent methods (see #top_down_analysis)
  >     - Use realistic scenarios (see #setup)
  >     - Use statistical methods for analysis (#stat)
  >     - Analyze generated assembly (see #annotate/#mca)
  >     - Visualize results (see #gnuplot)
  >     - Document / share analysis (see #html)
  >     - Automate expectations (see #testing)
  >     - Enhnace your understanding (see #performance)
  >     - Verify changes in production like system (see #prof) - [active-benchmarking](https://www.brendangregg.com/activebenchmarking.html)
  >     - Measure / verify consistently (see #json)
  >
  >   - Performance compilation flags?
  >     ```
  >     -O1                     # optimizations (level1) [0]
  >     -O2                     # optimizations (level1 + level2) [0]
  >     -O3                     # [unsafe] optimizations (level1 + level2 + level3) [0]
  >     -DNDEBUG                # disable asserts, etc.
  >     -march=native           # specifies architecture [1]
  >     -ffast-math             # [unsafe] faster math but non-conforming math [2]
  >     -g                      # debug symbols
  >     -fno-omit-frame-pointer # keep the frame pointer in a register for functions that don`t need one
  >     -fcf-protection=none    # [unsafe] stops emmitting `endbr64` # control-flow enforcement technology (cet)
  >     ```
  >
  >     - [0]: https://gcc.gnu.org/onlinedocs/gcc/Optimize-Options.html
  >     - [1]: https://gcc.gnu.org/onlinedocs/gcc/x86-Options.html
  >     - [2]: https://gcc.gnu.org/wiki/FloatingPointMath
  >
  >   - performance attributes?
  >     ```cpp
  >     [[gnu::target("avx2")]]
  >     ```
  >
  >     ```cpp
  >     [[gnu::optimize("O3")]
  >     ```
  >
  >     ```cpp
  >     [[gnu::optimize("ffast-math")]
  >     ```
  >
  >     note: gcc support is much more comprehensive than clang
  >
  >   - When to finish micro-benchmarking?
  >     - when IPC if you reached won`t get any better.
  >
  >   - How to handle cases when there are no obvious bottlenecks?
  >     - The most likely scenario is cache utilization.
  >
  >   - How to profile in production?
  >     - see `prof` API
  >
  >   - What are different micro-benchmarking workflows?
  >     - [cpp] # measure, analyze from cpp
  >       pros: self contained, easy itartions, supports unit testing, supports running on the server (PERF_PLOT_TERM="dumb")
  >       cons: requires multiple runs for different compiler/flags
  >       note: can be shared with scripts/html.sh | gist.sh
  >
  >     - [cpp->json->notebook] measure with cpp and export json, analyze results in notebook
  >       pros: can be unified across different for different compiler/flags and production runs, power of notebooks, easy to share, can be done offline
  >       cons: multiple places, longer runs, harder to change the code, harder to run on remote servers, harder to unit test expectations, requires GUI
  >
  >     - [cpp->json->CI] measure with cpp and use json for CI
  >       - used for continous benchmarking
  >
  >   - What is top-down microarchitecture analysis method?
  >     - https://www.intel.com/content/www/us/en/docs/vtune-profiler/cookbook/2023-0/top-down-microarchitecture-analysis-method.html
  >     - https://github.com/andikleen/pmu-tools/wiki/toplev-manual
  >
  >   - What is jupyter notebook and how to use it?
  >     - jupter-notebook is great for analysis and leaving a trace of it.
  >       however, C++ doesn`t integrate well with jupyter-notbook (cling is often not an option)
  >       so it doesn`t work well with servers (requires coping data) and it`s an additional file.
  >       perf approach is to leave trace of benchmarking with automatically testing it direclty from C++
  >       so that it can be seen for the future and explains what and why.
  >
  >       ```sh
  >       # apt install jupyter
  >       jupyter notebook -ip 0.0.0.0 --no-browser notebook.ipynb
  >       ```
  >
  > </details>

  > <details>
  >   <summary>Troubleshooting</summary>
  >
  >   - How to setup `linux performance counters`?
  >     - [setup.sh](https://gist.github.com/qlibs-dev/48674e96c340f5fa1a844c9b5dd5b733)
  >
  >     ```sh
  >     sudo mount -o remount,mode=755 /sys/kernel/debug
  >     sudo mount -o remount,mode=755 /sys/kernel/debug/tracing
  >     sudo chown `whoami` /sys/kernel/debug/tracing/uprobe_events
  >     sudo chmod a+rw /sys/kernel/debug/tracing/uprobe_events
  >     echo 0 | sudo tee /proc/sys/kernel/kptr_restrict
  >     echo -1 | sudo tee /proc/sys/kernel/perf_event_paranoid
  >     echo 1000 | sudo tee /proc/sys/kernel/perf_event_max_sample_rate
  >     ```
  >
  >   - How to setup `rdpmc`?
  >     - [rdpmc](https://www.felixcloutier.com/x86/rdpmc)
  >
  >     ```sh
  >     echo 2 | sudo tee /sys/devices/cpu_core/rdpmc
  >     ```
  >
  >   - How to find out which performance events are supported by the cpu?
  >     - `perf list` # https://perfmon-events.intel.com
  >
  >   - How to reduce noise when benchmarking?
  >     - `pyperf` (https://pyperf.readthedocs.io/en/latest/system.html)
  >       ```sh
  >       sudo pyperf system tune
  >       sudo pyperf system show
  >       sudo pyperf system reset
  >       ```
  >
  >     - Linux
  >       ```sh
  >       # Set Process CPU Affinity (apt install util-linux)
  >       # note: `perf::sys::thread::affinity` can be used intead
  >       taskset -c 0 ./a.out
  >
  >       # Set Process Scheduling Priority (apt install coreutils)
  >       # note: `perf::sys::thread::priority` can be used instead
  >       nice -n -20 taskset -c 0 ./a.out # -20..19 (most..less favorable to the process)
  >
  >       # Disable CPU Frequency Scaling (apt install cpufrequtils)
  >       sudo cpupower frequency-set --governor performance
  >       # cat /sys/devices/system/cpu/cpu*/cpufreq/scaling_governor
  >
  >       # Disable Address Space Randomization
  >       echo 0 > /proc/sys/kernel/randomize_va_space
  >
  >       # Disable Processor Boosting
  >       echo 0 | sudo tee /sys/devices/system/cpu/cpufreq/boost
  >
  >       # Disable Turbo Mode
  >       echo 1 > /sys/devices/system/cpu/intel_pstate/no_turbo
  >
  >       # Disable Hyperthreading/SMT
  >       echo off | sudo tee /sys/devices/system/cpu/smt/control
  >
  >       # Restrict memory to a single socket
  >       numactl -m 0 -N 0 ./a.out
  >
  >       # Enable Huge Pages
  >       sudo numactl --cpunodebind=1 --membind=1 hugeadm \
  >         --obey-mempolicy --pool-pages-min=1G:64
  >       sudo hugeadm --create-mounts
  >       ```
  >
  >     - boot/grub
  >       ```sh
  >       # Enable Kernel Mode Task-Isolation (https://lwn.net/Articles/816298)
  >       isolcpus=<cpu number>,...,<cpu number>
  >       # cat /sys/devices/system/cpu/isolated
  >
  >       # Disable P-states and C-states
  >       idle=pool intel_pstate=disable intel_idle.max_cstate=0 processor.max_cstate=1
  >       # cat /sys/devices/system/cpu/intel_pstate/status
  >
  >       # Disable NMI watchdog (cat /proc/sys/kernel/nmi_watchdog)
  >       nmi_watchdog=0
  >       ```
  >
  >     - [UEFI](https://en.wikipedia.org/wiki/UEFI)
  >       - [uefi-gist](https://gist.github.com/qlibs-dev/f03502d97cdc857832052ef772184a68)
  >
  >   - How to check cpu info?
  >     ```sh
  >     # System topology (apt install hwloc)
  >     lstopo # lstopo-no-graphics
  >
  >     # CPU info (apt install util-linux)
  >     lscpu | grep -E ^CPU|^Model|^Core|^Socket|^Thread
  >
  >     # Cache info
  >     lscpu | grep cache
  >     getconf -a | grep CACHE_LINESIZE
  >
  >     # Numa nodes
  >     lscpu | grep -E ^NUMA
  >
  >     # Huge pages
  >     cat /proc/meminfo | grep -i huge
  >     ```
  >
  > </details>

### Resources

  > <details>
  >   <summary>Specs</summary>
  >
  >   - Manuals
  >     - Intel - https://www.intel.com/content/www/us/en/developer/articles/technical/intel-sdm.html
  >     - AMD - https://www.amd.com/content/dam/amd/en/documents/processor-tech-docs/programmer-references/40332.pdf
  >     - ARM - https://developer.arm.com/documentation/ddi0487/latest
  >     - Apple - https://developer.apple.com/documentation/xcode/writing-arm64-code-for-apple-platforms, https://github.com/mikeroyal/Apple-Silicon-Guide
  >
  >   - Books
  >     - Optimizing Software in C++: An Optimization Guide for Windows, Linux and Mac platforms - https://www.agner.org/optimize/optimizing_cpp.pdf
  >     - Optimizing Subroutines in Assembly Language: An Optimization Guide for x86 platforms - https://www.agner.org/optimize/optimizing_assembly.pdf
  >     - The Microarchitecture of Intel, AMD and VIA CPUs: An Optimization Guide for Assembly programmers and compiler makers - https://www.agner.org/optimize/microarchitecture.pdf
  >     - Instruction Tables: Lists of instruction latencies, throughputs and micro-operation breakdowns for Intel, AMD and VIA CPUs - https://www.agner.org/optimize/instruction_tables.pdf
  >     - Calling Conventions for different C++ compilers and operating systems - https://www.agner.org/optimize/calling_conventions.pdf
  >     - What Every Programmer Should Know About Memory - https://www.akkadia.org/drepper/cpumemory.pdf
  >     - Performance Analysis and Tuning on Modern CPUs - https://github.com/dendibakh/perf-book/releases
  >     - Algorithms for Modern Hardware - https://en.algorithmica.org/hpc
  >     - Computer Architecture - https://dl.acm.org/doi/book/10.5555/1999263
  >     - Is Parallel Programming Hard, And, If So, What Can You Do About It? - https://www.kernel.org/pub/linux/kernel/people/paulmck/perfbook/perfbook.html
  >     - SIMD for C++ Developers - http://const.me/articles/simd/simd.pdf
  >     - Hackers Delight - https://doc.lagout.org/security/Hackers%20Delight.pdf
  >     - Data-Oriented Design - https://www.dataorienteddesign.com/dodbook
  >     - Top-Down Microarchitecture Analysis Method - https://www.intel.com/content/www/us/en/docs/vtune-profiler/cookbook/2023-0/top-down-microarchitecture-analysis-method.html
  >     - A Top-Down method for performance analysis and counters architecture - https://www.researchgate.net/publication/269302126_A_Top-Down_method_for_performance_analysis_and_counters_architecture
  >     - Top-Down Metrics - https://github.com/intel/perfmon/blob/main/TMA_Metrics-full.xlsx
  >     - Measuring Workloads With TopLev - https://github.com/andikleen/pmu-tools/wiki/toplev-manual
  >     - The Art of Writing Efficient Programs - https://www.packtpub.com/product/the-art-of-writing-efficient-programs
  >     - Bits Of Architecture - https://github.com/CoffeeBeforeArch/bits_of_architecture
  >     - Bit Twiddling Hacks - https://graphics.stanford.edu/~seander/bithacks.html
  >     - Memory Models - https://research.swtch.com/mm
  >     - nanoBench: A Low-Overhead Tool for Running Microbenchmarks on x86 Systems - https://arxiv.org/abs/1911.03282
  >     - The Linux Scheduler: a Decade of Wasted Cores - https://people.ece.ubc.ca/sasha/papers/eurosys16-final29.pdf
  >     - The Tail At Scale - https://www.barroso.org/publications/TheTailAtScale.pdf
  >     - Producing wrong data without doing anything obviously wrong! - https://dl.acm.org/doi/10.1145/1508284.1508275
  >     - Robust benchmarking in noisy environments - https://arxiv.org/abs/1608.04295
  >     - Can Seqlocks Get Along With Programming Language Memory Models - https://www.hpl.hp.com/techreports/2012/HPL-2012-68.pdf
  >     - Cache-Oblivious Algorithms and Data Structures - https://erikdemaine.org/papers/BRICS2002/
  >     - Performance suiteering of Software Systems - https://ocw.mit.edu/courses/6-172-performance-suiteering-of-software-systems-fall-2018
  >
  >   - Cheatsheets
  >     - Operation Costs in CPU Clock Cycles - http://ithare.com/infographics-operation-costs-in-cpu-clock-cycles
  >     - Intel Intrinsics Guide - https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html
  >     - `x86` Intrinsics Cheatsheet - https://db.in.tum.de/~finis/x86-intrin-cheatsheet-v2.1.pdf
  >     - A machine-readable CPUID data repository - https://x86-cpuid.org
  >     - Instruction Matrix - https://github.com/google/highway/blob/master/g3doc/instruction_matrix.pdf
  >     - Microarchitecture Cheatsheet - https://docs.google.com/spreadsheets/d/18ln8SKIGRK5_6NymgdB9oLbTJCFwx0iFI-vUs6WFyuE
  >     - Performance Monitoring Events - https://perfmon-events.intel.com
  >     - Core to Core Latency - https://github.com/nviennot/core-to-core-latency
  >     - Clock Time Analysis - https://gitlab.com/chriscox/CppPerformanceBenchmarks/-/wikis/ClockTimeAnalysis
  >
  >       ```cpp
  >       Speed of light ............................ ~1 foot/ns
  >       L1 cache reference ......................... 0.5 ns
  >       Branch mispredict ............................ 5 ns
  >       L2 cache reference ........................... 7 ns
  >       Mutex lock/unlock ........................... 25 ns
  >       Main memory reference ...................... 100 ns
  >       Send 2K bytes over 1 Gbps network ....... 20,000 ns  =  20 µs
  >       SSD random read ........................ 150,000 ns  = 150 µs
  >       Read 1 MB sequentially from memory ..... 250,000 ns  = 250 µs
  >       Round trip within same datacenter ...... 500,000 ns  = 0.5 ms
  >       Read 1 MB sequentially from SSD .....  1,000,000 ns  =   1 ms
  >       Read 1 MB sequentially from disk .... 20,000,000 ns  =  20 ms
  >       Send packet CA->UK->CA ....          150,000,000 ns  = 150 ms
  >       ```
  >
  >   - Guides
  >     - The Linux Kernel Documentation - https://www.kernel.org/doc/html/latest/index.html
  >     - RHEL Performance Guide - https://myllynen.github.io/rhel-performance-guide/
  >     - Monitoring and Managing System Status and Performance - https://docs.redhat.com/en/documentation/red_hat_enterprise_linux/8/html/monitoring_and_managing_system_status_and_performance
  >     - Active Benchmarking - https://www.brendangregg.com/activebenchmarking.html
  >     - A CPU research kernel with minimal noise for cycle-by-cycle micro-architectural introspection - https://gamozolabs.github.io/metrology/2019/08/19/sushi_roll.html
  >     - X3 Low Latency Quickstart - https://docs.amd.com/r/en-US/ug1586-onload-user/X3-Low-Latency-Quickstart
  >     - Modular platform for computer-system architecture research - https://www.gem5.org
  >     - Reverse debugging at scale - https://suiteering.fb.com/2021/04/27/developer-tools/reverse-debugging
  >     - Asynchronous Programming Under Linux - https://unixism.net/loti/async_intro.html
  >     - Phoronix Test Suite - https://www.phoronix-test-suite.com
  >     - CPU benchmark - https://www.cpubenchmark.net
  >     - Modern Microprocessors A 90-Minute Guide! - https://www.lighterra.com/papers/modernmicroprocessors
  >     - 7-Zip LZMA Benchmark - https://www.7-cpu.com
  >     - CPU Benchmarks - https://curiouscoding.nl/posts/cpu-benchmarks
  >     - JVM Anatomy Quarks - https://shipilev.net/jvm/anatomy-quarks
  >     - `linux-perf` Source Code - https://github.com/torvalds/linux/blob/master/tools/perf, https://elixir.bootlin.com/linux/v6.13-rc1/source/include/uapi/linux/perf_event.h
  >     - `gcc` optimization - https://wiki.gentoo.org/wiki/GCC_optimization
  >     - `gcc` assembler syntax - https://www.felixcloutier.com/documents/gcc-asm
  >     - `llvm` Scheduling Models - https://github.com/llvm/llvm-project/tree/main/llvm/lib/Target
  >     - `llvm` Optimization Passes - https://llvm.org/docs/Passes.html
  >     - `llvm` Vectorizers - https://llvm.org/docs/Vectorizers.html
  >
  >   - Tutorials
  >     - Performance Ninja Class - https://github.com/dendibakh/perf-ninja
  >     - Hardware Effects - https://github.com/Kobzol/hardware-effects
  >     - Performance Tuning - https://github.com/NAThompson/performance_tuning_tutorial
  >     - Mastering C++ with Google Benchmark - https://ashvardanian.com/posts/google-benchmark
  >     - Learning to Write Less Slow C, C++, and Assembly Code - https://github.com/ashvardanian/less_slow.cpp
  >
  > </details>

  > <details>
  >   <summary>Feeds</summary>
  >
  >   - News
  >     - Linux News - https://lwn.net
  >     - Chips and Cheese - https://chipsandcheese.com
  >     - WikiChip - https://wikichip.org
  >     - CPUID - https://www.cpuid.com/news.html
  >     - Real World Tech - https://www.realworldtech.com
  >     - Tom`s Hardware - https://www.tomshardware.com
  >     - Phoronix - https://www.phoronix.com
  >     - `comp.lang.asm.x86` - https://groups.google.com/g/comp.lang.asm.x86
  >
  >   - Blogs
  >     - Agner Fog`s Blog - https://www.agner.org
  >     - Denis Bakhvalov`s Blog - https://easyperf.net/blog
  >     - Daniel Lemire`s Blog - https://lemire.me/blog
  >     - Wojciech Mula`s Blog - http://0x80.pl/articles/index.html
  >     - Erik Rigtorp`s Blog - https://rigtorp.se
  >     - Johnny`s Software Blog - https://johnnysswlab.com
  >     - JabPerf`s Blog - https://jabperf.com/blog
  >     - Brendan Gregg`s Blog - https://brendangregg.com/blog
  >     - Geoff Langdale`s Blog - https://branchfree.org
  >     - Ragnar Groot Koerkamp`s Blog - https://curiouscoding.nl/posts/cpu-benchmarks
  >     - Travis Downs`s Blog - https://travisdowns.github.io
  >     - Tristan`s Blog - https://thume.ca/archive.html
  >     - Stefanos Baziotis`s Blog - https://sbaziotis.com/#blog
  >     - Gamozo Labs Blog - https://gamozolabs.github.io
  >     - Mechanical Sympathy Blog - https://mechanical-sympathy.blogspot.com
  >     - Performance Engineering Blog - https://pramodkumbhar.com
  >     - Dmitry Vyukov Blog - https://www.1024cores.net
  >     - John Farrier`s Blog - https://johnfarrier.com
  >     - Performance Tricks Blog - https://www.performetriks.com/blog
  >     - Coding Confessions Blog - https://blog.codingconfessions.com
  >     - The Netflix Tech Blog - https://netflixtechblog.com
  >     - Cloudflare Blog - https://blog.cloudflare.com
  >
  >   - Lists
  >     - C++ Links - https://github.com/MattPD/cpplinks
  >     - Awesome Performance C++ - https://github.com/fenbf/AwesomePerfCpp
  >     - Awesome Lock Free - https://github.com/rigtorp/awesome-lockfree
  >     - Awesome SIMD - https://github.com/awesome-simd/awesome-simd
  >     - Computer, Enhance! - https://www.computerenhance.com
  >     - Low Latency Trading Insights - https://lucisqr.substack.com
  >
  >   - Misc
  >     - Conferences - https://www.p99conf.io, https://supercomputing.org, https://hotchips.org, https://microarch.org
  >     - Podcasts - https://signals-threads.simplecast.com, https://microarch.club, https://tlbh.it, https://twoscomplement.org
  >     - C++ Low Latency Group (SG14) - https://github.com/WG21-SG14/SG14
  >
  > </details>

  > <details>
  >   <summary>Videos</summary>
  >
  >   - Channels
  >     - Computer Architecture - Onur Mutlu - https://www.youtube.com/@OnurMutluLectures
  >     - Computer, Enhance - Casey Muratori - https://www.youtube.com/@MollyRocket
  >     - Assembly / Creel - https://www.youtube.com/c/WhatsACreel
  >     - EasyPerf / Spaces - https://www.youtube.com/@easyperf3992
  >     - SIMD algorithms - Denis Yaroshevskiy - https://www.youtube.com/playlist?list=PLYCMvilhmuPEM8DUvY6Wg_jaSFHpmlSBD
  >
  >   - Benchmarking
  >     - Tuning C++: Benchmarks, and CPUs, and Compilers! Oh My! - Chandler Carruth - https://www.youtube.com/watch?v=nXaxk27zwlk
  >     - Counting Nanoseconds Microbenchmarking C++ Code - David Gross - https://www.youtube.com/watch?v=Czr5dBfs72U
  >     - Benchmarking C++ Code - Bryce Adelstein-Lelbach - https://www.youtube.com/watch?v=zWxSZcpeS8Q
  >     - Benchmarking C++, From video games to algorithmic trading - Alexander Radchenko - https://www.youtube.com/watch?v=7YVMC5v4qCA
  >     - Tuning C++: Benchmarks, and CPUs, and Compilers! Oh My! - Chandler Carruth - https://www.youtube.com/watch?v=nXaxk27zwlk
  >     - Going Nowhere Faster - Chandler Carruth - https://www.youtube.com/watch?v=2EWejmkKlxs
  >     - Measurement and Timing - Performance suiteering of Software Systems - https://www.youtube.com/watch?v=LvX3g45ynu8
  >     - How NOT to Measure Latency - Gil Tene - https://www.youtube.com/watch?v=lJ8ydIuPFeU
  >
  >   - Analyzing
  >     - Understanding the Performance of code using LLVM-MCA - A. Biagio & M. Davis - https://www.youtube.com/watch?v=Ku2D8bjEGXk
  >     - LLVM Optimization Remarks - Ofek Shilon - https://www.youtube.com/watch?v=qmEsx4MbKoc
  >
  >   - Profiling
  >     - From Top-down Microarchitecture Analysis to Structured Performance Optimizationsa - https://cassyni.com/events/YKbqoE4axHCgvQ9vuQq7Cy
  >     - Coz: finding code that counts with causal profiling - ACM - https://www.youtube.com/watch?v=jE0V-p1odPg
  >     - Take Advantage for Intel Instrumentation and Tracing Technology for Performance Analysis - https://www.youtube.com/watch?v=1zdVFLajewM&list=PLg-UKERBljNw3_6Q598CS3DE7KqDXjP-d
  >     - LIKWID Performance Tools - https://www.youtube.com/playlist?list=PLxVedhmuwLq2CqJpAABDMbZG8Whi7pKsk
  >     - Introduction to the Tracy Profiler - Bartosz Taudul - https://youtu.be/fB5B46lbapc
  >     - Performance Matters - Emery Berger - https://www.youtube.com/watch?v=r-TLSBdHe1A
  >
  >   - Optimizing
  >     - Understanding Compiler Optimization - Chandler Carruth - https://www.youtube.com/watch?v=haQ2cijhvhE
  >     - Efficiency with Algorithms, Performance with Data Structures - Chandler Carruth - https://www.youtube.com/watch?v=fHNmRkzxHWs
  >     - Design for Performance - Fedor Pikus - https://www.youtube.com/watch?v=m25p3EtBua4
  >     - Unlocking Modern CPU Power - Next-Gen C++ Optimization Techniques - https://www.youtube.com/watch?v=wGSSUSeaLgA
  >     - Branchless Programming in C++ - Fedor Pikus - https://www.youtube.com/watch?v=g-WPhYREFjk
  >     - CPU design effects - Jakub Beranek - youtube.com/watch?v=ICKIMHCw--Y
  >     - Fastware - Andrei Alexandrescu - https://www.youtube.com/watch?v=o4-CwDo2zpg
  >     - Performance Tuning - Matt Godbolt - https://www.youtube.com/watch?v=fV6qYho-XVs
  >     - Memory & Caches - Matt Godbolt - https://www.youtube.com/watch?v=4_smHyqgDTU
  >     - What Every Programmer Should Know about How CPUs Work - Matt Godbolt - https://www.youtube.com/watch?v=-HNpim5x-IE
  >     - There Are No Zero-cost Abstractions - Chandler Carruth - https://www.youtube.com/watch?v=rHIkrotSwcc&
  >     - Understanding Optimizers: Helping the Compiler Help You - Nir Friedman - https://www.youtube.com/watch?v=8nyq8SNUTSc
  >     - C++ Algorithmic Complexity, Data Locality, Parallelism, Compiler Optimizations, & Some Concurrency - Avi Lachmish - https://www.youtube.com/watch?v=0iXRRCnurvo
  >     - Software Optimizations Become Simple with Top-Down Analysis on Intel Skylake - Ahmad Yasin - https://www.youtube.com/watch?v=kjufVhyuV_A
  >     - Being Friendly to Your Computer Hardware in Software Development - Ignas Bagdonas - https://www.youtube.com/watch?v=eceFgsiPPmk
  >     - Want fast C++? Know your hardware - Timur Doumler - https://www.youtube.com/watch?v=BP6NxVxDQIs
  >     - What is Low Latency C++ - Timur Doumler - https://www.youtube.com/watch?v=EzmNeAhWqVs, https://www.youtube.com/watch?v=5uIsadq-nyk
  >     - Where Have All the Cycles Gone? - Sean Parent - https://www.youtube.com/watch?v=B-aDBB34o6Y
  >     - Understanding CPU Microarchitecture to Increase Performance - https://www.youtube.com/watch?v=rglmJ6Xyj1c
  >     - Performance Analysis & Tuning on Modern CPU - Denis Bakhvalov - https://www.youtube.com/watch?v=Ho3bCIJcMcc
  >     - Comparison of C++ Performance Optimization Techniques for C++ Programmers - Eduardo Madrid - https://www.youtube.com/watch?v=4DQqcRwFXOI
  >     - Simple Code, High Performance - Molly Rocket - https://www.youtube.com/watch?v=Ge3aKEmZcqY
  >     - Assembly, System Calls, and Hardware in C++ - David Sankel - https://www.youtube.com/watch?v=7xwjjolDnwg
  >     - Optimizing Binary Search - Sergey Slotin - https://www.youtube.com/watch?v=1RIPMQQRBWk
  >     - A Deep Dive Into Dispatching Techniques in C++ - Jonathan Muller - https://www.youtube.com/watch?v=vUwsfmVkKtY
  >     - C++ Memory Model: from C++11 to C++23 - Alex Dathskovsky - https://www.youtube.com/watch?v=SVEYNEWZLo4
  >     - Abusing Your Memory Model for Fun and Profit - Samy Al Bahra, Paul Khuong - https://www.youtube.com/watch?v=N07tM7xWF1U&t=1s
  >     - The speed of concurrency (is lock-free faster?) - Fedor Pikus - https://www.youtube.com/watch?v=9hJkWwHDDxs
  >     - Read, Copy, Update, then what? RCU for non-kernel programmers - Fedor Pikus - https://www.youtube.com/watch?v=rxQ5K9lo034
  >     - Single Producer Single Consumer Lock-free FIFO From the Ground Up - Charles Frasch - https://www.youtube.com/watch?v=K3P_Lmq6pw0
  >     - Introduction to Hardware Efficiency in Cpp - Ivica Bogosavljevic - https://www.youtube.com/watch?v=Fs_T070H9C8
  >     - The Performance Price of Dynamic Memory in C++ - Ivica Bogosavljevic - https://www.youtube.com/watch?v=LC4jOs6z-ZI
  >     - The Hidden Performance Price of C++ Virtual Functions - Ivica Bogosavljevic - https://www.youtube.com/watch?v=n6PvvE_tEPk
  >     - Why do Programs Get Slower with Time? - Ivica Bogosavljevic - https://www.youtube.com/watch?v=nS5vjnPKX0I
  >     - CPU Cache Effects - Sergey Slotin - https://www.youtube.com/watch?v=mQWuX_KgH00
  >     - Cpu Caches and Why You Care - Scott Meyers - https://www.youtube.com/watch?v=WDIkqP4JbkE
  >     - CPU vs FPGA - https://www.youtube.com/watch?v=BML1YHZpx2o
  >     - Designing for Efficient Cache Usage - Scott McMillan - https://www.youtube.com/watch?v=3-ityWN-FdE
  >     - Cache consistency and the C++ memory model - Yossi Moale - https://www.youtube.com/watch?v=Sa08x_NMZIg
  >     - std::simd: How to Express Inherent Parallelism Efficiently Via Data-parallel Types - Matthias Kretz - https://www.youtube.com/watch?v=LAJ_hywLtMA
  >     - The Art of SIMD Programming - Sergey Slotin - https://www.youtube.com/watch?v=vIRjSdTCIEU
  >     - Advanced SIMD Algorithms in Pictures - Denis Yaroshevskiy - https://www.youtube.com/watch?v=vGcH40rkLdA
  >     - Performance Optimization, SIMD and Cache - Sergiy Migdalskiy - https://www.youtube.com/watch?v=Nsf2_Au6KxU
  >     - Data-Oriented Design and C++ - Mike Acton - https://www.youtube.com/watch?v=rX0ItVEVjHc
  >     - Practical Data Oriented Design (DoD) - Andrew Kelley - https://www.youtube.com/watch?v=IroPQ150F6c
  >     - Data Orientation For The Win - Eduardo Madrid - https://www.youtube.com/watch?v=QbffGSgsCcQ
  >     - You Can Do Better than std::unordered_map - Malte Skarupke - https://www.youtube.com/watch?v=M2fKMP47slQ
  >     - Faster than Rust and C++: the PERFECT hash table - https://www.youtube.com/watch?v=DMQ_HcNSOAI
  >     - Designing a Fast, Efficient, Cache-friendly Hash Table, Step by Step - Matt Kulukundis - https://www.youtube.com/watch?v=ncHmEUmJZf4
  >     - C++ Run-Time Optimizations for Compile-Time Reflection - Kris Jusiak - https://www.youtube.com/watch?v=ncHmEUmJZf4 - https://www.youtube.com/watch?v=kCATOctR0BA
  >
  >   - High Frequency Trading
  >     - When Nanoseconds Matter: Ultrafast Trading Systems in C++ - David Gross - https://www.youtube.com/watch?v=sX2nF1fW7kI
  >     - When a Microsecond Is an Eternity: High Performance Trading Systems in C++ - Carl Cook - https://www.youtube.com/watch?v=NH1Tta7purM
  >     - The Speed Game: Automated Trading Systems in C++ - Carl Cook - https://www.youtube.com/watch?v=ulOLGX3HNCI
  >     - Low-Latency Trading Systems in C++ - Jason McGuiness - https://www.youtube.com/watch?v=FnMfhWiSweo
  >     - High Frequency Trading and Ultra Low Latency development techniques - Nimrod Sapir - https://www.youtube.com/watch?v=_0aU8S-hFQI
  >     - Trading at light speed: designing low latency systems in C++ - David Gross - https://www.youtube.com/watch?v=8uAW5FQtcvE&list=PLSkBiuVO9yj1MvDkYJ5WOnPeKsoRi3eiW&index=2
  >     - Optimizing Trading Strategies for FPGAs in C/C++ - https://www.youtube.com/watch?v=4Wklh0XS5i0
  >     - C++ Electronic Trading for Cpp Programmers - Mathias Gaunard - https://www.youtube.com/watch?v=ltT2fDqBCEo
  >     - Achieving performance in financial data processing through compile time introspection - Eduardo Madrid - https://www.youtube.com/watch?v=z6fo90R8q5U
  >     - How to Simulate a Low Latency Exchange in C++ - Benjamin Catterall - https://www.youtube.com/watch?v=QQrTE4YLkSE
  >     - Building Low Latency Trading Systems - https://www.youtube.com/watch?v=yBNpSqOOoRk
  >     - Cache Warming: Warm Up The Code - Jonathan Keinan - https://www.youtube.com/watch?v=XzRxikGgaHI
  >     - How Linux Took Over the World of Finance - Christoph H Lameter - https://www.youtube.com/watch?v=UUOM4KdaHkY
  >
  > </details>

  > <details>
  >   <summary>Tools</summary>
  >
  >   - Online
  >     - Compiler Explorer - https://compiler-explorer.com
  >     - Latency, Throughput, and Port Usage Information - https://uops.info / https://uica.uops.info
  >     - Latency, Memory Latency and CPUID dumps - http://instlatx64.atw.hu
  >     - Instruction Reference - https://www.felixcloutier.com/x86
  >     - `x86` Processor Information - https://sandpile.org
  >     - Memory Latency Data - https://chipsandcheese.com/memory-latency-data
  >     - Instruction Discovery And Analysis on `x86-64` - https://explore.liblisa.nl
  >     - Quick C++ Benchmark - https://quick-bench.com
  >
  >   - Profiling
  >     - linux-perf - https://perf.wiki.kernel.org
  >     - intel-vtune - https://www.intel.com/content/www/us/en/docs/vtune-profiler
  >     - intel-advisor - https://www.intel.com/content/www/us/en/developer/tools/oneapi/advisor.html
  >     - intel-sde - https://www.intel.com/content/www/us/en/developer/articles/tool/software-development-emulator.html
  >     - intel-pin - https://www.intel.com/content/www/us/en/developer/articles/tool/pin-a-dynamic-binary-instrumentation-tool.html
  >     - amd-uprof - https://www.amd.com/en/developer/uprof.html
  >     - callgrind - https://valgrind.org/docs/manual/cl-manual.html
  >     - pmu-tools - https://github.com/andikleen/pmu-tools
  >     - perf-tools - https://github.com/brendangregg/perf-tools
  >     - yperf - https://github.com/aayasin/perf-tools
  >     - ebpf - https://ebpf.io
  >     - dtrace - https://www.oracle.com/linux/downloads/linux-dtrace.html
  >     - ftrace - https://www.kernel.org/doc/html/latest/trace/ftrace.html
  >     - utrace - https://github.com/Gui774ume/utrace
  >     - strace - https://strace.io
  >     - magictrace - https://github.com/janestreet/magic-trace
  >     - omnitrace - https://github.com/ROCm/omnitrace
  >     - tracy - https://github.com/wolfpld/tracy
  >     - optick - https://github.com/bombomby/optick
  >     - rad_telemetry - https://www.radgametools.com/telemetry.html
  >     - wachy - https://rubrikinc.github.io/wachy
  >     - easy_profiler - https://github.com/yse/easy_profiler
  >     - gprof - https://ftp.gnu.org/old-gnu/Manuals/gprof-2.9.1/html_mono/gprof.html
  >     - gperftools - https://github.com/gperftools/gperftools
  >     - oprofile - https://oprofile.sourceforge.io
  >     - optview2 - https://github.com/OfekShilon/optview2
  >     - llvm-xray - https://llvm.org/docs/XRay.html
  >     - likwid - https://github.com/RRZE-HPC/likwid
  >     - lttng - https://lttng.org
  >     - sysprof - https://www.sysprof.com
  >     - coz - https://github.com/plasma-umass/coz
  >     - bcc - https://github.com/iovisor/bcc
  >
  >   - Analysis
  >     - llvm-mca - https://llvm.org/docs/CommandGuide/llvm-mca.html
  >     - osaca - https://github.com/RRZE-HPC/OSACA
  >     - uica - https://uica.uops.info
  >
  >   - Profile-Guided Optimization (PGO)
  >     - clang-pgo - https://clang.llvm.org/docs/UsersManual.html#profile-guided-optimization
  >     - gcc-pgo - https://gcc.gnu.org/onlinedocs/gcc/Optimize-Options.html
  >     - llvm-bolt - https://github.com/llvm/llvm-project/blob/main/bolt/README.md
  >     - llvm-propelleer - https://github.com/google/llvm-propeller
  >     - autofdo - https://github.com/google/autofdo
  >
  >   - Utilities
  >     - pyperf - https://github.com/psf/pyperf
  >     - kcachegrind - https://kcachegrind.sourceforge.net/html/Home.html
  >     - hyperfine - https://github.com/sharkdp/hyperfine
  >     - hotspot - https://github.com/KDAB/hotspot
  >     - numatop - https://github.com/intel/numatop
  >     - bpftop - https://github.com/Netflix/bpftop
  >     - pahole - https://github.com/acmel/dwarves
  >     - perfetto - https://perfetto.dev
  >     - speedscope - https://github.com/jlfwong/speedscope
  >     - retsnoop - https://github.com/anakryiko/retsnoop
  >     - core-to-core-latency - https://github.com/nviennot/core-to-core-latency
  >     - llvm-opt-report - https://llvm.org/docs/CommandGuide/llvm-opt-report.html
  >     - jupyter notebook - https://jupyter.org
  >
  >   - Libraries
  >     - `perf_event_open` - https://man7.org/linux/man-pages/man2/perf_event_open.2.html
  >     - perfmon2 - https://perfmon2.sourceforge.net
  >     - papi - https://github.com/icl-utk-edu/papi
  >     - libpfc - https://github.com/obilaniu/libpfc
  >     - intel_pt - https://github.com/intel/libipt
  >     - intel_pcm - https://github.com/intel/pcm
  >
  > </details>

### License

  > [MIT](LICENSE) / [Apache2:LLVM](LICENSE)*

<!--
#endif

/**
 * PERF: C++23 Performance library
 * - https://github.com/qlibs/perf
 */
#ifndef PERF
#define PERF (1, 0, 0) /// (MAJOR, MINOR, PATCH) # https://semver.org

#if (not defined(PERF_GNU)) or (defined(PERF_GNU) and PERF_GNU == 1)
#pragma GCC system_header
#if __has_include(<cpuid.h>)
#include <cpuid.h>
#endif
#ifndef PERF_GNU
#ifdef __GNUC__
  #define PERF_GNU 1
#else
  #define PERF_GNU 0
#endif
#endif
#endif

#if (not defined(PERF_LINUX)) or (defined(PERF_LINUX) and PERF_LINUX == 1)
#if __has_include(<unistd.h>) and \
    __has_include(<pthread.h>) and \
    __has_include(<sched.h>) and \
    __has_include(<sys/time.h>) and\
    __has_include(<sys/syscall.h>) and \
    __has_include(<sys/ioctl.h>) and \
    __has_include(<sys/mman.h>) and \
    __has_include(<linux/perf_event.h>)
#include <unistd.h>
#include <pthread.h>
#include <sched.h>
#include <sys/time.h>
#include <sys/syscall.h>
#include <sys/ioctl.h>
#include <sys/mman.h>
#include <linux/perf_event.h>
#ifndef PERF_LINUX
  #define PERF_LINUX 1
#endif
#elif PERF_LINUX == 1
  #error "[ERROR] perf: missing <linux> headers!"
#else
  #define PERF_LINUX 0
#endif
#endif

#if (not defined(PERF_LLVM)) or (defined(PERF_LLVM) and PERF_LLVM == 1)
#if __has_include(<llvm/TargetParser/Host.h>) and \
    __has_include(<llvm/TargetParser/Triple.h>) and \
    __has_include(<llvm/Support/TargetSelect.h>) and \
    __has_include(<llvm/MC/MCDisassembler/MCDisassembler.h>) and \
    __has_include(<llvm/DebugInfo/Symbolize/Symbolize.h>) and \
    __has_include(<llvm/MC/MCInstPrinter.h>) and \
    __has_include(<llvm/MC/MCTargetOptions.h>) and \
    __has_include(<llvm/MC/TargetRegistry.h>) and \
    __has_include(<llvm/MC/MCContext.h>) and \
    __has_include(<llvm/MC/MCAsmInfo.h>) and \
    __has_include(<llvm/MC/MCInstrInfo.h>) and \
    __has_include(<llvm/MC/MCRegisterInfo.h>) and \
    __has_include(<llvm/MC/MCSubtargetInfo.h>) and \
    __has_include(<llvm/MCA/View.h>) and \
    __has_include(<llvm/MCA/Pipeline.h>) and \
    __has_include(<llvm/MCA/CodeEmitter.h>) and \
    __has_include(<llvm/MCA/Context.h>) and \
    __has_include(<llvm/MCA/InstrBuilder.h>) and \
    __has_include(<llvm/MCA/Stages/EntryStage.h>) and \
    __has_include(<llvm/MCA/Stages/InstructionTables.h>) and \
    __has_include(<llvm/Support/MemoryBuffer.h>) and \
    __has_include(<llvm/Support/FileSystem.h>) and \
    __has_include(<llvm/Remarks/Remark.h>) and \
    __has_include(<llvm/Remarks/RemarkParser.h>)
#include <llvm/TargetParser/Host.h>
#include <llvm/TargetParser/Triple.h>
#include <llvm/Support/TargetSelect.h>
#include <llvm/MC/MCDisassembler/MCDisassembler.h>
#include <llvm/MC/MCInstPrinter.h>
#include <llvm/MC/MCTargetOptions.h>
#include <llvm/MC/TargetRegistry.h>
#include <llvm/MC/MCContext.h>
#include <llvm/MC/MCAsmInfo.h>
#include <llvm/MC/MCInstrInfo.h>
#include <llvm/MC/MCRegisterInfo.h>
#include <llvm/MC/MCSubtargetInfo.h>
#include <llvm/MCA/View.h>
#include <llvm/MCA/Pipeline.h>
#include <llvm/MCA/CodeEmitter.h>
#include <llvm/MCA/Context.h>
#include <llvm/MCA/InstrBuilder.h>
#include <llvm/MCA/Stages/EntryStage.h>
#include <llvm/MCA/Stages/InstructionTables.h>
#include <llvm/DebugInfo/Symbolize/Symbolize.h>
#include <llvm/Support/MemoryBuffer.h>
#include <llvm/Support/FileSystem.h>
#include <llvm/Remarks/Remark.h>
#include <llvm/Remarks/RemarkParser.h>
#ifndef PERF_LLVM
  #define PERF_LLVM 1
#endif
#elif PERF_LLVM == 1
  #error "[ERROR] perf: missing <llvm> headers!"
#else
  #define PERF_LLVM 0
#endif
#endif

#if (not defined(PERF_INTEL)) or (defined(PERF_INTEL) and PERF_INTEL == 1)
#if __has_include(<intel-pt.h>)
#include <intel-pt.h>
#ifndef PERF_INTEL
  #define PERF_INTEL 1
#endif
#elif PERF_INTEL == 1
  #error "[ERROR] perf: missing <intel-pt.h> header!"
#else
  #define PERF_INTEL 0
#endif
#endif

#if __has_include(<valgrind/callgrind.h>)
#include <valgrind/callgrind.h>
#endif

#if __has_include(<xray/xray_interface.h>) and \
    __has_include(<xray/xray_log_interface.h>)
#include <xray/xray_interface.h>
#include <xray/xray_log_interface.h>
#endif

#if __has_include(<fcntl.h>) and __has_include(<unistd.h>)
#include <fcntl.h>
#include <unistd.h>
#endif

#if __has_include(<gperftools/profiler.h>)
#include <gperftools/profiler.h>
#endif

#if __has_include(<ittnotify.h>)
#include <ittnotify.h>
#endif

#include <cstdint>
#include <cstring>

#include <bit>
#include <atomic>
#include <barrier>
#include <thread>
#include <chrono>
#include <random>
#include <utility>
#include <memory>
#include <type_traits>

#include <string>
#include <string_view>
#include <iostream>
#include <ostream>
#include <fstream>
#include <format>
#include <source_location>
#include <filesystem>

#include <array>
#include <vector>
#include <list>
#include <map>
#include <span>
#include <unordered_map>
#include <tuple>
#include <optional>
#include <variant>
#include <any>

#include <numeric>
#include <algorithm>

#define PERF_NS(MAJOR, MINOR, PATCH) v##MAJOR##_##MINOR##_##PATCH
#define PERF_EVAL(...) __VA_ARGS__
namespace perf::inline PERF_EVAL(PERF_NS PERF) {
#undef PERF_EVAL
#undef PERF_NS

template<class... Ts>
inline constexpr void log(std::ostream& os, std::format_string<Ts...> fmt, Ts&&... ts) {
  os << std::format(fmt, std::forward<Ts>(ts)...);
}

template<class... Ts>
inline constexpr void log(std::format_string<Ts...> fmt, Ts&&... ts) {
  log(std::clog, fmt, std::forward<Ts>(ts)...);
}

inline constexpr void log(const auto&... ts) requires (requires { ts.first; ts.second; } and ...) {
  //todo just log and formatter via struct
  auto max_info_len = 0ul, max_desc_len = 0ul;

  ([&] {
    const auto& [desc, info] = ts;
    max_desc_len = std::max(max_desc_len, std::format("{}", desc).size());
    max_info_len = std::max(max_info_len, std::format("{}", info).size());
  }(), ...);

  log("{:<{}}{}{:<{}}{}\n", "name", max_desc_len, ' ', "info", max_info_len, ' ');
  log("{:-<{}}{}{:-<{}}{}\n", "", max_desc_len, ' ', "", max_info_len, ' ');

  ([&] {
    const auto& [desc, info] = ts;
    log("{:<{}}{}{}{}\n", desc, max_desc_len, ' ', info, ' ');
  }(), ...);

  log("\n");
};

inline namespace utility {
  /**
   * verifies whether condition is satisfied - aborts if not
   */
  inline constexpr auto verify =
    []<class TMsg = std::string_view>(const std::same_as<bool> auto cond, const TMsg& msg = {},
      const std::source_location location = std::source_location::current())
      requires requires (std::ostream& os) { os << msg; } {
    if (cond) [[likely]] return;
    log(std::cerr, "{}:{}:{}\n", location.file_name(), location.line(), msg);
    std::abort();
  };

  /**
   * std::basic_fixed_string # non standard compliant
   * - https://wg21.link/p3094
   */
  template<std::size_t N, class T = char>
  struct fixed_string {
    constexpr fixed_string() = default;
    constexpr explicit(false) fixed_string(const T (&str)[N]) { std::copy_n(str, N, data_); }
    [[nodiscard]] constexpr auto operator<=>(const fixed_string&) const = default;
    [[nodiscard]] constexpr const auto& operator[](std::size_t i) const { return data_[i]; }
    [[nodiscard]] constexpr auto& operator[](std::size_t i) { return data_[i]; }
    [[nodiscard]] constexpr auto data() const { return data_; }
    [[nodiscard]] static constexpr auto size() { return N - 1u; }
    [[nodiscard]] constexpr operator std::string_view() const { return {data(), size()}; }
    T data_[N]{}; /// null terminated
  };

  /**
   * named type with compile-time name
   */
  template<fixed_string Name, class T>
    requires (std::is_class_v<T> and not std::is_final_v<T>)
  struct fixed_named : T {
    using underlying_type = T;
    constexpr explicit fixed_named(auto&&... ts) : T(std::forward<decltype(ts)>(ts)...) { }
    [[nodiscard]] static constexpr decltype(auto) name() {
      static constexpr auto name = Name;
      return (name);
    }
  };

  /**
   * named type
   */
  template<class TName, class T>
    requires (std::is_class_v<T> and not std::is_final_v<T>)
  struct named : T {
    using underlying_type = T;

    constexpr explicit named(TName name, const T& t)
      : T{t}, name_(std::move(name))
    { }

    [[nodiscard]] constexpr decltype(auto) name() const {
      return name_;
    }

   private:
    TName name_{};
  };

  template<fixed_string Name, class T>
  [[nodiscard]] inline constexpr auto make_fixed_named(T&& t)
    -> fixed_named<Name, std::remove_cvref_t<T>> {
    return fixed_named<Name, std::remove_cvref_t<T>>{std::forward<T>(t)};
  }

  template<class T, auto... Vs>
    requires (std::is_class_v<T> and not std::is_final_v<T>)
  struct initialized : T {
    constexpr initialized() : T{T::name(), init<Vs>{}...} {
      if not consteval {
        static_cast<T&>(*this) = T{T::name(), init<Vs>{}...};
      }
    }

   private:
    template<auto Value> struct init {
      template<class R>
      [[nodiscard]] constexpr operator R() const {
        if consteval {
          if constexpr (requires { std::integral_constant<R, Value>{}; }) {
            return Value;
          } else {
            return {};
          }
        } else {
          return Value;
        }
      }
    };
  };

  /**
   * scoped utility { .on_entry = constructor, .on_exit = destructor }
   */
  template<class TOnEntry = decltype([]{}), class TOnExit = decltype([]{})>
  struct scoped final {
    TOnEntry on_entry{};
    TOnExit on_exit{};

    struct _on_entry {
      constexpr explicit _on_entry(const auto& on_entry) {
        on_entry();
      }
    } _on_entry{on_entry};

    constexpr ~scoped() noexcept {
      on_exit();
    }
  };
} // namespace utility

namespace mp {
  /**
   * list of types
   */
  template<class...> struct type_list { };

  /**
   * unique type id
   */
  template<class...>
  inline constexpr auto type_id = +[]{};

  /**
   * size of type list
   */
  template<class...>
  inline constexpr auto size = 0u;

  template<template<class...> class T, class... Ts>
  inline constexpr auto size<T<Ts...>> = sizeof...(Ts);

  /**
   * unrolls fn N times # optionally passes index I to fn
   */
  template<std::size_t N>
  inline constexpr auto unroll = [](auto&& fn) {
    const auto invoke = [&]<std::size_t I> {
      if constexpr (requires { fn.template operator()<std::size_t{}>(); }) {
        fn.template operator()<I>();
      } else {
        fn();
      }
    };
    [&]<std::size_t... Ns>(std::index_sequence<Ns...>) {
      (invoke.template operator()<Ns>(), ...);
    }(std::make_index_sequence<N>{});
  };

  /**
   * for each element in the range
   */
  template<std::ranges::range auto Rng>
  inline constexpr auto for_each = [](auto&& fn) {
    [&]<std::size_t... Ns>(std::index_sequence<Ns...>) {
      (fn.template operator()<Rng[Ns]>(), ...);
    }(std::make_index_sequence<Rng.size()>{});
  };

  /**
   * value-based meta-programming # non-standard compliant implementation (gcc, clang, msvc)
   * - https://wg21.link/p2996
   */
  #if not __cpp_reflection
  namespace detail {
    template<auto...> struct id { constexpr auto friend get(id); };
    template<class T, auto... Tags>
    struct type { constexpr auto friend get(id<Tags...>) { return std::type_identity<T>{}; } };
  } // namespace detail

  /**
   * meta id # registers type and returns its id
   */
  template<class T, auto Id = type_id<T>, auto... Tags>
  inline constexpr auto meta = (detail::type<T, Id, Tags...>{}, Id);

  /**
   * meta info # same underlying type
   */
  using info = std::remove_cvref_t<decltype(meta<void>)>;

  /**
   * returns type of meta id
   */
  template<auto Info, auto... Tags>
  using type_of = typename decltype(get(detail::id<Info, Tags...>{}))::type;

  /**
   * applies type_of to given range
   */
  template<template<class...> class T, std::ranges::range auto Rng, auto... Tags>
  [[nodiscard]] inline constexpr auto apply() {
    return []<std::size_t... Ns>(std::index_sequence<Ns...>) {
      return T<type_of<Rng[Ns], Tags...>...>{};
    }(std::make_index_sequence<Rng.size()>{});
  }

  /**
   * applies type_of to given range
   */
  template<template<class...> class T, std::ranges::range auto Rng, auto... Tags>
  using apply_t = decltype(apply<T, Rng, Tags...>());

  /**
   * filters type list based on Fn
   */
  template<auto Fn, template<class...> class T, auto... Tags>
  inline constexpr auto filter = []<template<class...> class TList, class... Ts>(TList<Ts...>) {
    constexpr auto filtered = [] {
      struct {
        std::array<info, sizeof...(Ts)> data{};
        std::size_t size{};
      } v{};
      ([&] {
        if (Fn.template operator()<Ts>() and
          std::find(v.data.begin(), v.data.end(), meta<Ts>) == v.data.end()) {
          v.data[v.size++] = meta<Ts>;
        }
      }(), ...);
      return v;
    }();
    return [&]<std::size_t... Ns>(std::index_sequence<Ns...>) {
      if constexpr (requires { T{type_of<filtered.data[Ns], Tags...>{}...}; }) {
        return T{type_of<filtered.data[Ns], Tags...>{}...};
      } else if constexpr (requires { T<type_of<filtered.data[Ns], Tags...>...>{}; }) {
        return T<type_of<filtered.data[Ns], Tags...>...>{};
      }
    }(std::make_index_sequence<filtered.size>{});
  };

  /**
   * translation unit counter // increments counter by 1 starting from N [default: 0]
   */
  template<auto Tag = type_id<void>, auto N = 0ul, auto = []{}>
  [[nodiscard]] static constexpr auto tick() {
    if constexpr (requires { get(detail::id<N, Tag>{}); }) {
      return tick<Tag, N + 1u>();
    } else {
      return N;
    }
  }
  #endif // __cpp_reflection

  /**
   * lambda overloading
   */
  template<class... Ts> struct overload : Ts... { using Ts::operator()...; };
  template<class... Ts> overload(Ts...) -> overload<Ts...>;

  /**
   * function/lambda traits
   */
  template<class...> struct function_traits;
  template<class T> requires requires { &T::operator(); }
  struct function_traits<T>
    : function_traits<decltype(&T::operator())>
  { };
  template<class T> requires requires { &T::template operator()<>; }
  struct function_traits<T>
    : function_traits<decltype(&T::template operator()<>)>
  { };
  template<class R, class... Ts>
  struct function_traits<R (Ts...)> {
    using result_type = R;
    using args_type = type_list<Ts...>;
  };
  template<class R, class... Ts>
  struct function_traits<R (Ts...) noexcept> {
    using result_type = R;
    using args_type = type_list<Ts...>;
  };
  template<class R, class... Ts>
  struct function_traits<R (*)(Ts...)> {
    using result_type = R;
    using args_type = type_list<Ts...>;
  };
  template<class R, class... Ts>
  struct function_traits<R (*)(Ts...) noexcept> {
    using result_type = R;
    using args_type = type_list<Ts...>;
  };
  template<class R, class T, class... Ts>
  struct function_traits<R (T::*)(Ts...) const> {
    using result_type = R;
    using args_type = type_list<Ts...>;
  };
  template<class R, class T, class... Ts>
  struct function_traits<R (T::*)(Ts...) const noexcept> {
    using result_type = R;
    using args_type = type_list<Ts...>;
  };
  template<class R, class T, class... Ts>
  struct function_traits<R (T::*)(Ts...)> {
    using result_type = R;
    using args_type = type_list<Ts...>;
  };
  template<class R, class T, class... Ts>
  struct function_traits<R (T::*)(Ts...) noexcept> {
    using result_type = R;
    using args_type = type_list<Ts...>;
  };
} // namespace meta

namespace info {
  /**
   * https://semver.org
   */
  struct sem_ver {
    std::size_t major{};
    std::size_t minor{};
    std::size_t patch{};
  };
  inline constexpr auto version = [] {
    return sem_ver PERF;
  };

  namespace compiler {
    inline constexpr auto name = [] -> std::string_view {
      #if defined(__clang__)
      return "clang";
      #elif defined(__GNUC__)
      return "gcc";
      #elif defined(_MSC_VER)
      return "msvc";
      #else
      return "unknown";
      #endif
    };

    inline constexpr auto version = [] {
      #if defined(__clang__)
      return sem_ver{__clang_major__, __clang_minor__, __clang_patchlevel__};
      #elif defined(__GNUC__)
      return sem_ver{__GNUC__, __GNUC_MINOR__, __GNUC_PATCHLEVEL__};
      #elif defined(_MSC_VER)
      return sem_ver{_MSC_VER / 100, _MSC_VER % 100};
      #else
      return sem_ver{};
      #endif
    };
  } // namespace compiler

  /**
   * https://en.wikichip.org/wiki/intel/cpuid
   */
  namespace cpu {
    #if PERF_LINUX == 1
    inline constexpr auto name = [] -> std::string {
      std::array<char, 48u + 1u> name{};
      std::array<unsigned int, 4u> info{};
      std::size_t n{};
      for (auto i = 0u; i < name.size() - 1u; i += 16u) {
        __get_cpuid(0x80000002 + n++, &info[0], &info[1], &info[2], &info[3]);
        std::memcpy(name.data() + i, info.data(), sizeof(info));
      }
      /// ex. '12th Gen Intel(R) Core(TM) i7-12650H'
      const std::size_t size = std::distance(name.begin(), std::find(name.begin(), name.end(), 0)) - 1u;
      verify(size < name.size());
      return {name.data(), size};
    };
    #endif // PERF_LINUX

    #if PERF_LLVM == 1
    inline constexpr auto code_name = [] {
      return ::llvm::sys::getHostCPUName().str(); // ex. 'skylake', 'alderlake', ...
    };
    #endif // PERF_LLVM

    #if PERF_LINUX == 1
    struct cpu_ver {
      std::uint16_t family{};
      std::uint8_t model{};
      std::uint8_t stepping{};
    };
    inline constexpr auto version = [] {
      unsigned int eax{}, ebx{}, ecx{}, edx{};
      __get_cpuid(1u, &eax, &ebx, &ecx, &edx);

      const auto family = (eax >> 8u) & 0xF;
      const auto extended_family = (eax >> 20) & 0xFF;
      const auto model = (eax >> 4u) & 0xF;
      const auto extended_model = (eax >> 16u) & 0xF;

      return cpu_ver{
        .family = std::uint16_t(family == 0xF ? family + extended_family : family),
        .model = std::uint8_t(family == 0x6 or family == 0xF ? model + (extended_model << 4u) : model),
        .stepping = std::uint8_t(eax & 0xF),
      };
    };
    #endif // PERF_LLVM

    #if PERF_LLVM == 1
    //inline constexpr auto dispatch_width = []<class TLLVM = llvm<arch>>(TLLVM&& llvm = TLLVM{})
    inline constexpr auto dispatch_width = [](auto& llvm) {
      return llvm.subtarget_info->getSchedModel().IssueWidth;
    };

    inline constexpr auto features = [] {
      auto&& features = []<class TFeatures = ::llvm::StringMap<bool>>(auto&&... ts) {
        if constexpr (requires (TFeatures features) { ::llvm::sys::getHostCPUFeatures(features); }) {
          TFeatures features{};
          ::llvm::sys::getHostCPUFeatures(features);
          return features;
        } else if constexpr (requires { ::llvm::sys::getHostCPUFeatures(ts...); }) {
          return ::llvm::sys::getHostCPUFeatures(ts...);
        }
      }();
      std::vector<std::string> supported{};
      supported.reserve(features.size());
      for (const auto& [name, support] : features) {
        if (support) {
          supported.push_back(std::string(name));
        }
      }
      return supported; // { avx, avx2, bmi2, ... }
    };
    #endif // PERF_LLVM
  } // namespace cpu

  namespace memory {
    enum class cache {
      L1 = 1,
      L2 = 2,
      L3 = 3,
    };

    struct info {
      std::size_t size{};
      std::size_t line_size{};
      std::size_t assoc{};
    };

    #if PERF_LINUX == 1
    inline constexpr auto icache = [] {
      return std::map<cache, info>{
        {
          cache::L1, {
            std::size_t(sysconf(_SC_LEVEL1_ICACHE_SIZE)),
            std::size_t(sysconf(_SC_LEVEL1_ICACHE_LINESIZE)),
            std::size_t(sysconf(_SC_LEVEL1_ICACHE_ASSOC)),
          }
        },
      };
    };

    inline constexpr auto dcache = [] {
      return std::map<cache, info>{
        {
          cache::L1, {
            std::size_t(sysconf(_SC_LEVEL1_DCACHE_SIZE)),
            std::size_t(sysconf(_SC_LEVEL1_DCACHE_LINESIZE)),
            std::size_t(sysconf(_SC_LEVEL1_DCACHE_ASSOC)),
          }
        },
        {
          cache::L2, {
            std::size_t(sysconf(_SC_LEVEL2_CACHE_SIZE)),
            std::size_t(sysconf(_SC_LEVEL2_CACHE_LINESIZE)),
            std::size_t(sysconf(_SC_LEVEL2_CACHE_ASSOC)),
          }
        },
        {
          cache::L3, {
            std::size_t(sysconf(_SC_LEVEL3_CACHE_SIZE)),
            std::size_t(sysconf(_SC_LEVEL3_CACHE_LINESIZE)),
            std::size_t(sysconf(_SC_LEVEL3_CACHE_ASSOC)),
          }
        },
      };
    };
    #endif // PERF_LINUX
  } // namespace memory

  namespace sys {
    inline constexpr auto name = [] -> std::string_view {
      #if defined(__linux__)
        return "linux";
      #elif defined(__unix__)
        return "unix";
      #elif defined(__FreeBSD__)
        return "freebsd";
      #elif defined(__APPLE__) or defined(__MACH__)
        return "macos";
      #elif defined(_WIN32) or defined(_WIN64)
        return "windows";
      #else
        return "unknown";
      #endif
    };

    #if PERF_LLVM == 1
    inline constexpr auto triple = [] {
      return ::llvm::Triple::normalize(::llvm::sys::getDefaultTargetTriple()); // ex. `x86_64-pc-linux-gnu`
    };
    #endif // PERF_LLVM

    #if PERF_LINUX == 1
    inline constexpr auto page_size = [] {
      return sysconf(_SC_PAGESIZE); // ex. `4096` [kb]
    };

    /**
     * loads hw info form the given path and parse it based on the fmt
     */
    template<fixed_string Fmt, class T = std::size_t>
    inline constexpr auto hw = [](const auto& path) {
      static constexpr auto size = [] {
        const auto str = std::string_view(Fmt);
        return 1u + std::count(str.begin(), str.end(), ',');
      }();

      std::ifstream file{path};
      if (not file.good()) {
        return std::optional<std::array<T, size>>{};
      }
      std::string line{};
      std::getline(file, line);
      std::array<std::size_t, size> fields{};
      [&]<std::size_t... Ns>(std::index_sequence<Ns...>) {
        verify(std::sscanf(line.c_str(), std::string(Fmt).c_str(), &fields[Ns]...) == size);
      }(std::make_index_sequence<size>{});

      return std::optional{fields};
    };
    #endif // PERF_LINUX
  } // namespace sys

  namespace proc::inline self {
    #if PERF_LINUX == 1
    inline constexpr auto name = [] {
      return std::filesystem::canonical("/proc/self/exe"); // ex. `/tmp/a.out` # full path
    };

    inline constexpr auto base_address = [] {
      std::ifstream maps{"/proc/self/maps"};
      verify(maps.good());
      std::string line{};
      std::getline(maps, line);
      return std::stoull(line.substr(0, line.find('-')), nullptr, 16); // ex. `instruction_pointer - base_address()`
    };
    #endif // PERF_LINUX
  } // namespace proc::self

  namespace bin {
    #if PERF_LLVM == 1
    /**
     * Converts address into a file name:line number
     */
    inline constexpr auto addr_to_line = []<class TSymbolizer = ::llvm::symbolize::LLVMSymbolizer>(
      const std::string& executable, const std::uint64_t address, TSymbolizer&& symbolizer = {})
      -> std::optional<std::string> requires requires { symbolizer.symbolizeCode(executable, {address}); } {
      std::string line{};
      if (auto&& info = symbolizer.symbolizeCode(executable, {address}); info and info->Line) {
        std::ifstream file{info->FileName};
        verify(file.good());
        auto n = info->Line;
        while (n-- and std::getline(file, line));
        return line;
      }
      return {};
    };

    /**
     * Converts address into a function name
     */
    inline constexpr auto addr_to_fn_name = []<class TSymbolizer = ::llvm::symbolize::LLVMSymbolizer>(
      const std::string& executable, const std::uint64_t address, TSymbolizer&& symbolizer = {})
      -> std::optional<std::string> requires requires { symbolizer.symbolizeCode(executable, {address}); } {
      if (auto&& info = symbolizer.symbolizeCode(executable, {address}); info and not info->FunctionName.empty()) {
        return info->FunctionName;
      }
      return {};
    };

    /**
     * Converts address into a name
     */
    inline constexpr auto addr_to_name = []<class TSymbolizer = ::llvm::symbolize::LLVMSymbolizer>(
      const std::string& executable, const std::uint64_t address, TSymbolizer&& symbolizer = {})
      -> std::optional<std::string> requires requires { symbolizer.symbolizeData(executable, {address}); } {
      if (auto&& info = symbolizer.symbolizeData(executable, {address}); info and not info->Name.empty()) {
        return info->Name;
      }
      return {};
    };
    #endif // PERF_LLVM
  } // namespace bin
} // namespace info

inline namespace core {
  namespace code {
    #if PERF_GNU == 1
    /**
     * gnu version of `[[clang::code_align(alignment)]]`
     * @param: alignment has to be compile-time constant
     */
    template<std::size_t Alignment>
      requires (std::has_single_bit(Alignment))
    inline constexpr auto align = [] {
      asm volatile(".align %c0" : : "i"(Alignment));
    };

    /**
     * compile-time code label # ld: `__start_labels`, `__stop_labels`
     */
    template<auto Label>
    inline constexpr auto label = [] {
      asm volatile goto(
        ".pushsection labels, \"aw\" \n"
        ".quad %c0, %l[L]\n" /// .quad %c0, 0b # can be reordered
        ".popsection \n"
        : : "i"(Label) : "memory" : L
      ); L:;
    };

    /**
     * loads compile-time code labels # written by `perf::code::label`
     */
    extern "C" std::pair<std::uint64_t, const std::uint8_t*> __start_labels [[gnu::section("labels")]] [[gnu::weak]];
    extern "C" std::pair<std::uint64_t, const std::uint8_t*> __stop_labels [[gnu::section("labels")]] [[gnu::weak]];
    inline const struct labels : std::unordered_map<std::uint64_t, const std::uint8_t*> {
      using std::unordered_map<std::uint64_t, const std::uint8_t*>::unordered_map;
      [[nodiscard]] constexpr decltype(auto) operator[](const auto key) const
        requires requires { std::uint64_t(key); } {
        return at(std::uint64_t(key));
      }
    } labels{&__start_labels, &__stop_labels};
    #endif // PERF_GNU
  } // namespace code

  namespace compiler {
    /**
     * prevents memory instructions reorder
     */
    inline constexpr auto prevent_reorder = [](std::memory_order order) {
      std::atomic_signal_fence(order);
    };

    #if PERF_GNU == 1
    /**
     * prevents elision of given value
     */
    inline constexpr auto prevent_elision = [](auto&& t) -> decltype(auto) {
      if constexpr (std::is_pointer_v<std::remove_cvref_t<decltype(t)>>) {
        /* ret */ asm volatile("" :: "g"(t) : "memory");
      } else {
        #if defined(__clang__)
        /* ret */ asm volatile("" :: "r,m"(t) : "memory");
        #else
        /* ret */ asm volatile("" :: "m,r"(t) : "memory");
        #endif
      }
      return (t);
    };

    /**
     * returns true if given function is elided by the compiler, false otherwise
     */
    template<auto Begin = +[]{}, auto End = +[]{}>
    [[nodiscard]] inline constexpr auto is_elided(auto&& fn) {
      const auto invoke = [&] {
        code::label<Begin>();
        fn();
        code::label<End>();
      };
      const auto ptr = &decltype(invoke)::operator();
      compiler::prevent_elision(&ptr);
      return code::labels[Begin] == code::labels[End];
    }
    #endif // PERF_GNU
  } // namespace compiler

  namespace cpu::pipeline {
    #if PERF_GNU == 1 and defined(__x86_64__)
    inline constexpr auto flush = [] {
      asm volatile("cpuid" : : "a"(0), "c"(0) : "ebx", "edx", "memory");
    };
    #endif // PERF_GNU and __x86_64__
  } // namespace cpu::pipeline

  namespace memory {
    /**
     * aligns given pointer to specific alignment and offset
     */
    inline constexpr auto align =
      []<class T>(T* addr, const std::size_t alignment, std::size_t offset = {}) {
        verify(std::has_single_bit(alignment));
        const auto original = reinterpret_cast<std::uintptr_t>(addr);
        const auto aligned = (original + alignment - 1u) & ~(alignment - 1u);
        return reinterpret_cast<T*>(aligned + offset);
      };

    /**
     * returns true if given pointer is aligned to specified alignment
     */
    inline constexpr auto is_aligned =
      [](const auto* addr, const std::size_t alignment = alignof(std::max_align_t)) {
        verify(std::has_single_bit(alignment));
        return not (reinterpret_cast<std::uintptr_t>(addr) & (alignment - 1u));
      };

    /**
     * hardware memory barrier (x86: fence or lock)
     */
    inline constexpr auto fence(const std::memory_order memory_order) {
      std::atomic_thread_fence(memory_order);
    }

    /**
     * forces hardware memory barrier instruction (x86: mfence, sfence, lfence)
     */
    #if PERF_GNU == 1 and defined(__x86_64__)
    template<std::memory_order MemoryOrder>
    inline constexpr auto fence() {
      if constexpr (MemoryOrder == std::memory_order_acquire) {
        asm volatile("lfence" ::: "memory");
      } else if constexpr (MemoryOrder == std::memory_order_release) {
        asm volatile("sfence" ::: "memory");
      } else if constexpr (MemoryOrder == std::memory_order_acq_rel or
          MemoryOrder == std::memory_order_seq_cst) {
        asm volatile("mfence" ::: "memory");
      }
    }
    #endif // PERF_GNU and __x86_64__

    /**
     * https://gcc.gnu.org/onlinedocs/gcc/Other-Builtins.html
     */
    #if __has_builtin(__builtin_prefetch)
    enum class operation {
      read,   /// prepare the prefetch for a read
      write,  /// prepare the prefetch for a write to the memory
    };

    enum class locality {
      none,     /// the data can be removed from the cache after the access # streaming access
      low,      /// L3 cache, leave the data in the L3 cache level after the access
      moderate, /// L2 cache, leave the data in L2 and L3 cache levels after the access
      high,     /// L1 cache, leave the data in the L1, L2, and L3 cache levels after the access
    };

    /**
     * Prefetches memory from address
     */
    template<enum operation operation, enum locality locality>
    inline constexpr auto prefetch = [](const auto* addr) {
      __builtin_prefetch(
        addr,
        static_cast<std::underlying_type_t<enum operation>>(operation),
        static_cast<std::underlying_type_t<enum locality>>(locality)
      );
    };
    #endif

    #if PERF_LINUX == 1
    enum class prot {
      read  = PROT_READ,
      write = PROT_WRITE,
      exec  = PROT_EXEC,
    };
    inline constexpr prot operator|(const prot lhs, const prot rhs) {
      return static_cast<prot>(
        static_cast<std::underlying_type_t<prot>>(lhs) |
        static_cast<std::underlying_type_t<prot>>(rhs)
      );
    }

    inline constexpr auto protect = [](std::span<const std::uint8_t> data, const enum prot prot) {
      const auto page_size = info::sys::page_size();
      const auto offset = std::uint64_t(data.data()) % page_size;
      const auto addr = std::uint64_t(data.data()) - offset;
      return not mprotect(
        reinterpret_cast<void*>(addr),
        (((data.size() + offset) - 1u) / page_size + 1u) * page_size,
        static_cast<std::underlying_type_t<enum prot>>(prot)
      );
    };
    #endif // PERF_LINUX

    namespace cache {
      inline constexpr auto flush = [](std::span<const std::uint8_t> data) {
        const auto size = data.size();
        const auto cache_line_size = info::memory::dcache()[info::memory::cache::L1].line_size;
        const auto ptr = reinterpret_cast<std::uintptr_t>(data.begin());
        const auto aligned_start = ptr & ~(cache_line_size - 1u);
        const auto aligned_end = (ptr + size + cache_line_size - 1u) & ~(cache_line_size - 1u);
        const auto aligned_size = aligned_end - aligned_start;

        #if defined(__GNUC__) && defined(__x86_64__)
        for (auto i = 0u; i < aligned_size; i += cache_line_size) {
          asm volatile("clflush (%0)" :: "r"(reinterpret_cast<const void*>(aligned_start + i)));
        }
        #elif __has_builtin(__builtin___clear_cache)
        __builtin___clear_cache(
          reinterpret_cast<const std::uint8_t*>(aligned_start),
          reinterpret_cast<const std::uint8_t*>(aligned_start + aligned_size)
        );
        #endif
      };
    } // namespace heap

    namespace heap {
      /**
       * pollutes heap by making n allocations
       */
      template<class T = std::size_t>
      inline constexpr auto pollute(const std::size_t size)
        requires (requires { T{}; } or requires { T(size); }) {
        verify(size > 0u);
        std::list<T> data{};
        auto n = size;
        while (n--) {
          if constexpr (requires { T(n); }) {
            data.push_back(T(n));
          } else {
            data.push_back(T{});
          }
        }
      }
    } // namespace heap
  } // namespace memory

  inline namespace sys {
    namespace thread {
      #if PERF_LINUX == 1
      namespace affinity {
        /**
         * Set affinity on thread
         */
        inline constexpr auto set =
          [](const std::thread::native_handle_type thread, const std::ranges::range auto& affinities) {
            cpu_set_t cpuset{};
            CPU_ZERO(&cpuset);
            for (const auto affinity : affinities) {
              CPU_SET(affinity, &cpuset);
            }
            return not pthread_setaffinity_np(thread, sizeof(cpuset), &cpuset);
          };

        /**
         * Returns affinity for tgiven hread
         */
        inline constexpr auto get = [](const std::thread::native_handle_type thread) {
          std::vector<std::size_t> affinities{};
          cpu_set_t cpuset{};
          CPU_ZERO(&cpuset);
          if (pthread_getaffinity_np(thread, sizeof(cpuset), &cpuset)) {
            return affinities;
          }
          for (auto i = 0u; i < CPU_SETSIZE; ++i) {
            if (CPU_ISSET(i, &cpuset)) {
              affinities.push_back(i);
            }
          }
          return affinities;
        };
      } // namespace affinity

      namespace priority {
        inline constexpr auto set = [] {};
        inline constexpr auto get = [] {};
      } // namespace priority

      inline const std::thread::native_handle_type self = pthread_self();
      #endif
    } // namespace thread
  } // namespace sys
} // namespace core

inline namespace backend {
  template<class TArch>
  concept arch_like = requires (TArch arch) {
    arch.triple;
    arch.cpu;
    arch.features;
    arch.syntax;
  };

  #if PERF_LLVM == 1
  /**
   * llc -march=x86 -mcpu=help
   */
  struct arch {
    /// x86_64-pc-linux-gnu, arm-none-eabi, aarch64-unknown-linux-gnu, ...
    std::string triple = ::llvm::Triple::normalize(::llvm::sys::getDefaultTargetTriple());

    /// alderlake, westmere, btver2, ...
    std::string cpu = ::llvm::sys::getHostCPUName().str();

    /// +bmi2, -avx2, +avx512, ... # default: available features of the current cpu
    std::string features{};

    /// att: 'movl %eax, %ebx', intel: 'mov ebx, eax' # default: intel
    enum class syntax { att, intel } syntax = syntax::intel;

    /// overrides processor dispatch width
    std::size_t dispatch_width{};

    /// overrdies number of entries in the micro-op queue
    std::size_t micro_op_queue{};

    /// overrdies maximum throughput from the decoders (instructions per cycle)
    std::size_t decoder_throughput{};

    /// overrides maximum number of physical registers which can be used for register mappings
    std::size_t register_file_size{};

    /// overrides size of the load queue
    std::size_t load_queue_size{};

    /// overrides size of the store queue
    std::size_t store_queue_size{};

    /// loads and stores do/do not alias # default: no alias
    bool assume_no_alias = true;
  };

  /**
   * llvm
   * - https://github.com/llvm/llvm-project
   */
  template<arch_like TArch = arch>
  struct llvm {
    constexpr explicit llvm(const TArch& arch = {})
      : arch{arch} {
      if (::llvm::TargetRegistry::targets().empty()) {
        ::llvm::InitializeAllTargetInfos();
        ::llvm::InitializeAllTargetMCs();
        ::llvm::InitializeAllAsmParsers();
        ::llvm::InitializeAllDisassemblers();
        ::llvm::InitializeAllTargetMCAs();
      }

      auto&& [target, options] = [&] {
        ::llvm::MCTargetOptions options{};
        std::string error{};
        const auto target = ::llvm::TargetRegistry::lookupTarget(
          ::llvm::Triple(arch.triple).getTriple(),
          error
        );
        verify(bool(target), std::format("[ERROR] llvm: '{}'", error));
        return std::pair{target, options};
      }();

      register_info = std::unique_ptr<::llvm::MCRegisterInfo>(target->createMCRegInfo(::llvm::Triple(arch.triple).getTriple()));
      assembler_info = std::unique_ptr<::llvm::MCAsmInfo>(target->createMCAsmInfo(*register_info, ::llvm::Triple(arch.triple).getTriple(), options));
      instruction_info = std::unique_ptr<::llvm::MCInstrInfo>(target->createMCInstrInfo());
      subtarget_info = std::unique_ptr<::llvm::MCSubtargetInfo>(target->createMCSubtargetInfo(::llvm::Triple(arch.triple).getTriple(), arch.cpu, arch.features));
      instrument_manager = std::make_unique<::llvm::mca::InstrumentManager>(*subtarget_info, *instruction_info);
      instruction_analysis = std::unique_ptr<::llvm::MCInstrAnalysis>(target->createMCInstrAnalysis(instruction_info.get()));
      instruction_builder = [&]<class T = ::llvm::mca::InstrBuilder> {
        if constexpr (requires { T(*subtarget_info, *instruction_info, *register_info, instruction_analysis.get(), *instrument_manager, 0u); }) {
          return std::make_unique<T>(*subtarget_info, *instruction_info, *register_info, instruction_analysis.get(), *instrument_manager, 0u);
        } else {
          return std::make_unique<T>(*subtarget_info, *instruction_info, *register_info, instruction_analysis.get(), *instrument_manager);
        }
      }();
      instruction_printer = std::unique_ptr<::llvm::MCInstPrinter>(
        target->createMCInstPrinter(::llvm::Triple(arch.triple), std::size_t(arch.syntax), *assembler_info, *instruction_info, *register_info)
      );
      context = std::make_unique<::llvm::MCContext>(
        ::llvm::Triple(arch.triple), assembler_info.get(), register_info.get(), subtarget_info.get(), nullptr, &options
      );
      disassembler = std::unique_ptr<::llvm::MCDisassembler>(target->createMCDisassembler(*subtarget_info, *context));
    }

    TArch arch{};
    std::unique_ptr<::llvm::MCRegisterInfo> register_info{};
    std::unique_ptr<::llvm::MCAsmInfo> assembler_info{};
    std::unique_ptr<::llvm::MCInstrInfo> instruction_info{};
    std::unique_ptr<::llvm::MCSubtargetInfo> subtarget_info{};
    std::unique_ptr<::llvm::mca::InstrumentManager> instrument_manager{};
    std::unique_ptr<::llvm::MCInstrAnalysis> instruction_analysis{};
    std::unique_ptr<::llvm::mca::InstrBuilder> instruction_builder{};
    std::unique_ptr<::llvm::MCInstPrinter> instruction_printer{};
    std::unique_ptr<::llvm::MCContext> context{};
    std::unique_ptr<::llvm::MCDisassembler> disassembler{};
  };

  /**
   * disassembling # machine code
   * - https://github.com/llvm/llvm-project
   */
  namespace mc {
    template<class T>
    concept instruction_like = requires (T t) {
      t.opcode;
      t.ip;
      t.size;
    };

    struct instruction {
      std::uint64_t opcode{}; /// instruction opcode # unique but not defined
      std::uint64_t ip{};     /// instruction pointer
      std::uint64_t size{};   /// size [bytes]
    };

    template<class T>
    [[nodiscard]] inline constexpr auto disassemble(const T begin, const T end, auto& llvm) {
      verify(end >= begin);

      ::llvm::ArrayRef bytes{begin, std::size_t(end - begin)};
      std::vector<instruction> instructions{};
      instructions.reserve(end - begin);

      auto ip = std::uint64_t(begin);
      while (ip < std::uint64_t(end)) {
        ::llvm::MCInst instruction;
        std::size_t size;
        verify(llvm.disassembler->getInstruction(
          instruction,
          size,
          bytes.slice(ip - std::uint64_t(begin)),
          ip,
          ::llvm::nulls()
        ) == ::llvm::MCDisassembler::DecodeStatus::Success);

        instructions.push_back({
          .opcode = instruction.getOpcode(),
          .ip = ip,
          .size = size,
        });
        ip += size;
      }

      return instructions;
    }

    template<auto Begin = +[]{}, auto End = +[]{}>
    [[nodiscard]] inline constexpr auto disassemble(auto&& fn, auto& llvm) {
      const auto invoke = [&] [[gnu::noinline]] {
        code::label<Begin>();
        if constexpr (std::is_void_v<decltype(fn())>) {
          fn();
        } else {
          compiler::prevent_elision(fn());
        }
        code::label<End>();
      };
      const auto ptr = &decltype(invoke)::operator();
      compiler::prevent_elision(&ptr);
      return disassemble(code::labels[Begin], code::labels[End], llvm);
    }

    template<class T>
    [[nodiscard]] inline constexpr auto disassemble(const T begin, const T end, const auto& traces, auto& llvm) {
      std::map<std::uint64_t, std::remove_cvref_t<decltype(traces)>> data{};
      auto enabled = false;
      for (const auto& trace : traces) {
        if (trace.ip == std::uint64_t(begin)) {
          enabled = true;
        }
        if (trace.ip == std::uint64_t(end)) {
          enabled = false;
        }
        if (enabled) {
          data[trace.ip].push_back(trace);
        }
      }

      std::vector<instruction> instructions(data.size());
      std::size_t i{};
      for (const auto& [ip, traces] : data) {
        const auto* address = reinterpret_cast<const std::uint8_t*>(ip);
        instructions[i++] = mc::disassemble(address, address + traces[0].size, llvm)[0];
      }

      return instructions;
    }

    inline namespace info {
      struct assembly {
        using value_type = struct {
          std::string str{};
          enum arch::syntax syntax{};
        };
        [[nodiscard]] constexpr auto operator()(const auto& instruction, auto& llvm) const -> value_type
          requires requires { instruction.opcode; instruction.ip; instruction.size; } {
          ::llvm::MCInst inst{};
          std::size_t size{};
          verify(llvm.disassembler->getInstruction(
            inst,
            size,
            ::llvm::ArrayRef<std::uint8_t>{
              reinterpret_cast<const std::uint8_t*>(instruction.ip),
              reinterpret_cast<const std::uint8_t*>(instruction.ip) + instruction.size
            },
            instruction.ip,
            ::llvm::nulls()
          ) == ::llvm::MCDisassembler::DecodeStatus::Success);

          std::string str{};
          ::llvm::raw_string_ostream stream{str};
          llvm.instruction_printer->printInst(
            &inst, instruction.ip, {}, *llvm.subtarget_info, stream
          );

          return {str, llvm.arch.syntax};
        }
      };

      struct address {
        using value_type = struct {
          std::uint64_t ip{};
          std::uint64_t size{};
        };
        [[nodiscard]] constexpr auto operator()(const auto& instruction, auto&&...) const -> value_type
          requires requires { instruction.ip; instruction.size; } {
          return {instruction.ip, instruction.size};
        }
      };

      struct encoding {
        using value_type = struct {
          std::uint64_t ip{};
          std::uint64_t size{};
        };
        [[nodiscard]] constexpr auto operator()(const auto& instruction, auto&&...) const -> value_type
          requires requires { instruction.ip; instruction.size; } {
          return {instruction.ip, instruction.size};
        }
      };

      struct size {
        using value_type = struct { std::uint64_t value{}; };
        [[nodiscard]] constexpr auto operator()(const auto& instruction, auto&&...) const -> value_type
          requires requires { instruction.size; } {
          return {instruction.size};
        }
      };

      struct uops {
        using value_type = struct { std::uint64_t value{}; };
        [[nodiscard]] constexpr auto operator()(const auto& instruction, auto& llvm) const -> value_type
          requires requires { instruction.opcode; } {
          const auto& mc = llvm.instruction_info->get(instruction.opcode);
          const auto& sc = mc.getSchedClass();
          const auto& desc = *llvm.subtarget_info->getSchedModel().getSchedClassDesc(sc);
          return {desc.NumMicroOps};
        }
      };

      struct latency {
        using value_type = struct { std::uint64_t value{}; };
        [[nodiscard]] constexpr auto operator()(const auto& instruction, auto& llvm) const -> value_type
          requires requires { instruction.opcode; } {
          const auto& mc = llvm.instruction_info->get(instruction.opcode);
          const auto& sc = mc.getSchedClass();
          const auto& desc = *llvm.subtarget_info->getSchedModel().getSchedClassDesc(sc);
          return {
            ::llvm::MCSchedModel::computeInstrLatency(*llvm.subtarget_info, desc) +
            ::llvm::MCSchedModel::getForwardingDelayCycles(llvm.subtarget_info->getReadAdvanceEntries(desc))
          };
        }
      };

      struct rthroughput {
        using value_type = struct { double value{}; };
        [[nodiscard]] constexpr auto operator()(const auto& instruction, auto& llvm) const -> value_type
          requires requires { instruction.opcode; } {
          const auto& mc = llvm.instruction_info->get(instruction.opcode);
          const auto& sc = mc.getSchedClass();
          const auto& desc = *llvm.subtarget_info->getSchedModel().getSchedClassDesc(sc);
          return {::llvm::MCSchedModel::getReciprocalThroughput(*llvm.subtarget_info, desc)};
        }
      };

      struct may_load {
        using value_type = struct { bool value{}; };
        [[nodiscard]] constexpr auto operator()(const auto& instruction, auto& llvm) const -> value_type
          requires requires { instruction.opcode; } {
          return {llvm.instruction_info->get(instruction.opcode).mayLoad()};
        }
      };

      struct may_store {
        using value_type = struct { bool value{}; };
        [[nodiscard]] constexpr auto operator()(const auto& instruction, auto& llvm) const -> value_type
          requires requires { instruction.opcode; } {
          return {llvm.instruction_info->get(instruction.opcode).mayStore()};
        }
      };

      struct has_side_effects {
        using value_type = struct { bool value{}; };
        [[nodiscard]] constexpr auto operator()(const auto& instruction, auto& llvm) const -> value_type
          requires requires { instruction.opcode; } {
          return {llvm.instruction_info->get(instruction.opcode).hasUnmodeledSideEffects()};
        }
      };

      inline constexpr auto assembly = fixed_named<"mc.assembly", struct assembly>{};
      inline constexpr auto address = fixed_named<"mc.address", struct address>{};
      inline constexpr auto encoding = fixed_named<"mc.encoding", struct encoding>{};
      inline constexpr auto size = fixed_named<"mc.size", struct size>{};
      inline constexpr auto uops = fixed_named<"mc.uops", struct uops>{};
      inline constexpr auto latency = fixed_named<"mc.latency", struct latency>{};
      inline constexpr auto rthroughput = fixed_named<"mc.rthroughput", struct rthroughput>{};
      inline constexpr auto may_load = fixed_named<"mc.may_load", struct may_load>{};
      inline constexpr auto may_store = fixed_named<"mc.may_store", struct may_store>{};
      inline constexpr auto has_side_effects = fixed_named<"mc.has_side_effects", struct has_side_effects>{};

      namespace branch {
        struct is_conditional {
          using value_type = struct { bool value{}; };
          [[nodiscard]] constexpr auto operator()(const auto& instruction, auto& llvm) const -> value_type
            requires requires { instruction.opcode; } {
            const auto& mc = llvm.instruction_info->get(instruction.opcode);
            return {mc.isConditionalBranch()};
          }
        };

        struct is_unconditional {
          using value_type = struct { bool value{}; };
          [[nodiscard]] constexpr auto operator()(const auto& instruction, auto& llvm) const -> value_type
            requires requires { instruction.opcode; } {
            const auto& mc = llvm.instruction_info->get(instruction.opcode);
            return {mc.isUnconditionalBranch()};
          }
        };

        struct is_indirect {
          using value_type = struct { bool value{}; };
          [[nodiscard]] constexpr auto operator()(const auto& instruction, auto& llvm) const -> value_type
            requires requires { instruction.opcode; } {
            const auto& mc = llvm.instruction_info->get(instruction.opcode);
            return {mc.isIndirectBranch()};
          }
        };

        struct is_call {
          using value_type = struct { bool value{}; };
          [[nodiscard]] constexpr auto operator()(const auto& instruction, auto& llvm) const -> value_type
            requires requires { instruction.opcode; } {
            const auto& mc = llvm.instruction_info->get(instruction.opcode);
            return {mc.isCall()};
          }
        };

        struct is_ret {
          using value_type = struct { bool value{}; };
          [[nodiscard]] constexpr auto operator()(const auto& instruction, auto& llvm) const -> value_type
            requires requires { instruction.opcode; } {
            const auto& mc = llvm.instruction_info->get(instruction.opcode);
            return {mc.isReturn()};
          }
        };

        inline constexpr auto is_conditional = fixed_named<"mc.branch.is_conditional", struct branch::is_conditional>{};
        inline constexpr auto is_unconditional = fixed_named<"mc.branch.is_unconditional", struct branch::is_unconditional>{};
        inline constexpr auto is_indirect = fixed_named<"mc.branch.is_indirect", struct branch::is_indirect>{};
        inline constexpr auto is_call = fixed_named<"mc.branch.is_call", struct branch::is_call>{};
        inline constexpr auto is_ret = fixed_named<"mc.branch.is_ret", struct branch::is_ret>{};
      } // namespace branch
    } // namespace info

    #if PERF_LINUX == 1
    /**
     * Instruction to source code # line (requires debug symbols `-g`)
     */
    struct source {
      using value_type = struct { std::string value{}; };
      [[nodiscard]] constexpr auto operator()(const auto& instruction, auto& llvm) const -> value_type
        requires requires { instruction.ip; } {
        if (const auto& info = perf::info::bin::addr_to_line(binary_name, instruction.ip - base_address); info) {
          const auto& line = *info;
          std::size_t i{};
          while (line[i] == ' ' or line[i] == '\t') i++;
          if (last_ != line) {
            last_ = line;
            return {line.substr(i)};
          }
        }
        return {};
      }

     private:
      mutable std::string last_{}; /// cached
      inline static const auto binary_name = perf::info::proc::self::name();
      inline static const auto base_address = perf::info::proc::self::base_address();
    };

    inline const auto source = fixed_named<"mc.source", struct source>{};
    #endif // PERF_LINUX
  } // namespace mc

  /**
   * analyzing # machine code
   * - https://llvm.org/docs/CommandGuide/llvm-mca.html
   */
  namespace mca {
    template<class T>
    concept feature_like = requires (T t, const std::vector<mc::instruction>& instructions, llvm<arch>& llvm) {
      t(instructions, llvm);
    };

    /**
     * produces a detailed report of each instruction’s state transitions through an instruction pipeline
     * - https://github.com/llvm/llvm-project/tree/main/llvm/tools/llvm-mca
     * - https://llvm.org/docs/CommandGuide/llvm-mca.html
     *
     * this class is derived from llvm project
     * - https://github.com/llvm/llvm-project/blob/main/llvm/tools/llvm-mca/Views/TimelineView.h
     * - https://github.com/llvm/llvm-project/blob/main/llvm/tools/llvm-mca/Views/TimelineView.cpp
     *
     * modifcations
     * - split business logic and view logic
     * - used dispatch_width instead of hardcoded 5 in timeline view
     * - applied api changes required for integration with the library
     * - applied C++20 changes and formatting
     */
    struct timeline {
      std::size_t iterations = 1u; /// number of iterations to simulate

      [[nodiscard]] constexpr auto operator()(const std::ranges::range auto& instructions, auto& llvm) const {
        std::vector<std::unique_ptr<::llvm::mca::Instruction>> lowered_sequence{};
        const ::llvm::SmallVector<::llvm::mca::Instrument*> instruments{};
        for (const auto& instruction : instructions) {
          ::llvm::MCInst inst{};
          std::size_t size{};
          verify(llvm.disassembler->getInstruction(
            inst,
            size,
            ::llvm::ArrayRef<std::uint8_t>{
              reinterpret_cast<const std::uint8_t*>(instruction.ip),
              reinterpret_cast<const std::uint8_t*>(instruction.ip) + instruction.size
            },
            instruction.ip,
            ::llvm::nulls()
          ) == ::llvm::MCDisassembler::DecodeStatus::Success);
          lowered_sequence.emplace_back(
            std::move(llvm.instruction_builder->createInstruction(inst, instruments).get())
          );
        }

        if (lowered_sequence.empty()) {
          return std::vector<data>{};
        }

        ::llvm::mca::CircularSourceMgr mgr{lowered_sequence, iterations};
        ::llvm::mca::CustomBehaviour custom_behavior{*llvm.subtarget_info, mgr, *llvm.instruction_info};
        ::llvm::mca::PipelineOptions options{
          llvm.arch.micro_op_queue,
          llvm.arch.decoder_throughput,
          llvm.arch.dispatch_width,
          llvm.arch.register_file_size,
          llvm.arch.load_queue_size,
          llvm.arch.store_queue_size,
          llvm.arch.assume_no_alias,
        };
        ::llvm::mca::Context mca{*llvm.register_info, *llvm.subtarget_info};
        auto pipeline = mca.createDefaultPipeline(options, mgr, custom_behavior);

        std::vector<data> v(lowered_sequence.size() * iterations, {.dispatch_width = info::cpu::dispatch_width(llvm)});
        std::size_t current_cycle{};
        view view{v, current_cycle};

        pipeline->addEventListener(&view);
        verify(bool(pipeline->run()));

        for (auto& entry: v) {
          entry.last_cycle = current_cycle;
        }

        return v;
      }

     private:
      struct data {
        std::optional<std::size_t> cycle_dispatched{};
        std::size_t cycle_ready{};
        std::size_t cycle_issued{};
        std::size_t cycle_executed{};
        std::size_t cycle_retired{};
        std::size_t dispatch_width{};
        std::size_t last_cycle{};
      };

      struct view : ::llvm::mca::View {
        constexpr explicit view(auto& timeline, auto& current_cycle)
          : timeline_{timeline}, current_cycle_{current_cycle}
        { }

        constexpr void onEvent(const ::llvm::mca::HWInstructionEvent& event) override {
          const auto index = event.IR.getSourceIndex();
          if (index >= timeline_.size()) {
            return;
          }

          switch (event.Type) {
            case ::llvm::mca::HWInstructionEvent::Retired: {
              auto &entry = timeline_[index];
              entry.cycle_retired = current_cycle_;
              verify(entry.cycle_dispatched.has_value());
              verify(*entry.cycle_dispatched <= entry.cycle_ready);
              break;
            }

            case ::llvm::mca::HWInstructionEvent::Ready: {
              timeline_[index].cycle_ready = current_cycle_;
              break;
            }

            case ::llvm::mca::HWInstructionEvent::Issued: {
              timeline_[index].cycle_issued = current_cycle_;
              break;
            }

            case ::llvm::mca::HWInstructionEvent::Executed: {
              timeline_[index].cycle_executed = current_cycle_;
              break;
            }

            case ::llvm::mca::HWInstructionEvent::Dispatched: {
              if (not timeline_[index].cycle_dispatched) {
                timeline_[index].cycle_dispatched = current_cycle_;
              }
              break;
            }
          }
        }

        constexpr void onCycleEnd() override { ++current_cycle_; }

        ::llvm::StringRef getNameAsString() const override { return {}; }
        void printView(::llvm::raw_ostream &) const override { }

       private:
        std::vector<data>& timeline_;
        std::size_t& current_cycle_;
      };
    };

    /**
     * reports the average number of resource cycles consumed every iteration by instructions
     * for every processor resource unit available on the target
     * - https://github.com/llvm/llvm-project/tree/main/llvm/tools/llvm-mca
     * - https://llvm.org/docs/CommandGuide/llvm-mca.html
     *
     * this class is derived from llvm project
     * - https://github.com/llvm/llvm-project/blob/main/llvm/tools/llvm-mca/Views/ResourcePressureView.h
     * - https://github.com/llvm/llvm-project/blob/main/llvm/tools/llvm-mca/Views/ResourcePressureView.cpp
     *
     * modifcations made by Kris Jusiak in 2025
     * - split business logic and view logic
     * - applied api changes required for integration with the library
     * - applied C++20 changes and formatting
     */
    struct resource_pressure {
      std::size_t iterations = 1u; /// number of iterations to simulate

      [[nodiscard]] constexpr auto operator()(const std::ranges::range auto& instructions, auto& llvm) const {
        std::vector<std::unique_ptr<::llvm::mca::Instruction>> lowered_sequence{};
        const ::llvm::SmallVector<::llvm::mca::Instrument*> instruments{};

        for (const auto& instruction : instructions) {
          ::llvm::MCInst inst{};
          std::size_t size{};
          verify(llvm.disassembler->getInstruction(
            inst,
            size,
            ::llvm::ArrayRef<std::uint8_t>{
              reinterpret_cast<const std::uint8_t*>(instruction.ip),
              reinterpret_cast<const std::uint8_t*>(instruction.ip) + instruction.size
            },
            instruction.ip,
            ::llvm::nulls()
          ) == ::llvm::MCDisassembler::DecodeStatus::Success);
          lowered_sequence.emplace_back(
            std::move(llvm.instruction_builder->createInstruction(inst, instruments).get())
          );
        }

        ::llvm::mca::CircularSourceMgr mgr{lowered_sequence, iterations};
        ::llvm::mca::CustomBehaviour custom_behavior{*llvm.subtarget_info, mgr, *llvm.instruction_info};
        ::llvm::mca::PipelineOptions options{
          llvm.arch.micro_op_queue,
          llvm.arch.decoder_throughput,
          llvm.arch.dispatch_width,
          llvm.arch.register_file_size,
          llvm.arch.load_queue_size,
          llvm.arch.store_queue_size,
          llvm.arch.assume_no_alias,
          false /// disable bottleneck analysis
        };
        ::llvm::mca::Context mca{*llvm.register_info, *llvm.subtarget_info};
        auto pipeline = mca.createDefaultPipeline(options, mgr, custom_behavior);

        std::vector<::llvm::mca::ReleaseAtCycles> res_usage{};
        std::size_t current_id{};
        view view{llvm.subtarget_info->getSchedModel(), res_usage, current_id, instructions.size()};

        pipeline->addEventListener(&view);
        verify(bool(pipeline->run()));

        std::vector<::llvm::MCProcResourceDesc> units{};
        const auto &sm = llvm.subtarget_info->getSchedModel();
        for (auto i = 1u; i < sm.getNumProcResourceKinds(); ++i) {
          const auto& resource = *sm.getProcResource(i);
          if (resource.SubUnitsIdxBegin or not resource.NumUnits) {
            continue;
          }
          units.push_back(resource);
        }

        const auto executions = current_id / instructions.size() + 1u;
        std::vector<data> v{};
        std::size_t i{};
        for (const auto& instruction : instructions) {
          auto& unit = v.emplace_back(std::vector<typename data::unit>(units.size())).units;
          for (auto n = 0u; n < units.size(); ++n) {
            const auto usage = res_usage[n + (i * units.size())];
            const auto pressure = usage / executions;
            unit[n] = {
              .name = units[n].Name,
              .pressure = pressure < .005 ? 0. : std::floor((pressure * 100.) + .5) / 100.
            };
          }
          ++i;
        }

        return v;
      }

     private:
      struct data {
        struct unit {
          std::string name{};
          double pressure{};
        };
        std::vector<unit> units{};
      };

      struct view : ::llvm::mca::View {
        constexpr explicit view(const auto& sm, auto& res_usage, auto& current_id, const auto instructions)
          : sm_{sm}, res_usage_{res_usage}, current_id_{current_id}, instructions_{instructions} {
          std::size_t size{};
          const auto n_res = sm_.getNumProcResourceKinds();
          for (auto i = 0u; i < n_res; ++i) {
            const auto &res = *sm_.getProcResource(i);
            if (res.SubUnitsIdxBegin or not res.NumUnits) {
              continue;
            }
            res_index.insert(std::pair{i, size});
            size += res.NumUnits;
          }

          units_size_ = size;
          res_usage_.resize(units_size_ * (instructions_ + 1u));
          std::fill(res_usage_.begin(), res_usage_.end(), .0);
        }

        constexpr void onEvent(const ::llvm::mca::HWInstructionEvent& event) override {
          if (event.Type == ::llvm::mca::HWInstructionEvent::Dispatched) {
            current_id_ = event.IR.getSourceIndex();
            return;
          }

          if (event.Type != ::llvm::mca::HWInstructionEvent::Issued) {
            return;
          }

          const auto source_index = event.IR.getSourceIndex() % instructions_;
          for (const auto& [res, usage] : static_cast<const ::llvm::mca::HWInstructionIssuedEvent&>(event).UsedResources) {
            verify(res_index.contains(res.first));
            const auto index = res_index[res.first] + ::llvm::countr_zero(res.second);
            res_usage_[index + units_size_ * source_index] += usage;
            res_usage_[index + units_size_ * instructions_] += usage;
          }
        }

        ::llvm::StringRef getNameAsString() const override { return {}; }
        void printView(::llvm::raw_ostream &) const override { }

       private:
        const ::llvm::MCSchedModel &sm_;
        std::vector<::llvm::mca::ReleaseAtCycles>& res_usage_;
        std::size_t& current_id_;
        std::size_t instructions_{};
        ::llvm::DenseMap<unsigned, unsigned> res_index{};
        std::size_t units_size_{};
      };
    };

    /**
     * correlates increases in backend pressure to dynamic dispatch stalls
     * - https://github.com/llvm/llvm-project/tree/main/llvm/tools/llvm-mca
     * - https://llvm.org/docs/CommandGuide/llvm-mca.html
     *
     * this class is derived from llvm project
     * - https://github.com/llvm/llvm-project/blob/main/llvm/tools/llvm-mca/Views/BottleneckAnalysis.h
     * - https://github.com/llvm/llvm-project/blob/main/llvm/tools/llvm-mca/Views/BottleneckAnalysis.cpp
     *
     * modifcations
     * - split business logic and view logic
     * - applied api changes required for integration with the library
     * - applied C++20 changes and formatting
     */
    struct bottleneck {
      std::size_t iterations = 1u; /// number of iterations to simulate

      [[nodiscard]] constexpr auto operator()(const std::ranges::range auto& instructions, auto& llvm) const {
        std::vector<std::unique_ptr<::llvm::mca::Instruction>> lowered_sequence{};
        const ::llvm::SmallVector<::llvm::mca::Instrument*> instruments{};

        for (const auto& instruction : instructions) {
          ::llvm::MCInst inst{};
          std::size_t size{};
          verify(llvm.disassembler->getInstruction(
            inst,
            size,
            ::llvm::ArrayRef<std::uint8_t>{
              reinterpret_cast<const std::uint8_t*>(instruction.ip),
              reinterpret_cast<const std::uint8_t*>(instruction.ip) + instruction.size
            },
            instruction.ip,
            ::llvm::nulls()
          ) == ::llvm::MCDisassembler::DecodeStatus::Success);
          lowered_sequence.emplace_back(
            std::move(llvm.instruction_builder->createInstruction(inst, instruments).get())
          );
        }

        ::llvm::mca::CircularSourceMgr mgr{lowered_sequence, iterations};
        ::llvm::mca::CustomBehaviour custom_behavior{*llvm.subtarget_info, mgr, *llvm.instruction_info};
        ::llvm::mca::PipelineOptions options{
          llvm.arch.micro_op_queue,
          llvm.arch.decoder_throughput,
          llvm.arch.dispatch_width,
          llvm.arch.register_file_size,
          llvm.arch.load_queue_size,
          llvm.arch.store_queue_size,
          llvm.arch.assume_no_alias,
          true /// enable bottleneck analysis to emit HWPressureEvents
        };
        ::llvm::mca::Context mca{*llvm.register_info, *llvm.subtarget_info};
        auto pipeline = mca.createDefaultPipeline(options, mgr, custom_behavior);

        pressure_tracker tracker{llvm.subtarget_info->getSchedModel()};
        dependency_graph graph(instructions.size() * 3u);
        back_pressure_info bpi{iterations};
        view view{tracker, graph, bpi, instructions.size()};

        pipeline->addEventListener(&view);
        verify(bool(pipeline->run()));

        std::vector<data> v(instructions.size());
        const auto& critical_seq = graph.critial_seq();

        for (auto i = 0u; i < v.size(); ++i) {
          const auto seq = std::find_if(
            critical_seq.begin(),
            critical_seq.end(),
            [&](const auto& t) { return (t->FromIID % instructions.size()) == i; }
          );

          if (seq == critical_seq.end()) {
            continue;
          }

          const auto& dep = (*seq)->Dep;
          if (dep.type == data::dependency::reg) {
            std::string reg_name{};
            ::llvm::raw_string_ostream str{reg_name};
            llvm.instruction_printer->printRegName(str, dep.ResourceOrRegID);
            v[i] = {
              .dependency = data::dependency::reg,
              .name = reg_name,
            };
          } else if (dep.type == data::dependency::mem) {
            v[i] = {
              .dependency = data::dependency::mem
            };
          } else if (dep.type == data::dependency::res) {
            v[i] = {
              .dependency = data::dependency::res,
              .name = tracker.resolve_name(dep.ResourceOrRegID),
              .freq = ((*seq)->Frequency * 100.) / iterations,
              .cost = dep.Cost,
            };
          } else {
            verify(false, "[ERROR] mca::bottleneck: unsupported dependency!");
          }
        }
        return v;
      }

     private:
      struct data {
        enum class dependency {
          none, /// none dependency
          reg,  /// register dependency
          mem,  /// memory dependency
          res,  /// resource dependency
        } dependency{};
        std::optional<std::string> name{};
        std::optional<double> freq{};
        std::optional<double> cost{};
      };

      struct back_pressure_info {
        std::size_t iterations{};
        std::size_t PressureIncreaseCycles{};
        std::size_t ResourcePressureCycles{};
        std::size_t DataDependencyCycles{};
        std::size_t RegisterDependencyCycles{};
        std::size_t MemoryDependencyCycles{};
        bool SeenStallCycles{};
      };

      struct instruction_pressure_info {
        std::size_t RegisterPressureCycles{};
        std::size_t MemoryPressureCycles{};
        std::size_t ResourcePressureCycles{};
      };

      struct instruction_executed {
        std::size_t id{};
      };

      struct instruction_dispatched {
        std::size_t id{};
      };

      class pressure_tracker {
       public:
        constexpr explicit pressure_tracker(const auto& sm)
          : sm_{sm}
          , ResourcePressureDistribution(sm.getNumProcResourceKinds(), 0)
          , ProcResID2Mask(sm.getNumProcResourceKinds(), 0)
          , ResIdx2ProcResID(sm.getNumProcResourceKinds(), 0)
          , ProcResID2ResourceUsersIndex(sm.getNumProcResourceKinds(), 0) {
          ::llvm::mca::computeProcResourceMasks(sm_, ProcResID2Mask);

          auto index = 0u;
          for (auto i = 1u; i < sm_.getNumProcResourceKinds(); ++i) {
            ProcResID2ResourceUsersIndex[i] = index;
            index += sm_.getProcResource(i)->NumUnits;
            ResIdx2ProcResID[::llvm::mca::getResourceStateIndex(ProcResID2Mask[i])] = i;
          }

          users_.resize(index);
          std::fill(users_.begin(), users_.end(), std::make_pair<unsigned, unsigned>(~0U, 0U));
        }

        constexpr void on(const instruction_dispatched& event) {
          ipi_.insert(std::make_pair(event.id, instruction_pressure_info()));
        }

        constexpr void on(const instruction_executed& event) {
          ipi_.erase(event.id);
        }

        constexpr void on(const ::llvm::mca::HWInstructionIssuedEvent& event) {
          const auto id = event.IR.getSourceIndex();
          for (const auto &[use, index] : event.UsedResources) {
            const auto Index = ProcResID2ResourceUsersIndex[use.first] + ::llvm::countr_zero(use.second);
            users_[Index] = std::make_pair(id, index.getNumerator());
          }
        }

        constexpr void on(const ::llvm::mca::HWPressureEvent& event) {
          verify(event.Reason != ::llvm::mca::HWPressureEvent::INVALID, "[ERROR] mca::bottleneck: unexpected invalid event!");

          switch (event.Reason) {
            default: break;

            case ::llvm::mca::HWPressureEvent::RESOURCES: {
              const uint64_t ResourceMask = event.ResourceMask;
              update(event.ResourceMask);

              for (const auto &IR : event.AffectedInstructions) {
                const auto &IS = *IR.getInstruction();
                const auto BusyResources = IS.getCriticalResourceMask() & ResourceMask;
                if (!BusyResources) {
                  continue;
                }
                ipi_[IR.getSourceIndex()].ResourcePressureCycles++;
              }
              break;
            }

            case ::llvm::mca::HWPressureEvent::REGISTER_DEPS: {
              for (const auto &IR : event.AffectedInstructions) {
                ipi_[IR.getSourceIndex()].RegisterPressureCycles++;
              }
              break;
            }

            case ::llvm::mca::HWPressureEvent::MEMORY_DEPS: {
              for (const auto &IR : event.AffectedInstructions) {
                ipi_[IR.getSourceIndex()].MemoryPressureCycles++;
              }
            }
          }
        }

        constexpr void update(auto cumulative_mask) {
          while (cumulative_mask) {
            auto current = cumulative_mask & (-cumulative_mask);
            auto ResIdx = ::llvm::mca::getResourceStateIndex(current);
            auto ProcResID = ResIdx2ProcResID[ResIdx];
            auto mask = ProcResID2Mask[ProcResID];

            if (mask == current) {
              ResourcePressureDistribution[ProcResID]++;
              cumulative_mask ^= current;
              continue;
            }

            mask ^= current;
            while (mask) {
              const auto unit = mask & (-mask);
              ResIdx = ::llvm::mca::getResourceStateIndex(unit);
              ProcResID = ResIdx2ProcResID[ResIdx];
              ResourcePressureDistribution[ProcResID]++;
              mask ^= unit;
            }

            cumulative_mask ^= current;
          }
        }

        [[nodiscard]] constexpr auto resolve_name(const auto mask) const {
          const auto Index = ::llvm::mca::getResourceStateIndex(mask);
          const auto ProcResID = ResIdx2ProcResID[Index];
          const auto &PRDesc = *sm_.getProcResource(ProcResID);
          return PRDesc.Name;
        }

        [[nodiscard]] constexpr auto reg_pressure_cycles(const auto id) const {
          verify(ipi_.contains(id), "[ERROR] mca::bottleneck: instruction is not tracked!");
          const auto &Info = ipi_.find(id)->second;
          return Info.RegisterPressureCycles;
        }

        [[nodiscard]] constexpr auto mem_pressure_cycles(const auto id) const {
          verify(ipi_.contains(id), "[ERROR] mca::bottleneck: instruction is not tracked!");
          const auto &Info = ipi_.find(id)->second;
          return Info.MemoryPressureCycles;
        }

        [[nodiscard]] constexpr auto res_pressure_cycles(const auto id) const {
          verify(ipi_.contains(id), "[ERROR] mca::bottleneck: instruction is not tracked!");
          const auto &Info = ipi_.find(id)->second;
          return Info.ResourcePressureCycles;
        }

        [[nodiscard]] constexpr auto res_pressure_distribution() const -> ::llvm::ArrayRef<unsigned> {
          return ResourcePressureDistribution;
        }

        constexpr void users(const auto mask, auto& users) const {
          auto Index = ::llvm::mca::getResourceStateIndex(mask);
          auto ProcResID = ResIdx2ProcResID[Index];
          const auto &PRDesc = *sm_.getProcResource(ProcResID);
          for (auto I = 0u; I < PRDesc.NumUnits; ++I) {
            const auto U = users_[ProcResID2ResourceUsersIndex[ProcResID] + I];
            if (U.second and ipi_.contains(U.first)) {
              users.emplace_back(U);
            }
          }
        }

      private:
        const ::llvm::MCSchedModel &sm_;
        ::llvm::SmallVector<unsigned, 4u> ResourcePressureDistribution{};
        ::llvm::SmallVector<std::uint64_t, 4u> ProcResID2Mask{};
        ::llvm::SmallVector<unsigned, 4u> ResIdx2ProcResID{};
        ::llvm::SmallVector<unsigned, 4u> ProcResID2ResourceUsersIndex{};
        ::llvm::SmallVector<std::pair<unsigned, unsigned>, 4u> users_{};
        ::llvm::DenseMap<unsigned, instruction_pressure_info> ipi_{};
      };

      struct dependency_graph {
        struct dependency {
          enum data::dependency type{};
          std::uint64_t ResourceOrRegID{};
          std::uint64_t Cost{};
        };

        struct edge {
          dependency Dep{};
          unsigned FromIID{};
          unsigned ToIID{};
          unsigned Frequency{};
        };

        struct node {
          unsigned NumPredecessors{};
          unsigned NumVisitedPredecessors{};
          unsigned Depth{};
          std::uint64_t Cost{};
          edge CriticalPredecessor{};
          ::llvm::SmallVector<edge, 0u> OutgoingEdges{};
        };

       public:
        /*constexpr*/ dependency_graph(const dependency_graph &) = delete;
        /*constexpr*/ dependency_graph(dependency_graph&&) = default;
        /*constexpr*/ explicit dependency_graph(const std::size_t size)
          : nodes_(size)
        { }

        constexpr void init(auto& root) const {
          for (auto i = 0u; i < nodes_.size(); ++i) {
            const auto &node = nodes_[i];
            if (not node.NumPredecessors and not node.OutgoingEdges.empty()) {
              root.emplace_back(i);
            }
          }
        }

        constexpr void add_reg_dep(const auto From, const auto To, const auto RegID, const auto Cost) {
          add(From, To, {data::dependency::reg, RegID, Cost});
        }

        constexpr void add_mem_dep(const auto From, const auto To, const auto Cost) {
          add(From, To, {data::dependency::mem, {}, Cost});
        }

        constexpr void add_res_dep(const auto From, const auto To, const auto Mask, const auto Cost) {
          add(From, To, {data::dependency::res, Mask, Cost});
        }

        constexpr void propagate(auto& root) {
          ::llvm::SmallVector<unsigned, 8u> visit{};
          do {
            for (auto id : root) {
              const auto& node = nodes_[id];
              for (const auto &DepEdge : node.OutgoingEdges) {
                const auto ToIID = DepEdge.ToIID;
                auto& To = nodes_[ToIID];
                const auto Cost = node.Cost + DepEdge.Dep.Cost;
                if (Cost > To.Cost) {
                  To.CriticalPredecessor = DepEdge;
                  To.Cost = Cost;
                  To.Depth = node.Depth + 1;
                }
                To.NumVisitedPredecessors++;
                if (To.NumVisitedPredecessors == To.NumPredecessors) {
                  visit.emplace_back(ToIID);
                }
              }
            }

            std::swap(root, visit);
            visit.clear();
          } while (not root.empty());
        }

        constexpr void prune(const auto iterations) {
          for (auto& node : nodes_) {
            auto NumPruned = 0u;
            const auto Size = node.OutgoingEdges.size();
            for (auto I = 0ul, E = Size; I < E; ++I) {
              edge &Edge = node.OutgoingEdges[I];
              if (Edge.Frequency == iterations) {
                continue;
              }
              const auto Factor = double(Edge.Frequency) / iterations;
              if (.10 < Factor) {
                continue;
              }
              nodes_[Edge.ToIID].NumPredecessors--;
              std::swap(Edge, node.OutgoingEdges[E - 1]);
              --E;
              ++NumPruned;
            }

            if (NumPruned) {
              node.OutgoingEdges.resize(Size - NumPruned);
            }
          }
        }

        constexpr void finalize(const auto iterations) {
          ::llvm::SmallVector<unsigned, 16u> root{};
          prune(iterations);
          init(root);
          propagate(root);
        }

        [[nodiscard]] constexpr auto critial_seq() const -> ::llvm::SmallVector<const edge *, 16u> {
          ::llvm::SmallVector<const edge *, 16u> seq{};
          const auto It = std::max_element(nodes_.begin(), nodes_.end(), [](const auto& lhs, const auto& rhs) {
            return lhs.Cost < rhs.Cost;
          });
          auto id = std::distance(nodes_.begin(), It);
          seq.resize(nodes_[id].Depth);
          for (const auto *& de : ::llvm::reverse(seq)) {
            const auto& node = nodes_[id];
            de = &node.CriticalPredecessor;
            id = node.CriticalPredecessor.FromIID;
          }
          return seq;
        }

       private:
        constexpr void add(const auto From, const auto To, dependency&& Dep) {
          auto& NodeFrom = nodes_[From];
          auto& NodeTo = nodes_[To];
          auto& Vec = NodeFrom.OutgoingEdges;
          const auto It = find_if(Vec, [To, Dep](auto& de) {
            return de.ToIID == To and de.Dep.ResourceOrRegID == Dep.ResourceOrRegID;
          });

          if (It != Vec.end()) {
            It->Dep.Cost += Dep.Cost;
            It->Frequency++;
            return;
          }

          Vec.emplace_back(edge{Dep, From, To, 1});
          NodeTo.NumPredecessors++;
        }

        ::llvm::SmallVector<node, 16u> nodes_{};
      };

      struct view : ::llvm::mca::View {
        constexpr view(auto& tracker, auto& graph, auto& bpi, const auto instructions)
          : tracker_{tracker}
          , graph_{graph}
          , bpi_{bpi}
          , instructions_{instructions}
        { }

        constexpr void onEvent(const ::llvm::mca::HWStallEvent &) override {
          bpi_.SeenStallCycles = true;
        }

        constexpr void onEvent(const ::llvm::mca::HWInstructionEvent &Event) override {
          const auto id = Event.IR.getSourceIndex();

          if (Event.Type == ::llvm::mca::HWInstructionEvent::Dispatched) {
            tracker_.on(instruction_dispatched{id});
            return;
          }

          if (Event.Type == ::llvm::mca::HWInstructionEvent::Executed) {
            tracker_.on(instruction_executed{id});
            return;
          }

          if (Event.Type != ::llvm::mca::HWInstructionEvent::Issued) {
            return;
          }

          const auto &IS = *Event.IR.getInstruction();
          const auto To = id % instructions_;
          auto Cycles = 2u * tracker_.res_pressure_cycles(id);
          auto ResourceMask = IS.getCriticalResourceMask();
          ::llvm::SmallVector<std::pair<unsigned, unsigned>, 4u> users{};

          while (ResourceMask) {
            const auto Current = ResourceMask & (-ResourceMask);
            tracker_.users(Current, users);
            for (const auto& user : users) {
              add_res_dep(user.first % instructions_, To, Current, user.second + Cycles);
            }
            users.clear();
            ResourceMask ^= Current;
          }

          const auto &RegDep = IS.getCriticalRegDep();
          if (RegDep.Cycles) {
            Cycles = RegDep.Cycles + 2u * tracker_.reg_pressure_cycles(id);
            const auto From = RegDep.IID % instructions_;
            add_reg_dep(From, To, RegDep.RegID, Cycles);
          }

          const auto &MemDep = IS.getCriticalMemDep();
          if (MemDep.Cycles) {
            Cycles = MemDep.Cycles + 2u * tracker_.mem_pressure_cycles(id);
            const auto  From = MemDep.IID % instructions_;
            add_mem_dep(From, To, Cycles);
          }

          tracker_.on(static_cast<const ::llvm::mca::HWInstructionIssuedEvent&>(Event));

          if (id == ((bpi_.iterations * instructions_) - 1u)) {
            graph_.finalize(bpi_.iterations);
          }
        }

        constexpr void onEvent(const ::llvm::mca::HWPressureEvent &Event) override {
          verify(Event.Reason != ::llvm::mca::HWPressureEvent::INVALID, "[ERROR] mca::bottleneck: unexpected invalid event!");
          tracker_.on(Event);

          switch (Event.Reason) {
            default: break;
            case ::llvm::mca::HWPressureEvent::RESOURCES:
              pressure_.res = true;
              break;
            case ::llvm::mca::HWPressureEvent::REGISTER_DEPS:
              pressure_.reg = true;
              break;
            case ::llvm::mca::HWPressureEvent::MEMORY_DEPS:
              pressure_.mem = true;
              break;
            }
        }

        constexpr void onCycleEnd() override {
          const auto pressure_data = pressure_.reg or pressure_.mem;
          if (not pressure_.res and not pressure_data) {
            return;
          }

          ++bpi_.PressureIncreaseCycles;
          if (pressure_.reg) {
            ++bpi_.RegisterDependencyCycles;
          }

          if (pressure_.mem) {
            ++bpi_.MemoryDependencyCycles;
          }
          if (pressure_data) {
            ++bpi_.DataDependencyCycles;
          }

          if (pressure_.res) {
            ++bpi_.ResourcePressureCycles;
          }

          pressure_ = {};
        }

        ::llvm::StringRef getNameAsString() const override { return {}; }
        void printView(::llvm::raw_ostream &) const override { }

       private:
        constexpr void add_reg_dep(const auto From, const auto To, const auto RegID, const auto Cost) {
          if (const auto IsLoopCarried = From >= To; IsLoopCarried) {
            graph_.add_reg_dep(From, To + instructions_, RegID, Cost);
            graph_.add_reg_dep(From + instructions_, To + (instructions_ * 2u), RegID, Cost);
            return;
          }
          graph_.add_reg_dep(From + instructions_, To + instructions_, RegID, Cost);
        }

        constexpr void add_mem_dep(const auto From, const auto To, const auto Cost) {
          if (const auto IsLoopCarried = From >= To; IsLoopCarried) {
            graph_.add_mem_dep(From, To + instructions_, Cost);
            graph_.add_mem_dep(From + instructions_, To + (instructions_ * 2u), Cost);
            return;
          }
          graph_.add_mem_dep(From + instructions_, To + instructions_, Cost);
        }

        constexpr void add_res_dep(const auto From, const auto To, const auto Mask, const auto Cost) {
          if (const auto IsLoopCarried = From >= To; IsLoopCarried) {
            graph_.add_res_dep(From, To + instructions_, Mask, Cost);
            graph_.add_res_dep(From + instructions_, To + (instructions_ * 2u), Mask, Cost);
            return;
          }
          graph_.add_res_dep(From + instructions_, To + instructions_, Mask, Cost);
        }

        pressure_tracker& tracker_;
        dependency_graph& graph_;
        back_pressure_info& bpi_;
        std::size_t instructions_{};
        struct { bool res{}; bool reg{}; bool mem{}; } pressure_{};
      };
    };

    inline constexpr auto timeline = fixed_named<"mca.timeline", struct timeline>{};
    inline constexpr auto resource_pressure = fixed_named<"mca.resource_pressure", struct resource_pressure>{};
    inline constexpr auto bottleneck = fixed_named<"mca.bottleneck", struct bottleneck>{};
  } // namespace mca
  #endif // PERF_LLVM
} // namespace backend

inline namespace prof {
  namespace dsl {
    namespace detail {
      template<class TLhs, auto Fn, class TRhs>
      struct op {
        using element_type = mp::type_list<TLhs, TRhs>;

        [[nodiscard]] constexpr auto operator()(const std::ranges::range auto& lhs, const std::ranges::range auto& rhs) const {
          //todo
          //verify(lhs.size() == rhs.size());
          std::vector<typename std::remove_cvref_t<decltype(lhs)>::value_type> res(lhs.size());
          for (auto i = 0u; i < lhs.size(); ++i) {
            //res[i] = Fn(lhs[i], rhs[i]);
            res[i] = typename std::remove_cvref_t<decltype(lhs)>::value_type(Fn(lhs[i], typename std::remove_cvref_t<decltype(lhs)>::value_type(rhs[0])));
          }
          return res;
        }

        TLhs lhs{}; TRhs rhs{};
      };

      inline constexpr auto concat = []<class... Ts>(const Ts&... ts)
        requires (requires { ts.data(); Ts::size(); } and ...) {
        fixed_string<(Ts::size() + ... + 1u)> v{};
        std::size_t n{};
        ([&] {
          for (auto i = 0u; i < ts.size(); ++i) { v[n + i] = ts[i]; }
          n += ts.size();
        }(), ...);
        return v;
      };
    } // namesapce detail

    template<class TLhs, class TRhs>
    [[nodiscard]] constexpr auto operator+(const TLhs& lhs, const TRhs& rhs) requires requires { lhs.name(); rhs.name(); } {
      return fixed_named<
        detail::concat(TLhs::name(), fixed_string{"+"}, TRhs::name()),
        detail::op<TLhs, [](const auto& lhs, const auto& rhs) { return lhs + rhs; }, TRhs>
      >{lhs, rhs};
    }

    template<class TLhs, class TRhs>
    [[nodiscard]] constexpr auto operator-(const TLhs& lhs, const TRhs& rhs) requires requires { lhs.name(); rhs.name(); } {
      return fixed_named<
        detail::concat(TLhs::name(), fixed_string{"-"}, TRhs::name()),
        detail::op<TLhs, [](const auto& lhs, const auto& rhs) { return lhs - rhs; }, TRhs>
      >{lhs, rhs};
    }

    template<class TLhs, class TRhs>
    [[nodiscard]] constexpr auto operator*(const TLhs& lhs, const TRhs& rhs) requires requires { lhs.name(); rhs.name(); } {
      return fixed_named<
        detail::concat(TLhs::name(), fixed_string{"*"}, TRhs::name()),
        detail::op<TLhs, [](const auto& lhs, const auto& rhs) { return lhs * rhs; }, TRhs>
      >{lhs, rhs};
    }

    template<class TLhs, class TRhs>
    [[nodiscard]] constexpr auto operator/(const TLhs& lhs, const TRhs& rhs) requires requires { lhs.name(); rhs.name(); } {
      return fixed_named<
        detail::concat(TLhs::name(), fixed_string{"/"}, TRhs::name()),
        //detail::op<TLhs, [](const auto& lhs, const auto& rhs) { return rhs ? lhs / rhs : 0.; }, TRhs>
        detail::op<TLhs, [](const auto& lhs, const auto& rhs) { return lhs / rhs; }, TRhs>
      >{lhs, rhs};
    }
  } // namespace dsl

  template<class T>
  concept profiler_like = requires(T t) {
    t.start();
    t.stop();
  };

  /**
   * timing # duration
   */
  namespace time {
    template<class T>
    concept time_like = profiler_like<T> and requires(T t) {
      { *t } -> std::same_as<std::chrono::duration<double, std::nano>>;
    };

    template<class T>
      requires requires { T::period::den; } and (T::period::den == std::nano::den)
    struct chrono {
      constexpr auto start() { start_ = T::now(); }
      constexpr auto stop()  { stop_ = T::now(); }

      [[nodiscard]] constexpr auto operator*() const {
        return std::chrono::duration<double, std::nano>{stop_ - start_};
      }

     private:
      T::time_point start_{};
      T::time_point stop_{};
    };

    #if PERF_LINUX == 1
    /**
     * clock_gettime overhead is not stable (may lock)
     * @tparam ClockId
     *  CLOCK_MONOTONIC
     *  CLOCK_MONOTONIC_RAW
     *  CLOCK_REALTIME
     *  CLOCK_REALTIME_COARSE
     *  CLOCK_PROCESS_CPUTIME_ID
     *  CLOCK_THREAD_CPUTIME_ID
     */
    template<clockid_t ClockId>
    struct clock {
      constexpr auto start() { clock_gettime(ClockId, &start_); }
      constexpr auto stop()  { clock_gettime(ClockId, &stop_); }

      [[nodiscard]] constexpr auto operator*() const {
        return std::chrono::duration<double, std::nano>{
          (stop_.tv_sec - start_.tv_sec) * 1e9 + (stop_.tv_nsec - start_.tv_nsec)
        };
      }

     private:
      timespec start_{};
      timespec stop_{};
    };
    #endif // PERF_LINUX

    #if __has_builtin(__builtin_ia32_rdtsc)
    /**
     * https://en.wikipedia.org/wiki/Time_Stamp_Counter
     * https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=rdtsc
     */
    struct tsc {
      static auto freq() -> const double&;

      /// for serialization before the call
      /// - `cpu::pipeline::flush()`
      /// - `memory::fence()`
      /// - `compiler::prevent_reorder(std::memory_order_seq_cst)`
      constexpr void start() {
        start_ = __builtin_ia32_rdtsc(); /// no serialization
      }

      /// for serialization before the call
      /// - `cpu::flush_pipeline()`
      /// - `memory::fence()`
      /// - `compiler::prevent_reorder(std::memory_order_seq_cst)`
      constexpr void stop() {
        unsigned int aux{};
        stop_ = __builtin_ia32_rdtscp(&aux); /// partial serialization
      }

      [[nodiscard]] constexpr auto operator*() const {
        return std::chrono::duration<double, std::nano>{(stop_ - start_) / freq()};
      }

     private:
      std::uint64_t start_{};
      std::uint64_t stop_{};
    };

    auto tsc::freq() -> const double& {
      static const auto freq = [] {
        double freq{};
        tsc tsc{};
        for (auto i = 0u; i < 3u; ++i) {
          const auto start = std::chrono::steady_clock::now();
          std::chrono::steady_clock::time_point stop{};
          const auto min = start + std::chrono::milliseconds(10);

          tsc.start();
          for (;;) {
            stop = std::chrono::steady_clock::now();
            tsc.stop();
            if (stop >= min) break;
          }

          return std::max(freq, (tsc.stop_ - tsc.start_) /
            std::chrono::duration<double, std::nano>(stop - start).count());
        }
        return freq;
      }();
      return freq;
    }
    #endif // __builtin_ia32_rdtsc

    template<class... Ts>
      requires (time_like<Ts> and ...)
    struct timer {
      constexpr explicit timer(const Ts&... ts)
        : ts_{ts...}
      { }

      constexpr void start() {
        std::apply([](auto&... ts) { (ts.start(), ...); }, ts_);
      }

      constexpr void stop() {
        std::apply([](auto&... ts) { [[maybe_unused]] int _{}; (_ = ... = (ts.stop(), _)); }, ts_); /// reverse order
      }

      [[nodiscard]] constexpr auto operator*() const {
        std::unordered_map<std::string_view, std::chrono::duration<double, std::nano>> results{};
        std::apply([&](const auto&... ts) { ((results[ts.name()] = *ts), ...); }, ts_);
        return results;
      }

     private:
      std::tuple<Ts...> ts_{};
    };

    /// monotonic-time / guranateed to be always increasing
    inline constexpr time_like auto steady_clock = fixed_named<"time.steady_clock", chrono<std::chrono::steady_clock>>{};

    #if PERF_LINUX == 1
    /// user-time + sys-time
    inline constexpr time_like auto cpu = fixed_named<"time.cpu", clock<CLOCK_PROCESS_CPUTIME_ID>>{};

    /// cpu-time for the current thread
    inline constexpr time_like auto thread = fixed_named<"time.thread", clock<CLOCK_THREAD_CPUTIME_ID>>{};

    /// wall-time
    inline constexpr time_like auto real = fixed_named<"time.real", clock<CLOCK_REALTIME>>{};

    /// wall-time / guranateed to be always increasing
    inline constexpr time_like auto monotonic = fixed_named<"time.monotonic", clock<CLOCK_MONOTONIC>>{};
    #endif // PERF_LINUX

    #if __has_builtin(__builtin_ia32_rdtsc)
    /// time-stamp-counter # https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=rdtsc
    inline constexpr time_like auto tsc = fixed_named<"time.tsc", struct tsc>{};
    #endif // __builtin_ia32_rdtsc

    inline namespace metric {
      using dsl::operator+;
      using dsl::operator-;
      using dsl::operator*;
      using dsl::operator/;

      /// latency is the time it takes for a single operation to complete (ns)
      //inline constexpr auto latency = steady_clock / iterations;

      /// throughput is the total number of operations or tasks completed in a given amount of time (op/s)
      //inline constexpr auto throughput = iterations / steady_clock;

      //// todo requires non-sequential execution
      /// inverse throughput is the time it takes for a single operation  (ns/op)
      //inline constexpr auto inverse_throuput = steady_clock / iteartions;
    } // namespace metric
  } // namespace time

  /**
   * counting # perf counters
   * - https://perf.wiki.kernel.org
   */
  namespace stat {
    #if PERF_LINUX == 1
    template<class T>
    concept event_like = requires (T t) {
      t.name;
      t.type;
      t.config[0]; t.config[1]; t.config[2];
      t.priority;
    };

    /**
     * https://perfmon-events.intel.com # intel's list of events per architecture
     */
    struct event {
      std::string_view name{};
      std::uint32_t type{};
      std::array<std::uint64_t, 3u> config{};                       /// perf_event_attr.{config1, config2, config3}
      enum priority { high, medium, low } priority = priority::low; /// high = group leader
    };

    struct config {
      enum mode {
        user        = 0b00001,
        kernel      = 0b00010,
        hypervisor  = 0b00100,
        idle        = 0b01000,
        guest       = 0b10000,
      } mode = mode::user;

      std::optional<std::int32_t> pid{};    /// pid of the process {not_set: current process, N: process<N>}
      std::optional<std::int32_t> cpu{};    /// cpu of the process {not_set: any_cpu, N: cpu<N>}
    };

    /**
     * `perf stat -e`
     */
    template<event_like... TEvents> class counter {
      static constexpr auto read_format =
        PERF_FORMAT_ID                      /// event.id
      | PERF_FORMAT_GROUP                   /// group counters
      | PERF_FORMAT_TOTAL_TIME_ENABLED      /// multiplexing correction [optional]
      | PERF_FORMAT_TOTAL_TIME_RUNNING      /// multiplexing correction [optional]
      ;

      struct read_data {
        std::uint64_t size{};               /// events.size
        std::uint64_t total_time_enabled{}; /// multiplexing correction [optional]
        std::uint64_t total_time_running{}; /// multiplexing correction [optional]
        struct value_id {
          std::uint64_t value{};
          std::uint64_t id{};
        } values[sizeof...(TEvents)]{};     /// `verify(size == sizeof...(TEvents))`
      };

      struct info {
        std::string_view name{};
        std::uint64_t id{};
      };

     public:
      constexpr counter(const counter&) = delete;
      constexpr counter(counter&&) = delete;
      constexpr explicit counter(const TEvents&... events)
        : counter{config{}, events...}
      { }
      template<class TConfig = config>
        requires requires (TConfig config) { config.mode; config.pid; config.cpu; }
      constexpr explicit counter(const TConfig& config, const TEvents&... events) {
        if constexpr (not sizeof...(TEvents)) {
          return;
        }
        std::size_t i{};
        const auto pid = config.pid ? *config.pid : 0;
        const auto cpu = config.cpu ? *config.cpu : -1;
        const auto setup = [&](const auto& event) {
          const perf_event_attr attr{
            .type = event.type,
            .size = sizeof(perf_event_attr),
            .config = event.config[0],
            .read_format = read_format,
            .disabled = fd_ == -1,
            .exclude_user = not (config.mode & decltype(config.mode)::user),
            .exclude_kernel = not (config.mode & decltype(config.mode)::kernel),
            .exclude_hv = not (config.mode & decltype(config.mode)::hypervisor),
            .exclude_idle = not (config.mode & decltype(config.mode)::idle),
            .exclude_guest = not (config.mode & decltype(config.mode)::guest),
            .config1 = event.config[1],
            .config2 = event.config[2],
          };
          const auto fd = syscall(__NR_perf_event_open, &attr, pid, cpu, fd_, 0);
          verify(fd != -1, std::format("[ERROR] counter: '{}'", std::strerror(errno)));
          ids_[i].name = event.name();
          ioctl(fd, PERF_EVENT_IOC_ID, &ids_[i].id);
          if (fd_ == -1) {
            fd_ = fd;
          }
          ++i;
        };

        ([&] { if (events.priority == decltype(events.priority)::high) { setup(events); } }(), ...);
        ([&] { if (events.priority == decltype(events.priority)::medium) { setup(events); } }(), ...);
        ([&] { if (events.priority == decltype(events.priority)::low) { setup(events); } }(), ...);
      }

      ~counter() noexcept {
        if (fd_ != -1) {
          verify(not close(fd_));
        }
      }

      void start() {
        if constexpr (not sizeof...(TEvents)) {
          return;
        }
        ioctl(fd_, PERF_EVENT_IOC_RESET, PERF_IOC_FLAG_GROUP);
        ioctl(fd_, PERF_EVENT_IOC_ENABLE, PERF_IOC_FLAG_GROUP);
      }

      // todo rdpmc

      void stop() {
        if constexpr (not sizeof...(TEvents)) {
          return;
        }
        verify(read(fd_, &read_data_, sizeof(read_data_)) == sizeof(read_data_));
        ioctl(fd_, PERF_EVENT_IOC_DISABLE, PERF_IOC_FLAG_GROUP);
      }

      [[nodiscard]] constexpr auto operator*() const {
        std::unordered_map<std::string_view, double> data{};
        if constexpr (not sizeof...(TEvents)) {
          return data;
        }
        verify(read_data_.size == sizeof...(TEvents));
        const auto correction = double(read_data_.total_time_enabled) /
                                double(read_data_.total_time_running);
        for (auto i = 0u; i < read_data_.size; ++i) {
          const auto& [value, id] = read_data_.values[i];
          const auto found = std::find_if(ids_.begin(), ids_.end(),
            [&](const auto& info) { return info.id == id; }
          );
          verify(found != ids_.end());
          data[found->name] = value * correction;
        }
        return data;
      }

     private:
      std::int32_t fd_{-1};
      read_data read_data_{};
      std::array<info, sizeof...(TEvents)> ids_{};
    };

    /**
     * perf list                        - shows available events
     * perf stat -vv -e {event} sleep 0 - finds config values for given event
     */
    inline constexpr event_like auto cpu_clock = initialized<fixed_named<"stat.cpu_clock", event>, PERF_TYPE_SOFTWARE, std::array<std::uint64_t, 3u>{PERF_COUNT_SW_CPU_CLOCK}>{};
    inline constexpr event_like auto task_clock = initialized<fixed_named<"stat.task_clock", event>, PERF_TYPE_SOFTWARE, std::array<std::uint64_t, 3u>{PERF_COUNT_SW_TASK_CLOCK}>{};
    inline constexpr event_like auto page_faults = initialized<fixed_named<"stat.page_faults", event>, PERF_TYPE_SOFTWARE, std::array<std::uint64_t, 3u>{PERF_COUNT_SW_PAGE_FAULTS}>{};
    inline constexpr event_like auto faults = initialized<fixed_named<"stat.faults", event>, PERF_TYPE_SOFTWARE, std::array<std::uint64_t, 3u>{PERF_COUNT_SW_PAGE_FAULTS}>{};
    inline constexpr event_like auto major_faults = initialized<fixed_named<"stat.major_faults", event>, PERF_TYPE_SOFTWARE, std::array<std::uint64_t, 3u>{PERF_COUNT_SW_PAGE_FAULTS_MAJ}>{};
    inline constexpr event_like auto minor_faults = initialized<fixed_named<"stat.minor_faults", event>, PERF_TYPE_SOFTWARE, std::array<std::uint64_t, 3u>{PERF_COUNT_SW_PAGE_FAULTS_MIN}>{};
    inline constexpr event_like auto alignment_faults = initialized<fixed_named<"stat.alignment_faults", event>, PERF_TYPE_SOFTWARE, std::array<std::uint64_t, 3u>{PERF_COUNT_SW_ALIGNMENT_FAULTS}>{};
    inline constexpr event_like auto emulation_faults = initialized<fixed_named<"stat.emulation_faults", event>, PERF_TYPE_SOFTWARE, std::array<std::uint64_t, 3u>{PERF_COUNT_SW_EMULATION_FAULTS}>{};
    inline constexpr event_like auto context_switches = initialized<fixed_named<"stat.context_switches", event>, PERF_TYPE_SOFTWARE, std::array<std::uint64_t, 3u>{PERF_COUNT_SW_CONTEXT_SWITCHES}>{};
    inline constexpr event_like auto bpf_output = initialized<fixed_named<"stat.bpf_output", event>, PERF_TYPE_SOFTWARE, std::array<std::uint64_t, 3u>{PERF_COUNT_SW_BPF_OUTPUT}>{};
    inline constexpr event_like auto cgroup_switches = initialized<fixed_named<"stat.cgroup_switches", event>, PERF_TYPE_SOFTWARE, std::array<std::uint64_t, 3u>{PERF_COUNT_SW_CGROUP_SWITCHES}>{};
    inline constexpr event_like auto cpu_migrations = initialized<fixed_named<"stat.cpu_migrations", event>, PERF_TYPE_SOFTWARE, std::array<std::uint64_t, 3u>{PERF_COUNT_SW_CPU_MIGRATIONS}>{};
    inline constexpr event_like auto migrations = initialized<fixed_named<"stat.migrations", event>, PERF_TYPE_SOFTWARE, std::array<std::uint64_t, 3u>{PERF_COUNT_SW_CPU_MIGRATIONS}>{};
    inline constexpr event_like auto cycles = initialized<fixed_named<"stat.cycles", event>, PERF_TYPE_HARDWARE, std::array<std::uint64_t, 3u>{PERF_COUNT_HW_CPU_CYCLES}>{};
    inline constexpr event_like auto instructions = initialized<fixed_named<"stat.instructions", event>, PERF_TYPE_HARDWARE, std::array<std::uint64_t, 3u>{PERF_COUNT_HW_INSTRUCTIONS}>{};
    inline constexpr event_like auto branch_misses = initialized<fixed_named<"stat.branch_misses", event>, PERF_TYPE_HARDWARE, std::array<std::uint64_t, 3u>{PERF_COUNT_HW_BRANCH_MISSES}>{};
    inline constexpr event_like auto bus_cycles = initialized<fixed_named<"stat.bus_cycles", event>, PERF_TYPE_HARDWARE, std::array<std::uint64_t, 3u>{PERF_COUNT_HW_BUS_CYCLES}>{};
    inline constexpr event_like auto cache_misses = initialized<fixed_named<"stat.cache_misses", event>, PERF_TYPE_HARDWARE, std::array<std::uint64_t, 3u>{PERF_COUNT_HW_CACHE_MISSES}>{};
    inline constexpr event_like auto cache_references = initialized<fixed_named<"stat.cache_references", event>, PERF_TYPE_HARDWARE, std::array<std::uint64_t, 3u>{PERF_COUNT_HW_CACHE_REFERENCES}>{};
    inline constexpr event_like auto branches = initialized<fixed_named<"stat.branches", event>, PERF_TYPE_HARDWARE, std::array<std::uint64_t, 3u>{PERF_COUNT_HW_BRANCH_INSTRUCTIONS}>{};
    inline constexpr event_like auto branch_instructions = initialized<fixed_named<"stat.branch_instructions", event>, PERF_TYPE_HARDWARE, std::array<std::uint64_t, 3u>{PERF_COUNT_HW_BRANCH_INSTRUCTIONS}>{};
    inline constexpr event_like auto stalled_cycles_backend = initialized<fixed_named<"stat.stalled_cycles_backend", event>, PERF_TYPE_HARDWARE, std::array<std::uint64_t, 3u>{PERF_COUNT_HW_STALLED_CYCLES_BACKEND}>{};
    inline constexpr event_like auto idle_cycles_backend = initialized<fixed_named<"stat.idle_cycles_backend", event>, PERF_TYPE_HARDWARE, std::array<std::uint64_t, 3u>{PERF_COUNT_HW_STALLED_CYCLES_BACKEND}>{};
    inline constexpr event_like auto stalled_cycles_frontend = initialized<fixed_named<"stat.stalled_cycles_frontend", event>, PERF_TYPE_HARDWARE, std::array<std::uint64_t, 3u>{PERF_COUNT_HW_STALLED_CYCLES_FRONTEND}>{};
    inline constexpr event_like auto idle_cycles_frontend = initialized<fixed_named<"stat.idle_cycles_frontend", event>, PERF_TYPE_HARDWARE, std::array<std::uint64_t, 3u>{PERF_COUNT_HW_STALLED_CYCLES_FRONTEND}>{};
    inline constexpr event_like auto llc_misses = initialized<fixed_named<"stat.LLC_misses", event>, PERF_TYPE_HARDWARE, std::array<std::uint64_t, 3u>{PERF_COUNT_HW_CACHE_MISSES}>{};
    inline constexpr event_like auto l1_misses = initialized<fixed_named<"stat.L1_misses", event>, PERF_TYPE_HW_CACHE, std::array<std::uint64_t, 3u>{PERF_COUNT_HW_CACHE_L1D | (PERF_COUNT_HW_CACHE_OP_READ << 8u) | (PERF_COUNT_HW_CACHE_RESULT_MISS << 16u)}>{};
    inline constexpr event_like auto l1_dcache_loads = initialized<fixed_named<"stat.L1_dcache_loads", event>, PERF_TYPE_HW_CACHE, std::array<std::uint64_t, 3u>{PERF_COUNT_HW_CACHE_L1D | (PERF_COUNT_HW_CACHE_OP_READ << 8u) | (PERF_COUNT_HW_CACHE_RESULT_ACCESS << 16u)}>{};
    inline constexpr event_like auto l1_dcache_load_misses = initialized<fixed_named<"stat.L1_dcache_load_misses", event>, PERF_TYPE_HW_CACHE, std::array<std::uint64_t, 3u>{PERF_COUNT_HW_CACHE_L1D | (PERF_COUNT_HW_CACHE_OP_READ << 8u) | (PERF_COUNT_HW_CACHE_RESULT_MISS << 16u)}>{};
    inline constexpr event_like auto l1_icache_loads = initialized<fixed_named<"stat.L1_icache_loads", event>, PERF_TYPE_HW_CACHE, std::array<std::uint64_t, 3u>{PERF_COUNT_HW_CACHE_L1I | (PERF_COUNT_HW_CACHE_OP_READ << 8u) | (PERF_COUNT_HW_CACHE_RESULT_ACCESS << 16u)}>{};
    inline constexpr event_like auto l1_icache_load_misses = initialized<fixed_named<"stat.L1_icache_load_misses", event>, PERF_TYPE_HW_CACHE, std::array<std::uint64_t, 3u>{PERF_COUNT_HW_CACHE_L1I | (PERF_COUNT_HW_CACHE_OP_READ << 8u) | (PERF_COUNT_HW_CACHE_RESULT_MISS << 16u)}>{};
    inline constexpr event_like auto dtlb_loads = initialized<fixed_named<"stat.dTLB_loads", event>, PERF_TYPE_HW_CACHE, std::array<std::uint64_t, 3u>{PERF_COUNT_HW_CACHE_DTLB | (PERF_COUNT_HW_CACHE_OP_READ << 8u) | (PERF_COUNT_HW_CACHE_RESULT_ACCESS << 16u)}>{};
    inline constexpr event_like auto dtlb_load_misses = initialized<fixed_named<"stat.dTLB_load_misses", event>, PERF_TYPE_HW_CACHE, std::array<std::uint64_t, 3u>{PERF_COUNT_HW_CACHE_DTLB | (PERF_COUNT_HW_CACHE_OP_READ << 8u) | (PERF_COUNT_HW_CACHE_RESULT_MISS << 16u)}>{};
    inline constexpr event_like auto itlb_loads = initialized<fixed_named<"stat.iTLB_loads", event>, PERF_TYPE_HW_CACHE, std::array<std::uint64_t, 3u>{PERF_COUNT_HW_CACHE_ITLB | (PERF_COUNT_HW_CACHE_OP_READ << 8u) | (PERF_COUNT_HW_CACHE_RESULT_ACCESS << 16u)}>{};
    inline constexpr event_like auto itlb_load_misses = initialized<fixed_named<"stat.iTLB_load_misses", event>, PERF_TYPE_HW_CACHE, std::array<std::uint64_t, 3u>{PERF_COUNT_HW_CACHE_ITLB | (PERF_COUNT_HW_CACHE_OP_READ << 8u) | (PERF_COUNT_HW_CACHE_RESULT_MISS << 16u)}>{};

    inline namespace metric {
      using dsl::operator+;
      using dsl::operator-;
      using dsl::operator*;
      using dsl::operator/;

      /// instruction per cycle (ipc)
      inline constexpr auto ipc = instructions / cycles;

      /// cycles per instruction (cpi, inverse of ipc)
      inline constexpr auto cpi = cycles / instructions;

      /// branch miss rate (branch misses per branch instruction)
      inline constexpr auto branch_miss_rate = branch_misses / branches;

      /// cache miss rate (cache misses per cache reference)
      inline constexpr auto cache_miss_rate = cache_misses / cache_references;

       /// llc miss rate
      inline constexpr auto llc_miss_rate = llc_misses / cache_references;

      /// l1 data cache miss rate
      inline constexpr auto l1_dcache_miss_rate = l1_dcache_load_misses / l1_dcache_loads;

      /// l1 instruction cache miss rate
      inline constexpr auto l1_icache_miss_rate = l1_icache_load_misses / l1_icache_loads;

      /// dtlb miss rate
      inline constexpr auto dtlb_miss_rate = dtlb_load_misses / dtlb_loads;

      /// itlb miss rate
      inline constexpr auto itlb_miss_rate = itlb_load_misses / itlb_loads;

      /// stalled cycles rate (frontend)
      inline constexpr auto frontend_stall_rate = stalled_cycles_frontend / cycles;

      /// stalled cycles rate (backend)
      inline constexpr auto backend_stall_rate = stalled_cycles_backend / cycles;

      /// memory access rate
      inline constexpr auto memory_stall_ratio = stalled_cycles_backend / cycles;

      /// overall stall rate
      inline constexpr auto total_stall_rate = (stalled_cycles_backend + stalled_cycles_frontend) / cycles;

       /// cpu migrations per cycles
      inline constexpr auto cpu_migration_rate = cpu_migrations / cycles;

      /// context switches per cycles
      inline constexpr auto context_switch_rate = context_switches / cycles;

      /// page fault rate
      inline constexpr auto page_fault_rate = faults / cycles;

      /// page fault rate (major faults per total faults)
      inline constexpr auto major_fault_rate = major_faults / cycles;

      /// page fault rate (minor faults per total faults)
      inline constexpr auto minor_fault_rate = minor_faults / cycles;
    } // namespace metric

    /**
     * top-down microarchitecture analysis method
     *  - https://www.intel.com/content/www/us/en/docs/vtune-profiler/cookbook/2023-0/top-down-microarchitecture-analysis-method.html
     *  - https://github.com/torvalds/linux/blob/master/tools/perf/Documentation/topdown.txt
     *  - https://github.com/andikleen/pmu-tools/wiki/toplev-manual
     *  - https://github.com/intel/perfmon/blob/main/TMA_Metrics-full.xlsx # top-down metrics and events published by intel
     *  - ls /sys/bus/event_source/devices/cpu/events                      # to get supported top_down events (it might be cpu_core instead of cpu)
     */
    namespace top_down {
      namespace aux {
        namespace detail {
          template<fixed_string Name> struct event_config {
            [[nodiscard]] constexpr operator std::array<std::uint64_t, 3u>() const {
              const auto cfg = [] {
                const auto& cpu = info::sys::hw<"event=%zx,umask=%zx">(std::format("/sys/bus/event_source/devices/cpu/events/{}", std::string_view(Name)));
                const auto& cpu_core = info::sys::hw<"event=%zx,umask=%zx">(std::format("/sys/bus/event_source/devices/cpu_core/events/{}", std::string_view(Name)));
                return cpu ? cpu : cpu_core;
              }();
              verify(cfg and 2u == cfg->size(), std::format("[ERROR] event '{}' not found!", std::string_view(Name)));
              return {((*cfg)[1] << CHAR_BIT) | (*cfg)[0]};
            }
          };
        } // namespace detail

        inline constexpr event_like auto slots = initialized<fixed_named<"stat.slots", event>, PERF_TYPE_RAW, detail::event_config<"slots">{}, event::priority::high>{};
        inline constexpr event_like auto retiring = initialized<fixed_named<"stat.top_down.retiring", event>, PERF_TYPE_RAW, detail::event_config<"topdown-retiring">{}>{};
        inline constexpr event_like auto bad_speculation = initialized<fixed_named<"stat.top_down.bad_speculation", event>, PERF_TYPE_RAW, detail::event_config<"topdown-bad-spec">{}>{};
        inline constexpr event_like auto frontend_bound = initialized<fixed_named<"stat.top_down.fronted_bound", event>, PERF_TYPE_RAW, detail::event_config<"topdown-fe-bound">{}>{};
        inline constexpr event_like auto backend_bound = initialized<fixed_named<"stat.top_down.backend_bound", event>, PERF_TYPE_RAW, detail::event_config<"topdown-be-bound">{}>{};
        inline constexpr event_like auto heavy_operations = initialized<fixed_named<"stat.top_down.heavy_operations", event>, PERF_TYPE_RAW, detail::event_config<"topdown-heavy-ops">{}>{};
        inline constexpr event_like auto branch_mispredict = initialized<fixed_named<"stat.top_down.branch_mispredict", event>, PERF_TYPE_RAW, detail::event_config<"topdown-br-mispredict">{}>{};
        inline constexpr event_like auto fetch_latency = initialized<fixed_named<"stat.top_down.fetch_latency", event>, PERF_TYPE_RAW, detail::event_config<"topdown-fetch-lat">{}>{};
        inline constexpr event_like auto memory_bound = initialized<fixed_named<"stat.top_down.memory_bound", event>, PERF_TYPE_RAW, detail::event_config<"topdown-mem-bound">{}>{};
      } // namespace aux

      inline namespace metric {
        using dsl::operator+;
        using dsl::operator-;
        using dsl::operator*;
        using dsl::operator/;

        /// level-1
        inline constexpr auto retiring = aux::retiring / aux::slots;
        inline constexpr auto bad_speculation = aux::bad_speculation / aux::slots;
        inline constexpr auto frontend_bound = aux::frontend_bound / aux::slots;
        inline constexpr auto backend_bound = aux::backend_bound / aux::slots;

        /// level-2.retiring
        inline constexpr auto heavy_operations = aux::heavy_operations / aux::slots;
        inline constexpr auto light_operations = retiring - heavy_operations;

        /// level-2.bad_speculation
        inline constexpr auto branch_mispredict = aux::branch_mispredict / aux::slots;
        inline constexpr auto machine_clears = bad_speculation - branch_mispredict;

        /// level-2.frontend_bound
        inline constexpr auto fetch_latency = aux::fetch_latency / aux::slots;
        inline constexpr auto fetch_bandwidth = frontend_bound - fetch_latency;

        /// level-2.backend_bound
        inline constexpr auto memory_bound = aux::memory_bound / aux::slots;
        inline constexpr auto core_bound = backend_bound - memory_bound;
      } // namespace metric
    } // namespace top_down
    #endif // PERF_LINUX
  } // namespace stat

  /**
   * sampling # perf counters
   * - https://perf.wiki.kernel.org
   */
  namespace record {
    #if PERF_LINUX == 1
    template<class T>
    concept event_like = requires (T t) {
      t.name;
      t.type;
      t.config[0]; t.config[1]; t.config[2];
      t.kind;
    };

    /**
     * https://perfmon-events.intel.com # intel's list of events per architecture
     */
    struct event {
      std::string_view name{};
      std::uint32_t type{};
      std::array<std::uint64_t, 3u> config{}; /// perf_event_attr.{config1, config2, config3}
      enum class kind { event, memory, branch } kind = kind::event;
    };

    namespace aux {
      struct memory {
        enum class operation {
          load        = PERF_MEM_OP_LOAD,
          store       = PERF_MEM_OP_STORE,
          prefetch    = PERF_MEM_OP_PFETCH,
          exec        = PERF_MEM_OP_EXEC,
        };

        struct cache {
          enum class access {
            uncached  = PERF_MEM_LVL_UNC,
            hit       = PERF_MEM_LVL_HIT,
            miss      = PERF_MEM_LVL_MISS,
          } access{};

          enum class level {
            L1        = PERF_MEM_LVL_L1,
            L2        = PERF_MEM_LVL_L2,
            L3        = PERF_MEM_LVL_L3,
            RAM       = PERF_MEM_LVL_LOC_RAM,
            LFB       = PERF_MEM_LVL_LFB, /// line fill buffer
            IO        = PERF_MEM_LVL_IO,
          } level{};
        };

        struct tlb { /// translation lookaside buffer (tlb)
          enum class access {
            hit       = PERF_MEM_TLB_HIT,
            weak_hit  = PERF_MEM_TLB_WK,
            miss      = PERF_MEM_TLB_MISS,
          } access{};

          enum class level {
            L1        = PERF_MEM_TLB_L1,
            L2        = PERF_MEM_TLB_L2,
            OS        = PERF_MEM_TLB_OS,
          } level{};
        };

        std::uint64_t address{};
        operation operation{};
        cache cache{};
        tlb tlb{};
      };

      struct branch { /// last branch record (lbr)
        std::uint64_t from{};   /// indicates the source instruction
        std::uint64_t to{};     /// branch target
        std::uint64_t cycles{}; /// number of cycles elapsed since the previous branch stack update
        bool mispredicted{};    /// branch target was mispredicted
        bool predicted{};       /// branch target was predicted
        bool in_tx{};           /// branch in transactional memory transaction
        bool abort{};           /// transactional memory aborts
      };
    } // namespace aux

    struct config {
      enum mode {
        user            = 0b00001,
        kernel          = 0b00010,
        hypervisor      = 0b00100,
        idle            = 0b01000,
        guest           = 0b10000,
      } mode = mode::user;

      enum skid {             /// precise_ip
        arbitrary       = 0,  /// - can have arbitrary skid
        constant        = 1,  /// - must have constant skid
        requested_zero  = 2,  /// - requested to have 0 skid
        zero            = 3,  /// - must have 0 skid
      } skid = skid::zero;

      struct period { std::int32_t value{}; };
      struct frequency { std::int32_t value{}; };
      struct interval {
        constexpr interval(const frequency& f) : value{f.value} { }
        constexpr interval(const period& p) : value{-p.value} { }
        [[nodiscard]] constexpr operator auto() const {
          return value;
        }
       private:
        std::int32_t value{};
      } interval = period{100}; /// ervery 100th event

      std::optional<std::int32_t> pid{}; /// pid of the process {not_set: current process, N: process<N>}
      std::optional<std::int32_t> cpu{}; /// cpu of the process {not_set: any_cpu, N: cpu<N>}

      std::size_t buffer_size = 8192u * info::sys::page_size() + 1u;  /// the first page is not used
    };

    /**
     * `perf record -e`
     * `perf mem`
     * `perf record -b` / last branch record (lbr)
     */
    template<event_like... TEvents>
    class sampler {
      struct info {
        std::string_view name{};
        std::uint64_t id{};
      };

      using event_t = std::unordered_map<std::uint64_t, std::vector<double>>;
      using memory_t = std::unordered_map<std::uint64_t, std::vector<aux::memory>>;
      using branch_t = std::unordered_map<std::uint64_t, std::vector<std::vector<aux::branch>>>;

     public:
      constexpr sampler(const sampler&) = delete;
      constexpr sampler(sampler&&) = delete;
      constexpr explicit sampler(const TEvents&... events)
        : sampler{config{}, events...}
      { }
      template<class TConfig = config>
        requires requires (TConfig config) {
          config.skid;
          config.mode;
          config.interval;
          config.pid;
          config.cpu;
          config.buffer_size;
      } constexpr explicit sampler(const TConfig& config, const TEvents&... events)
        : kind_{kind(((events.kind == decltype(events.kind)::event) or ...) |
                    (((events.kind == decltype(events.kind)::memory) or ...) << 1u) |
                    (((events.kind == decltype(events.kind)::branch) or ...) << 2u))}
        , buffer_size_{config.buffer_size} {
        if constexpr (not sizeof...(TEvents)) {
          return;
        }
        const auto pid = config.pid ? *config.pid : 0;
        const auto cpu = config.cpu ? *config.cpu : -1;

        const auto sample_type = [&] {
          std::uint64_t sample_type = PERF_SAMPLE_IP;
          if (kind_ & kind::event) {
            sample_type |= PERF_SAMPLE_READ;
          }
          if (kind_ & kind::memory) {
            sample_type |= PERF_SAMPLE_ADDR | PERF_SAMPLE_DATA_SRC;
          }
          if (kind_ & kind::branch) {
            sample_type |= PERF_SAMPLE_BRANCH_STACK;
          }
          return sample_type;
        }();

        const auto read_format = [&] {
          std::int32_t read_format = PERF_FORMAT_ID | PERF_FORMAT_GROUP;
          if (kind_ & kind::event) {
            /// multiplexing correction [optional]
            read_format |= PERF_FORMAT_TOTAL_TIME_ENABLED | PERF_FORMAT_TOTAL_TIME_RUNNING;
          }
          return read_format;
        }();

        auto fd_aux = -1;
        const auto setup = [&](const auto& event) {
          const auto lbr = [&] {
            if constexpr (requires { event.kind; }) {
              return event.kind == decltype(event.kind)::branch;
            }
            return false;
          }();

          perf_event_attr attr{
            .type = not lbr * event.type,
            .size = sizeof(perf_event_attr),
            .config = not lbr * event.config[0],
            .sample_type = sample_type,
            .read_format = read_format,
            .disabled = fd_ == -1,
            .exclude_user = not (config.mode & decltype(config.mode)::user),
            .exclude_kernel = not (config.mode & decltype(config.mode)::kernel),
            .exclude_hv = not (config.mode & decltype(config.mode)::hypervisor),
            .exclude_idle = not (config.mode & decltype(config.mode)::idle),
            .exclude_guest = not (config.mode & decltype(config.mode)::guest),
            .wakeup_events = 1,
            .config1 = not lbr * event.config[1],
            .config2 = not lbr * event.config[2],
            .branch_sample_type = lbr * event.config[0],
          };

          if (config.interval > 0) {
            attr.sample_freq = config.interval;
          } else {
            attr.sample_period = std::abs(config.interval);
          }

          auto fd = -1;
          for (auto pip = std::int32_t(config.skid); pip >= 0; --pip) { /// aim for minimal skid
            attr.precise_ip = pip;
            fd = syscall(__NR_perf_event_open, &attr, pid, cpu, fd_, 0);
            if (fd != -1) break;
          }
          verify(fd != -1, std::format("[ERROR] sampler: '{}'", std::strerror(errno)));
          if (fd_ != -1 and fd_aux == -1) {
            fd_aux = fd;
          }
          if (fd_ == -1) {
            fd_ = fd; /// leader
          }
          return fd;
        };

        std::size_t i{};
        ([&] {
          ids_[i].name = events.name();
          ioctl(setup(events), PERF_EVENT_IOC_ID, &ids_[i].id);
          ++i;
        }(), ...);

        buffer_ = mmap(nullptr, buffer_size_, PROT_READ | PROT_WRITE, MAP_SHARED, fd_aux == -1 ? fd_ : fd_aux, 0);
        verify(buffer_ != MAP_FAILED, std::format("[ERROR] sampler - '{}'", std::strerror(errno)));
      }

      ~sampler() noexcept {
        if (fd_ != -1) {
          verify(not close(fd_));
          verify(not munmap(buffer_, buffer_size_));
        }
      }

      void start() {
        if constexpr (not sizeof...(TEvents)) {
          return;
        }
        ioctl(fd_, PERF_EVENT_IOC_RESET, PERF_IOC_FLAG_GROUP);
        ioctl(fd_, PERF_EVENT_IOC_ENABLE, PERF_IOC_FLAG_GROUP);
      }

      void stop() {
        if constexpr (not sizeof...(TEvents)) {
          return;
        }
        ioctl(fd_, PERF_EVENT_IOC_DISABLE, PERF_IOC_FLAG_GROUP);
      }

      [[nodiscard]] constexpr auto operator*() const {
        std::unordered_map<std::string_view, std::variant<std::monostate, event_t, memory_t, branch_t>> data{};
        if constexpr (not sizeof...(TEvents)) {
          return data;
        }
        const auto mmap_page = reinterpret_cast<perf_event_mmap_page*>(buffer_);
        const auto begin = reinterpret_cast<const std::byte*>(buffer_) + perf::info::sys::page_size();
        const auto end = begin + mmap_page->data_head;
        for (auto it = begin; it < end; it += reinterpret_cast<const perf_event_header*>(it)->size) {
          if (reinterpret_cast<const perf_event_header*>(it)->type != PERF_RECORD_SAMPLE) {
            continue;
          }
          verify(flush(data, it + sizeof(perf_event_header)) ==
            reinterpret_cast<const perf_event_header*>(it)->size
          );
        }
        return data;
      }

     private:
      template<class T>
      [[nodiscard]] static constexpr auto read(const auto*& buffer) {
        const T value = *reinterpret_cast<const T*>(buffer);
        buffer += sizeof(T);
        return value;
      }

      [[nodiscard]] constexpr auto flush(auto& data, const auto* buffer) const -> std::uint64_t {
        const auto tmp = buffer;
        const auto ip = read<std::uint64_t>(buffer);

        if (kind_ & kind::memory) {
          auto&& memory = [&] -> decltype(auto) {
            if (auto* memory = std::get_if<memory_t>(&data["record.memory"]); memory) {
              return *memory;
            } else {
              return data["record.memory"].template emplace<memory_t>();
            }
          }();
          memory[ip].push_back({.address = read<std::uint64_t>(buffer)});
        }

        if (kind_ & kind::event) {
          const auto size = read<std::uint64_t>(buffer);
          verify(size >= ids_.size());

          const auto total_time_enabled = read<std::uint64_t>(buffer);
          const auto total_time_running = read<std::uint64_t>(buffer);
          const auto correction = double(total_time_enabled) / double(total_time_running);

          for (auto i = 0u; i < size; ++i) {
            const auto value = read<std::uint64_t>(buffer);
            const auto id = read<std::uint64_t>(buffer);
            const auto found = std::find_if(ids_.begin(), ids_.end(),
              [&](const auto& info) { return info.id == id; }
            );
            if (found == ids_.end()) {
              continue;
            }
            auto&& event = [&] -> decltype(auto) {
              if (auto* event = std::get_if<event_t>(&data[found->name]); event) {
                return *event;
              } else {
                return data[found->name].template emplace<event_t>();
              }
            }();
            event[ip].push_back(value * correction);
          }
        }

        if (kind_ & kind::branch) {
          const auto size = read<std::uint64_t>(buffer);
          const auto entries = reinterpret_cast<const perf_branch_entry*>(buffer);
          auto&& branch = [&] -> decltype(auto) {
            if (auto* branch = std::get_if<branch_t>(&data["record.branch"]); branch) {
              return *branch;
            } else {
              return data["record.branch"].template emplace<branch_t>();
            }
          }();

          auto&& branches = branch[ip].emplace_back(std::vector<aux::branch>(size));
          for (auto i = 0u; i < size; ++i) {
            const auto& branch = entries[i];
            branches.push_back({
              branch.from,
              branch.to,
              branch.cycles,
              bool(branch.mispred),
              bool(branch.predicted),
              bool(branch.in_tx),
              bool(branch.abort),
            });
          }
          buffer += sizeof(perf_branch_entry) * size;
        }

        if (kind_ & kind::memory) {
          verify(data.contains("record.memory"));
          auto& mem = std::get<memory_t>(data["record.memory"])[ip].back();

          const auto data_src = read<std::uint64_t>(buffer);
          const auto operation = reinterpret_cast<const perf_mem_data_src*>(&data_src)->mem_op;
          const auto cache = reinterpret_cast<const perf_mem_data_src*>(&data_src)->mem_lvl;
          const auto tlb = reinterpret_cast<const perf_mem_data_src*>(&data_src)->mem_dtlb;

          mem.operation = static_cast<enum aux::memory::operation>(operation);

          mem.cache.access = static_cast<enum aux::memory::cache::access>(
            cache & (std::uint64_t(aux::memory::cache::access::uncached) |
             std::uint64_t(aux::memory::cache::access::hit) |
             std::uint64_t(aux::memory::cache::access::miss)));

          mem.cache.level = static_cast<enum aux::memory::cache::level>(
            cache & (std::uint64_t(aux::memory::cache::level::LFB) |
             std::uint64_t(aux::memory::cache::level::L1) |
             std::uint64_t(aux::memory::cache::level::L2) |
             std::uint64_t(aux::memory::cache::level::L3) |
             std::uint64_t(aux::memory::cache::level::RAM) |
             std::uint64_t(aux::memory::cache::level::IO)));

          mem.tlb.access = static_cast<enum aux::memory::tlb::access>(
            tlb & (std::uint64_t(aux::memory::tlb::access::hit) |
             std::uint64_t(aux::memory::tlb::access::weak_hit) |
             std::uint64_t(aux::memory::tlb::access::miss)));

          mem.tlb.level = static_cast<enum aux::memory::tlb::level>(
            tlb & (std::uint64_t(aux::memory::tlb::level::L1) |
             std::uint64_t(aux::memory::tlb::level::L2) |
             std::uint64_t(aux::memory::tlb::level::OS)));
        }

        return buffer - tmp + sizeof(std::uint64_t); /// bytes consumed
      }

      enum kind : std::uint8_t { event = 0b001, memory = 0b010, branch = 0b100 } kind_{};
      std::size_t buffer_size_{};
      std::array<info, sizeof...(TEvents)> ids_{};
      std::int32_t fd_{-1};
      void* buffer_{};
    };

    /**
     * perf list # shows available events
     * perf stat -vv -e {event} sleep 0 # finds config values for given event
     */
    inline constexpr event_like auto cpu_clock = initialized<fixed_named<"record.cpu_clock", event>, PERF_TYPE_SOFTWARE, std::array<std::uint64_t, 3u>{PERF_COUNT_SW_CPU_CLOCK}>{};
    inline constexpr event_like auto task_clock = initialized<fixed_named<"record.task_clock", event>, PERF_TYPE_SOFTWARE, std::array<std::uint64_t, 3u>{PERF_COUNT_SW_TASK_CLOCK}>{};
    inline constexpr event_like auto page_faults = initialized<fixed_named<"record.page_faults", event>, PERF_TYPE_SOFTWARE, std::array<std::uint64_t, 3u>{PERF_COUNT_SW_PAGE_FAULTS}>{};
    inline constexpr event_like auto faults = initialized<fixed_named<"record.faults", event>, PERF_TYPE_SOFTWARE, std::array<std::uint64_t, 3u>{PERF_COUNT_SW_PAGE_FAULTS}>{};
    inline constexpr event_like auto major_faults = initialized<fixed_named<"record.major_faults", event>, PERF_TYPE_SOFTWARE, std::array<std::uint64_t, 3u>{PERF_COUNT_SW_PAGE_FAULTS_MAJ}>{};
    inline constexpr event_like auto minor_faults = initialized<fixed_named<"record.minor_faults", event>, PERF_TYPE_SOFTWARE, std::array<std::uint64_t, 3u>{PERF_COUNT_SW_PAGE_FAULTS_MIN}>{};
    inline constexpr event_like auto alignment_faults = initialized<fixed_named<"record.alignment_faults", event>, PERF_TYPE_SOFTWARE, std::array<std::uint64_t, 3u>{PERF_COUNT_SW_ALIGNMENT_FAULTS}>{};
    inline constexpr event_like auto emulation_faults = initialized<fixed_named<"record.emulation_faults", event>, PERF_TYPE_SOFTWARE, std::array<std::uint64_t, 3u>{PERF_COUNT_SW_EMULATION_FAULTS}>{};
    inline constexpr event_like auto context_switches = initialized<fixed_named<"record.context_switches", event>, PERF_TYPE_SOFTWARE, std::array<std::uint64_t, 3u>{PERF_COUNT_SW_CONTEXT_SWITCHES}>{};
    inline constexpr event_like auto bpf_output = initialized<fixed_named<"record.bpf_output", event>, PERF_TYPE_SOFTWARE, std::array<std::uint64_t, 3u>{PERF_COUNT_SW_BPF_OUTPUT}>{};
    inline constexpr event_like auto cgroup_switches = initialized<fixed_named<"record.cgroup_switches", event>, PERF_TYPE_SOFTWARE, std::array<std::uint64_t, 3u>{PERF_COUNT_SW_CGROUP_SWITCHES}>{};
    inline constexpr event_like auto cpu_migrations = initialized<fixed_named<"record.cpu_migrations", event>, PERF_TYPE_SOFTWARE, std::array<std::uint64_t, 3u>{PERF_COUNT_SW_CPU_MIGRATIONS}>{};
    inline constexpr event_like auto migrations = initialized<fixed_named<"record.migrations", event>, PERF_TYPE_SOFTWARE, std::array<std::uint64_t, 3u>{PERF_COUNT_SW_CPU_MIGRATIONS}>{};
    inline constexpr event_like auto cycles = initialized<fixed_named<"record.cycles", event>, PERF_TYPE_HARDWARE, std::array<std::uint64_t, 3u>{PERF_COUNT_HW_CPU_CYCLES}>{};
    inline constexpr event_like auto instructions = initialized<fixed_named<"record.instructions", event>, PERF_TYPE_HARDWARE, std::array<std::uint64_t, 3u>{PERF_COUNT_HW_INSTRUCTIONS}>{};
    inline constexpr event_like auto branch_misses = initialized<fixed_named<"record.branch_misses", event>, PERF_TYPE_HARDWARE, std::array<std::uint64_t, 3u>{PERF_COUNT_HW_BRANCH_MISSES}>{};
    inline constexpr event_like auto bus_cycles = initialized<fixed_named<"record.bus_cycles", event>, PERF_TYPE_HARDWARE, std::array<std::uint64_t, 3u>{PERF_COUNT_HW_BUS_CYCLES}>{};
    inline constexpr event_like auto cache_misses = initialized<fixed_named<"record.cache_misses", event>, PERF_TYPE_HARDWARE, std::array<std::uint64_t, 3u>{PERF_COUNT_HW_CACHE_MISSES}>{};
    inline constexpr event_like auto cache_references = initialized<fixed_named<"record.cache_references", event>, PERF_TYPE_HARDWARE, std::array<std::uint64_t, 3u>{PERF_COUNT_HW_CACHE_REFERENCES}>{};
    inline constexpr event_like auto branches = initialized<fixed_named<"record.branches", event>, PERF_TYPE_HARDWARE, std::array<std::uint64_t, 3u>{PERF_COUNT_HW_BRANCH_INSTRUCTIONS}>{};
    inline constexpr event_like auto branch_instructions = initialized<fixed_named<"record.branch_instructions", event>, PERF_TYPE_HARDWARE, std::array<std::uint64_t, 3u>{PERF_COUNT_HW_BRANCH_INSTRUCTIONS}>{};
    inline constexpr event_like auto stalled_cycles_backend = initialized<fixed_named<"record.stalled_cycles_backend", event>, PERF_TYPE_HARDWARE, std::array<std::uint64_t, 3u>{PERF_COUNT_HW_STALLED_CYCLES_BACKEND}>{};
    inline constexpr event_like auto idle_cycles_backend = initialized<fixed_named<"record.idle_cycles_backend", event>, PERF_TYPE_HARDWARE, std::array<std::uint64_t, 3u>{PERF_COUNT_HW_STALLED_CYCLES_BACKEND}>{};
    inline constexpr event_like auto stalled_cycles_frontend = initialized<fixed_named<"record.stalled_cycles_frontend", event>, PERF_TYPE_HARDWARE, std::array<std::uint64_t, 3u>{PERF_COUNT_HW_STALLED_CYCLES_FRONTEND}>{};
    inline constexpr event_like auto idle_cycles_frontend = initialized<fixed_named<"record.idle_cycles_frontend", event>, PERF_TYPE_HARDWARE, std::array<std::uint64_t, 3u>{PERF_COUNT_HW_STALLED_CYCLES_FRONTEND}>{};
    inline constexpr event_like auto llc_misses = initialized<fixed_named<"record.LLC_misses", event>, PERF_TYPE_HARDWARE, std::array<std::uint64_t, 3u>{PERF_COUNT_HW_CACHE_MISSES}>{};
    inline constexpr event_like auto l1_misses = initialized<fixed_named<"record.L1_misses", event>, PERF_TYPE_HW_CACHE, std::array<std::uint64_t, 3u>{PERF_COUNT_HW_CACHE_L1D | (PERF_COUNT_HW_CACHE_OP_READ << 8u) | (PERF_COUNT_HW_CACHE_RESULT_MISS << 16u)}>{};
    inline constexpr event_like auto l1_dcache_loads = initialized<fixed_named<"record.L1_dcache_loads", event>, PERF_TYPE_HW_CACHE, std::array<std::uint64_t, 3u>{PERF_COUNT_HW_CACHE_L1D | (PERF_COUNT_HW_CACHE_OP_READ << 8u) | (PERF_COUNT_HW_CACHE_RESULT_ACCESS << 16u)}>{};
    inline constexpr event_like auto l1_dcache_load_misses = initialized<fixed_named<"record.L1_dcache_load_misses", event>, PERF_TYPE_HW_CACHE, std::array<std::uint64_t, 3u>{PERF_COUNT_HW_CACHE_L1D | (PERF_COUNT_HW_CACHE_OP_READ << 8u) | (PERF_COUNT_HW_CACHE_RESULT_MISS << 16u)}>{};
    inline constexpr event_like auto l1_icache_loads = initialized<fixed_named<"record.L1_icache_loads", event>, PERF_TYPE_HW_CACHE, std::array<std::uint64_t, 3u>{PERF_COUNT_HW_CACHE_L1I | (PERF_COUNT_HW_CACHE_OP_READ << 8u) | (PERF_COUNT_HW_CACHE_RESULT_ACCESS << 16u)}>{};
    inline constexpr event_like auto l1_icache_load_misses = initialized<fixed_named<"record.L1_icache_load_misses", event>, PERF_TYPE_HW_CACHE, std::array<std::uint64_t, 3u>{PERF_COUNT_HW_CACHE_L1I | (PERF_COUNT_HW_CACHE_OP_READ << 8u) | (PERF_COUNT_HW_CACHE_RESULT_MISS << 16u)}>{};
    inline constexpr event_like auto dtlb_loads = initialized<fixed_named<"record.dTLB_loads", event>, PERF_TYPE_HW_CACHE, std::array<std::uint64_t, 3u>{PERF_COUNT_HW_CACHE_DTLB | (PERF_COUNT_HW_CACHE_OP_READ << 8u) | (PERF_COUNT_HW_CACHE_RESULT_ACCESS << 16u)}>{};
    inline constexpr event_like auto dtlb_load_misses = initialized<fixed_named<"record.dTLB_load_misses", event>, PERF_TYPE_HW_CACHE, std::array<std::uint64_t, 3u>{PERF_COUNT_HW_CACHE_DTLB | (PERF_COUNT_HW_CACHE_OP_READ << 8u) | (PERF_COUNT_HW_CACHE_RESULT_MISS << 16u)}>{};
    inline constexpr event_like auto itlb_loads = initialized<fixed_named<"record.iTLB_loads", event>, PERF_TYPE_HW_CACHE, std::array<std::uint64_t, 3u>{PERF_COUNT_HW_CACHE_ITLB | (PERF_COUNT_HW_CACHE_OP_READ << 8u) | (PERF_COUNT_HW_CACHE_RESULT_ACCESS << 16u)}>{};
    inline constexpr event_like auto itlb_load_misses = initialized<fixed_named<"record.iTLB_load_misses", event>, PERF_TYPE_HW_CACHE, std::array<std::uint64_t, 3u>{PERF_COUNT_HW_CACHE_ITLB | (PERF_COUNT_HW_CACHE_OP_READ << 8u) | (PERF_COUNT_HW_CACHE_RESULT_MISS << 16u)}>{};

    namespace memory {
      namespace detail {
        template<fixed_string Name> struct event_config {
          template<class T>
          [[nodiscard]] constexpr operator T() const {
            const auto cfg = [] {
              const auto& cpu = info::sys::hw<"event=%zx,umask=%zx">(std::format("/sys/bus/event_source/devices/cpu/events/{}", std::string_view(Name)));
              const auto& cpu_core = info::sys::hw<"event=%zx,umask=%zx">(std::format("/sys/bus/event_source/devices/cpu_core/events/{}", std::string_view(Name)));
              return cpu ? cpu : cpu_core;
            }();
            verify(cfg and 2u == cfg->size());
            return {((*cfg)[1] << CHAR_BIT) | (*cfg)[0]};
          }
        };
        template<fixed_string Name> struct event_ldlat_config {
          template<class T>
          [[nodiscard]] constexpr operator T() const {
            const auto cfg = [] {
              const auto& cpu = info::sys::hw<"event=%zx,umask=%zx,ldlat=%zd">(std::format("/sys/bus/event_source/devices/cpu/events/{}", std::string_view(Name)));
              const auto& cpu_core = info::sys::hw<"event=%zx,umask=%zx,ldlat=%zd">(std::format("/sys/bus/event_source/devices/cpu_core/events/{}", std::string_view(Name)));
              return cpu ? cpu : cpu_core;
            }();
            verify(cfg and 3u == cfg->size());
            return {((*cfg)[1] << CHAR_BIT) | (*cfg)[0], (*cfg)[2]};
          }
        };
      } // namespace detail

      namespace aux {
        /// https://lore.kernel.org/lkml/1612296553-21962-3-git-send-email-kan.liang@linux.intel.com # required for `alderlake`, `sapphirerapids (needs to be passed before memory::loads)
        inline constexpr event_like auto loads = initialized<fixed_named<"record.memory.aux.loads", event>, PERF_TYPE_RAW, detail::event_config<"mem-loads-aux">{}, event::kind::memory>{};
      } // namespace aux

      inline constexpr event_like auto loads = initialized<fixed_named<"record.memory.loads", event>, PERF_TYPE_RAW, detail::event_ldlat_config<"mem-loads">{}, event::kind::memory>{};
      inline constexpr event_like auto stores = initialized<fixed_named<"record.memory.stores", event>, PERF_TYPE_RAW, detail::event_config<"mem-stores">{}, event::kind::memory>{};
    } // namespace memory

    namespace branch {
      inline constexpr event_like auto any = initialized<fixed_named<"record.branch.any", event>, PERF_TYPE_RAW, std::array<std::uint64_t, 3u>{PERF_SAMPLE_BRANCH_ANY}, event::kind::branch>{};
      inline constexpr event_like auto jmp = initialized<fixed_named<"record.branch.jmp", event>, PERF_TYPE_RAW, std::array<std::uint64_t, 3u>{PERF_SAMPLE_BRANCH_IND_JUMP | PERF_SAMPLE_BRANCH_COND}, event::kind::branch>{};
      inline constexpr event_like auto call = initialized<fixed_named<"record.branch.call", event>, PERF_TYPE_RAW, std::array<std::uint64_t, 3u>{PERF_SAMPLE_BRANCH_ANY_CALL}, event::kind::branch>{};
      inline constexpr event_like auto ret = initialized<fixed_named<"record.branch.ret", event>, PERF_TYPE_RAW, std::array<std::uint64_t, 3u>{PERF_SAMPLE_BRANCH_ANY_RETURN}, event::kind::branch>{};
    } // namespace branch
    #endif // PERF_LINUX
  } // namespace record

  /**
   * tracing # processor instructions
   * - https://perfwiki.github.io/main/perf-tools-support-for-intel-processor-trace
   * - https://github.com/intel/libipt
   *
   * `libipt` is distributed under `BSD-3-Clause` license
   * - https://github.com/intel/libipt/blob/master/LICENSE
   */
  namespace trace {
    #if PERF_LINUX == 1 and PERF_INTEL == 1
    template<class T>
    concept trace_like = requires (T t, pt_insn_decoder* decoder) {
      t.name;
      t.config;
      t(decoder);
    };

    struct trace {
      std::uint64_t ip{};   /// instruction pointer
      std::uint64_t size{}; /// instruction size [bytes]

      enum info {
        speculative,        /// executed speculatively
        truncated           /// truncated in its image section
      } info{};

      std::unordered_map<std::string_view, std::uint64_t> data{};
    };

    struct config {
      struct vendor { /// used for timing calibration
        std::string name = perf::info::cpu::name();
        std::uint16_t family = perf::info::cpu::version().family;
        std::uint8_t model = perf::info::cpu::version().model;
        std::uint8_t stepping = perf::info::cpu::version().stepping;
      } vendor{};

      enum mode {
        user        = 0b00001,
        kernel      = 0b00010,
        hypervisor  = 0b00100,
        idle        = 0b01000,
        guest       = 0b10000,
      } mode = mode::user;

      std::optional<std::int32_t> pid{}; /// pid of the process {not_set: current process, N: process<N>}
      std::optional<std::int32_t> cpu{}; /// cpu of the process {not_set: any_cpu, N: cpu<N>}

      std::size_t buffer_size = 512u * info::sys::page_size() + 1u; /// the first page is not used
      std::size_t aux_size = 2048u * info::sys::page_size(); /// intel_pt instructions are stored under aux buffer in compressed format
    };

    /**
     * `perf record -e intel_pt/`
     */
    template<trace_like... TTraces>
    class tracer {
     public:
      constexpr tracer(const tracer&) = delete;
      constexpr tracer(tracer&&) = delete;
      constexpr explicit tracer(const TTraces&... traces)
        : tracer{config{}, traces...}
      { }
      template<class TConfig = config>
        requires requires (TConfig config) {
          config.vendor.name; config.vendor.family; config.vendor.model; config.vendor.stepping;
          config.mode;
          config.cpu; config.pid;
          config.buffer_size; config.aux_size;
      } constexpr explicit tracer(const TConfig& config, const TTraces&... traces)
        : cpu_{
          .vendor = config.vendor.name.contains("Intel") ? pcv_intel : pcv_unknown,
          .family = config.vendor.family,
          .model = config.vendor.model,
          .stepping = config.vendor.stepping}
        , buffer_size_{config.buffer_size}
        , aux_size_{config.aux_size} {
        const auto pid = config.pid ? *config.pid : 0;
        const auto cpu = config.cpu ? *config.cpu : -1;
        const perf_event_attr attr{
          .type = [] {
            const auto type = perf::info::sys::hw<"%zd">("/sys/bus/event_source/devices/intel_pt/type");
            verify(type and 1u == type->size());
            return std::uint32_t((*type)[0]);
          }(),
          .size = sizeof(perf_event_attr),
          .config = ((1ull << traces.config) | ... | 0),
          .disabled = 1,
          .exclude_user = not (config.mode & decltype(config.mode)::user),
          .exclude_kernel = not (config.mode & decltype(config.mode)::kernel),
          .exclude_hv = not (config.mode & decltype(config.mode)::hypervisor),
          .exclude_idle = not (config.mode & decltype(config.mode)::idle),
          .exclude_guest = not (config.mode & decltype(config.mode)::guest),
        };

        fd_ = syscall(__NR_perf_event_open, &attr, pid, cpu, -1, 0);
        verify(fd_ != -1);

        buffer_ = mmap(nullptr, buffer_size_, PROT_WRITE, MAP_SHARED, fd_, 0);
        verify(buffer_ != MAP_FAILED);

        auto header = reinterpret_cast<perf_event_mmap_page*>(buffer_);
        header->aux_offset = header->data_offset + header->data_size;
        header->aux_size = aux_size_;

        aux_ = mmap(nullptr, header->aux_size, PROT_READ, MAP_SHARED, fd_, header->aux_offset);
        verify(aux_ != MAP_FAILED);
      }

      constexpr ~tracer() noexcept {
        if (fd_ != -1) {
          verify(not close(fd_));
          verify(not munmap(aux_, aux_size_));
          verify(not munmap(buffer_, buffer_size_));
        }
      }

      void start() {
        ioctl(fd_, PERF_EVENT_IOC_RESET, 0);
        ioctl(fd_, PERF_EVENT_IOC_ENABLE, 0);
      }

      void stop() {
        ioctl(fd_, PERF_EVENT_IOC_DISABLE, 0);
      }

      [[nodiscard]] constexpr auto operator*() const {
        pt_config config{
          .size = sizeof(pt_config),
          .begin = reinterpret_cast<std::uint8_t*>(aux_),
          .end = reinterpret_cast<std::uint8_t*>(aux_) + aux_size_,
          .cpu = cpu_,
        };

        auto status = pt_cpu_errata(&config.errata, &config.cpu);
        verify(status >= 0, pt_errstr(pt_errcode(status)));

        auto *decoder = pt_insn_alloc_decoder(&config);
        verify(bool(decoder));

        auto *image = pt_insn_get_image(decoder);
        status = pt_image_set_callback(image, read_mem, nullptr);
        verify(status >= 0, pt_errstr(pt_errcode(status)));

        status = pt_insn_sync_forward(decoder);
        verify(status >= 0, pt_errstr(pt_errcode(status)));

        fixed_named<"trace.traces", std::vector<trace>> traces{}; /// todo
        traces.reserve((aux_size_ / sizeof(trace)) << 1u);
        while (true) {
          while (status & pts_event_pending) {
            pt_event event{};
            status = pt_insn_event(decoder, &event, sizeof(event));
            verify(status >= 0, pt_errstr(pt_errcode(status)));
          }

          decltype(trace::data) data{};
          (data.emplace(TTraces{}(decoder)), ...);

          pt_insn insn{};
          status = pt_insn_next(decoder, &insn, sizeof(insn));
          if (status == -pte_eos) {
            break;
          } else if (status < 0) {
            verify(status >= 0, std::format(
              "[WARNING] {} (not enough size - increase `config.aux_size`)", pt_errstr(pt_errcode(status)))
            );
            break;
          }

          traces.push_back({
            .ip = insn.ip,
            .size = insn.size,
            .info = static_cast<enum trace::info>(insn.speculative + (insn.truncated << 1u)),
            .data = std::move(data),
          });
        }

        pt_insn_free_decoder(decoder);
        return traces;
      }

     private:
      static constexpr auto read_mem =
        [](std::uint8_t* buffer, std::size_t size, const pt_asid*,
           std::uint64_t addr, [[maybe_unused]] void* context) -> int {
          std::memcpy(buffer, reinterpret_cast<void*>(addr), size);
          return size;
        };

      pt_cpu cpu_{};
      std::size_t buffer_size_{};
      std::size_t aux_size_{};
      std::int32_t fd_{-1};
      void* buffer_{};
      void* aux_{};
    };

    template<auto Fn> struct event {
      std::string_view name{};
      std::uint32_t config{};
      template<class... Ts>
      [[nodiscard]] constexpr auto operator()(Ts&&... ts) const {
        return Fn(std::forward<Ts>(ts)...);
      }
    };

    namespace detail {
      template<fixed_string Name> struct event_config {
        [[nodiscard]] constexpr operator std::uint32_t() const {
          const auto cfg = perf::info::sys::hw<"config:%zd">(
            std::format("/sys/bus/event_source/devices/intel_pt/format/{}", std::string_view(Name))
          );
          verify(cfg and 1u == cfg->size());
          return (*cfg)[0];
        }
      };
    } // namespace detail

    inline constexpr trace_like auto cyc = initialized<
      fixed_named<"trace.cyc", event<
        []([[maybe_unused]] auto* decoder) { return std::pair{"trace.cyc", 0u}; }
      >>,
      detail::event_config<"cyc">{}
    >{};

    inline constexpr trace_like auto tsc = initialized<
      fixed_named<"trace.tsc", event<
      [](auto* decoder) {
        std::uint64_t tsc{};
        auto status = pt_insn_time(decoder, &tsc, nullptr, nullptr);
        verify(status >= 0, pt_errstr(pt_errcode(status)));
        return std::pair{"trace.tsc", tsc};
      }>>,
      detail::event_config<"tsc">{}
    >{};

    /**
     * traces function instructions
     * @code
     * for (const auto& instruction : perf::mc::disassemble(perf::trace(fn, tracer)), llvm)) {
     *   std::print("{}", perf::mc::assembly(instruction));
     * }
     * @endcode
     */
    template<auto Begin = +[]{}, auto End = +[]{}>
    [[nodiscard]] inline constexpr auto trace(auto&& fn, auto& tracer)
      requires requires { tracer.start(); tracer.stop(); *tracer; } {
      tracer.start();
      code::label<Begin>();
      if constexpr (std::is_void_v<decltype(fn())>) {
        fn();
      } else {
        compiler::prevent_elision(fn());
      }
      code::label<End>();
      tracer.stop();

      const auto& traces = *tracer;
      const auto begin = std::find_if(traces.begin(), traces.end(),
        [](const auto& trace) { return trace.ip == std::uint64_t(code::labels[Begin]); }
      );
      const auto end = std::find_if(traces.begin(), traces.end(),
        [](const auto& trace) { return trace.ip == std::uint64_t(code::labels[End]); }
      );
      verify(begin != traces.end() and end != traces.end() and end >= begin);

      decltype(*tracer) v{};
      v.reserve(traces.size());
      std::copy(begin, end, std::back_inserter(v));
      return v;
    }
    #endif // PERF_LINUX and PERF_INTEL
  } // namespace trace

  namespace tool {
    #if __has_include(<fcntl.h>) and __has_include(<unistd.h>)
    /**
     * perf record --control=fifo:/dev/shm/perf --delay=-1 ./a.out
     */
    class perf {
      static constexpr auto enable = "enable\n";
      static constexpr auto disable = "disable\n";

     public:
      /*constexpr*/ explicit perf(const char* control)
        : fd_{open(control, O_WRONLY)}
      { }

      constexpr perf(perf&& other) : fd_{static_cast<int&&>(other.fd_)} {
        other.fd_ = -1;
      }
      constexpr perf(const perf&) = delete;

      /*constexpr*/ auto start() {
        return write(fd_, enable, sizeof(enable));
      }

      /*constexpr*/ auto stop() {
        return write(fd_, disable, sizeof(disable));
      }

      /*constexpr*/ ~perf() noexcept {
        if (fd_ == -1) return;
        close(fd_);
      }

     private:
      int fd_{};
    };
    #endif

    #if __has_include(<xray/xray_interface.h>) and \
        __has_include(<xray/xray_log_interface.h>)
    /**
     * `-fxray-instrument -fxray-instruction-threshold=1`
     * ./a.out`
     * `llvm-xray account xray-log.* --top=10 --sort=sum --sortorder=dsc --instr_map=./a.out`
     * https://llvm.org/docs/XRay.html
     */
    class xray {
     public:
      constexpr explicit xray(const char* mode = "xray-fdr",
                              const char* cfg = "xray_logfile_base=xray-log.%") {
        __xray_log_select_mode(mode);
        __xray_log_init_mode(mode, cfg);
      }

      constexpr xray(xray&&) = default;
      constexpr xray(const xray&) = delete;

      [[clang::xray_never_instrument]] /*constexpr*/ auto start() {
        return __xray_patch();
      }

      [[clang::xray_never_instrument]] /*constexpr*/ auto stop() {
        return __xray_unpatch();
      }

      /*constexpr*/ auto flush() {
        __xray_log_finalize();
        __xray_log_flushLog();
      }

      /*constexpr*/ ~xray() noexcept {
        flush();
      }
    };
    #endif

    #if __has_include(<gperftools/profiler.h>)
    /**
     * $CXX -g -O3 perf.cpp -lprofiler
     * CPUPROFILE_FREQUENCY=1000 ./a.out
     * google-pprof a.out profile.prof
     */
    class gperf {
     public:
      constexpr explicit gperf(const char* fname)
        : fname_{fname}
      { }

      constexpr gperf(gperf&&) = default;
      constexpr gperf(const gperf&) = delete;

      /*constexpr*/ auto start() {
        return ProfilerStart(fname_);
      }

      /*constexpr*/ auto stop() {
        return ProfilerStop();
      }

      /*constexpr*/ auto flush() {
        return ProfilerFlush();
      }

      /*constexpr*/ ~gperf() noexcept {
        flush();
      }

     private:
      const char* fname_{};
    };
    #endif

    #if __has_include(<ittnotify.h>)
    /**
     * `-littnotify`
     * `vtune -collect performance-snapshot -start-paused -finalization-mode=full -r result -- ./a.out`
     * https://www.intel.com/content/www/us/en/developer/tools/oneapi/vtune-profiler.html
     * https://github.com/intel/ittapi
     */
    class vtune {
     public:
      constexpr explicit intel_vtune(const char* domain, const char* task)
        : domain_{__itt_domain_create(domain)}, task_name_{__itt_string_handle_create(task)} {
        __itt_task_begin(domain_, __itt_null, __itt_null, task_name_);
      }

      constexpr intel_vtune(intel_vtune&&) = default;
      constexpr intel_vtune(const intel_vtune&) = delete;

      /*constexpr*/ auto start() {
        return __itt_resume();
      }

      /*constexpr*/ auto stop() {
        return __itt_pause();
      }

      /*constexpr*/ ~intel_vtune() noexcept {
        __itt_task_end(domain_);
      }

     private:
      __itt_domain* domain_{};
      __itt_string_handle* task_name_{};
    };
    #endif

    #if __has_include(<valgrind/callgrind.h>)
    /**
     * valgrind --tool=callgrind --instr-atstart=no ./a.out
     * kcachegrind callgrind.*
     */
    class callgrind {
     public:
      constexpr explicit callgrind(const char* profile)
        : profile_{profile}
      { }

      constexpr callgrind(callgrind&&) = default;
      constexpr callgrind(const callgrind&) = delete;

      /*constexpr*/ auto start() {
        CALLGRIND_START_INSTRUMENTATION;
      }

      /*constexpr*/ auto stop() {
        CALLGRIND_STOP_INSTRUMENTATION;
      }

      /*constexpr*/ auto flush() {
        CALLGRIND_DUMP_STATS_AT(profile_);
      }

      /*constexpr*/ ~callgrind() noexcept {
        flush();
      }

     private:
      const char* profile_{};
    };
    #endif
  } // namespace tool
} // namespace prof

namespace bench {
  namespace data {
    /**
     * generates repeated sequence
     */
    template<class T>
    struct repeat {
      using value_type = T;

      std::vector<T> values{};

      [[nodiscard]] constexpr auto operator()(const std::size_t n, ...) const {
        std::vector<value_type> r(n);
        if (not n) {
          return r;
        }
        auto i = 0u;
        while (true) { /// repeats the sequence until size(n) is reached
          for (const auto& v : values) {
            verify(i < n);
            r[i++] = v;
            if (i >= n) return r;
          }
        }
        return r;
      }
    };

    /**
     * generates range of values
     */
    template<class T>
    struct range {
      using value_type = T;

      T start{};
      T stop{};
      T step{1};

      [[nodiscard]] constexpr auto operator()(const std::size_t n, ...) const {
        std::vector<value_type> r(n);
        for (auto i = 0u; i < n; ++i) {
          r[i] = (value_type(i) * step + start) % stop;
        }
        return r;
      }
    };

    /**
     * generates choices based on provided probabilities
     */
    template<
      class T,
      class TEngine = std::mt19937,
      class TSeed = typename TEngine::result_type
    > struct choice {
      using value_type = T;

      std::vector<T> values{};
      std::vector<double> probabilities{};
      TSeed seed{};

      [[nodiscard]] constexpr auto operator()(const std::size_t n, ...) const {
        verify(not values.empty(),
          "[ERROR] choice empty: values"
        );
        verify(not probabilities.empty(),
          "[ERROR] choice empty: probabilities"
        );
        verify(values.size() == probabilities.size(),
          std::format("[ERROR] choice size: {} != {}", values.size(), probabilities.size())
        );
        verify(std::fabs(std::accumulate(probabilities.begin(), probabilities.end(), .0) - 1.) < 1e-9,
          std::format("[ERROR] choice probablity: {} != 1.",
            std::accumulate(probabilities.begin(), probabilities.end(), .0)
          )
        );

        std::vector<std::size_t> pi(100u);
        std::uniform_int_distribution<std::size_t> dist{0u, 100u - 1u};
        TEngine engine{seed};
        auto id = 0u;
        auto iv = probabilities[id] * 100.;
        for (auto i = 0; i < pi.size(); ++i) {
          if (i > iv) {
            ++id;
            iv += probabilities[id] * 100.;
          }
          pi[i] = id;
        }

        std::vector<T> r(n);
        for (auto& v : r) {
          v = values[pi[dist(engine)]];
        }

        return r;
      }
    };

    /**
     * generates uniform distrubtion of values in <min, max> range and seed
     */
    template<
      class TMin,
      class TMax = TMin,
      class TEngine = std::mt19937,
      class TDistribution = std::conditional_t<
        std::is_floating_point_v<TMin>,
        std::uniform_real_distribution<TMin>,
        std::uniform_int_distribution<TMin>
      >,
      class TSeed = typename TEngine::result_type
    > requires std::same_as<TMin, TMax>
    struct uniform {
      using value_type = TMin;

      TMin min = std::numeric_limits<TMin>::min();
      TMax max = std::numeric_limits<TMax>::max();
      TSeed seed{};

      [[nodiscard]] constexpr auto operator()(const std::size_t n, const std::size_t sub_seed = {}) const {
        std::vector<typename TDistribution::result_type> r(n);
        verify(max >= min, std::format("[ERROR] uniform: {} >= {}", max, min));
        TDistribution dist{min, max};
        TEngine engine{seed + sub_seed};
        for (auto& v : r) {
          v = dist(engine);
        }
        return r;
      }
    };

    /**
     * generates normal distrubtion of floating point values based on mean, std_dev and seed
     */
    template<
      class T,
      class TEngine = std::mt19937,
      class TDistribution = std::normal_distribution<T>,
      class TSeed = typename TEngine::result_type
    > requires std::is_floating_point_v<T>
    struct normal {
      using value_type = T;

      T mean{};
      T std_dev{1};
      TSeed seed{};

      [[nodiscard]] constexpr auto operator()(const std::size_t n, const std::size_t sub_seed = {}) const {
        std::vector<typename TDistribution::result_type> r(n);
        TDistribution dist{value_type(mean), value_type(std_dev)};
        TEngine engine{seed + sub_seed};
        for (auto& v : r) {
          v = dist(engine);
        }
        return r;
      }
    };

    //todo polute heap
    inline constexpr auto take = [](const std::size_t n) {
      return [n]<class TGen>(TGen gen) {
        return utility::named{std::format("{}|{}", gen, n), [gen, n](const std::size_t size) {
          std::vector<decltype(std::declval<TGen>()(n))> r(size);
          for (auto& e : r) {
            e = std::move(gen(n));
          }
          return r;
        }};
      };
    };

    [[nodiscard]] constexpr auto operator|(auto&& gen, auto&& op)
      requires requires(std::size_t n) { gen(n); op(gen)(n); } {
      return op(gen);
    }
  } // namespace data

  namespace stat {
    inline constexpr auto min = make_fixed_named<"min">(
      []<std::ranges::range T>(const T& v) {
        using value_type = typename T::value_type;
        if (v.empty()) {
          return value_type{};
        }
        return *std::min_element(v.begin(), v.end());
      }
    );

    inline constexpr auto max = make_fixed_named<"max">(
      []<std::ranges::range T>(const T& v) {
        using value_type = typename T::value_type;
        if (v.empty()) {
          return value_type{};
        }
        return *std::max_element(v.begin(), v.end());
      }
    );

    inline constexpr auto median = make_fixed_named<"median">(
      []<std::ranges::range T>(const T& v) {
        using value_type = typename T::value_type;
        if (v.empty()) {
          return value_type{};
        }
        std::vector<value_type> res{v.begin(), v.end()};
        std::sort(res.begin(), res.end());
        if (const auto n = res.size(); n % 2u) {
          return value_type(res[n / 2u]);
        } else {
          return value_type((res[n / 2u - 1u] + res[n / 2u]) / 2.);
        }
      }
    );

    inline constexpr auto mean = make_fixed_named<"mean">(
      []<class R = double>(const std::ranges::range auto& v) {
        if (v.empty()) {
          return R{};
        }
        return std::accumulate(v.begin(), v.end(), R{}) / v.size();
      }
    );

    inline constexpr auto geometric_mean = make_fixed_named<"geometric_mean">(
      []<class R = double>(const std::ranges::range auto& v) {
        if (v.empty()) {
          return R{};
        }
        const auto product = std::accumulate(
          v.begin(), v.end(), R(1.), std::multiplies{}
        );
        return std::pow(product, R(1.) / v.size());
      }
    );

    inline const auto percentile = [](const std::size_t p) {
      return perf::utility::named{std::format("percentile{{{}}}", p),
        [p]<std::ranges::range T>(const T& v) {
          using value_type = typename T::value_type;
          if (v.empty()) {
            return value_type{};
          }
          std::vector<value_type> res{v.begin(), v.end()};
          std::sort(res.begin(), res.end());
          return res[value_type(p) / 100. * (res.size() - value_type(1))];
        }
      };
    };

    inline constexpr auto variance = make_fixed_named<"variance">(
      []<class R = double>(const std::ranges::range auto& v) {
        if (v.empty()) {
          return R{};
        }
        return std::accumulate(
          v.begin(), v.end(), R{},
          [&](const auto acc, const auto value) {
            return acc + std::pow(value - mean(v), R(2.));
          }
        ) / v.size();
      }
    );

    inline constexpr auto std_dev = make_fixed_named<"std_dev">(
      []<class R = double>(const std::ranges::range auto& v, const std::size_t degrees_of_freedom = 1u) {
        if (v.empty()) {
          return R{};
        }
        return std::sqrt(variance(v) / degrees_of_freedom);
      }
    );

    inline constexpr auto std_err = make_fixed_named<"std_err">(
      []<class R = double>(const std::ranges::range auto& v) {
        if (v.empty()) {
          return R{};
        }
        return std_dev(v) / std::sqrt(v.size());
      }
    );

    inline constexpr auto median_absolute_dev = make_fixed_named<"median_absolute_dev">(
      []<std::ranges::range T>(const T& v) {
        if (v.empty()) {
          return typename T::value_type{};
        }
        std::vector<typename T::value_type> deviations(v.size());
        const auto median = stat::median(v);
        std::size_t i{};
        for (const auto& value : v) {
          deviations[i++] = std::fabs(value - median);
        }
        return stat::median(deviations);
      }
    );

    inline constexpr auto median_absolute_err = make_fixed_named<"median_absolute_err">(
      []<class R = double>(const std::ranges::range auto& v) {
        if (v.empty()) {
          return R{};
        }
        std::vector<R> errors(v.size());
        auto i = 0u;
        const auto predicted = median(v);
        for (auto it = v.begin(); it != v.end(); ++it) {
          errors[i++] = std::abs((*it - predicted) / (*it ? * it : *it + 1e9));
        }
        return median(errors);
      }
    );

    inline constexpr auto coefficient_of_varation = make_fixed_named<"coefficient_of_varation">(
      []<class R = double>(const std::ranges::range auto& v) {
        if (v.empty()) {
          return R{};
        }
        return std_dev(v) / mean(v);
      }
    );

    inline constexpr auto z_score = make_fixed_named<"z_score">(
      mp::overload{
        [](const std::floating_point auto confidence) {
          const auto p = 1 - (1 - confidence) / 2;

          /// wilson-hilferty transformation
          const auto t = std::sqrt(-2 * std::log(1 - p));

          /// approximation using beasley-springer-moro's method
          const auto c0 = 2.515517, c1 = 0.802853, c2 = 0.010328;
          const auto d1 = 1.432788, d2 = 0.189269, d3 = 0.001308;
          const auto n = c0 + (c1 * t) + (c2 * t * t);
          const auto d = 1 + (d1 * t) + (d2 * t * t) + (d3 * t * t * t);

          return t - (n / d);
        },
        []<std::ranges::range T>(const T& v) {
          const auto mean = stat::mean(v);
          const auto std_dev = stat::std_dev(v);
          std::vector<typename T::value_type> z(v.size());
          std::size_t i{};
          for (const auto& value : v) {
            z[i++] = (value - mean) / std_dev;
          }
          return z;
        }
      }
    );

    inline constexpr auto t_score = make_fixed_named<"t_score">(
      mp::overload{
        [](const std::floating_point auto confidence, const std::size_t degrees_of_freedom) {
          const auto d = degrees_of_freedom;
          const auto z = z_score(confidence);
          const auto q = std::sqrt(1 - (1. / (4. * d)));

          /// approximation using hill's method
          return (z + (z * z * z + 3 * z) / (4 * d) +
                 (z * z * z * z * z + 16 * z * z * z + 15 * z) /
                 (96 * d * d)) / q;
        },
        []<std::ranges::range T>(const T& v) {
          const auto mean = stat::mean(v);
          const auto std_dev = stat::std_dev(v);
          std::vector<typename T::value_type> t(v.size());
          std::size_t i{};
          for (const auto& value : v) {
            t[i++] = (value - mean) / (std_dev / std::sqrt(v.size()));
          }
          return t;
        }
      }
    );

    inline const auto summary = std::tuple{
      median,
      mean,
      min,
      max,
      percentile(50),
      percentile(75),
      percentile(99),
      median_absolute_err,
    };
  } // namespace stat

  namespace utility {
    class fn_entry_tag; class fn_exit_tag;

    template<class Fn, auto Debug = true>
    concept debug_like = []<class... TArgs>(mp::type_list<TArgs...>) {
      return requires (Fn fn, TArgs... args) { fn.template operator()<Debug>(args...); };
    }(typename mp::function_traits<Fn>::args_type{});

    inline constexpr auto debug = []<auto Debug = true, class Fn>(Fn fn)
      requires debug_like<Fn, Debug> {
        return [fn]<class... TArgs>(mp::type_list<TArgs...>) {
          return [fn](TArgs... args) { /// qualified types
            return fn.template operator()<Debug>(args...);
          };
        }(typename mp::function_traits<Fn>::args_type{});
      };

    inline constexpr auto overhead = []<class Fn>(Fn) {
      return []<class... TArgs>(mp::type_list<TArgs...>) {
        return [](TArgs...) { };
      }(typename mp::function_traits<Fn>::args_type{});
    };

    template<class T>
    inline constexpr auto make = [](auto&& t, auto n) {
      if constexpr (requires { t(n); }) {
        // todo convert
        static_assert(std::is_same_v<decltype(t(n)), std::vector<T>>);
        return t(n);
      } else {
        return std::vector<T>(n, t);
      }
    };

    template<class T>
    struct composite {
      constexpr explicit composite(T& t) : t_{t} { }
      constexpr auto start() {
        if constexpr (requires (T t) { t.start(); }) {
          t_.start();
        } else {
          std::apply([&](auto&&... ts) { (ts.start(), ...); }, t_);
        }
      }
      constexpr auto stop() {
        if constexpr (requires (T t) { t.stop(); }) {
          t_.stop();
        } else {
          std::apply([](auto&&... ts) { int _; (_ = ... = (ts.stop(), _)); }, t_); /// reverse order
        }
      }
      [[nodiscard]] constexpr auto operator*() const {
        if constexpr (requires (T t) { *t; }) {
          return *t_;
        } else {
          return std::apply([](auto&&... ts) { return std::tuple{*ts...}; }, t_);
        }
      }

     private:
      T& t_;
    };

    template<class TData>
    struct result {
      std::size_t iterations{};
      struct { std::uint64_t entry{}; std::uint64_t exit{}; } fn{};
      TData data{};
    };
  } // namespace utility

  template<class TKey = std::string, class TMapped = std::any>
  struct dataset {
    template<class T>
    constexpr auto& operator+=(const utility::result<T>& result) {
      verify(result.iterations > 0u);
      // todo
      //data[runner] = "latency";
      //data[affinity] = ...;
      //data[threads] = ...;

      get_or_create("bench.samples", std::vector<std::size_t>(1u))[0u]++;
      get_or_create("bench.iterations", std::vector<decltype(result.iterations)>{}).push_back(result.iterations);
      get_or_create("fn.entry", result.fn.entry) = result.fn.entry;
      get_or_create("fn.exit", result.fn.exit) = result.fn.exit;

      std::apply([&](const auto&... ts) {
        ([&] {
          using type = std::remove_cvref_t<decltype(ts)>;
          if constexpr (requires { ts.name(); }) {
            auto&& data = data_[TKey(ts.name())];
            verify(not data.has_value());
            data = std::move(static_cast<const typename std::remove_cvref_t<decltype(ts)>::underlying_type&>(ts));
          } else if constexpr (requires { typename type::key_type; typename type::mapped_type; }) {
            if constexpr (requires { typename std::variant_size<typename type::mapped_type>::type; }) {
              for (auto&& [name, data] : ts) {
                std::visit([&](auto&& t) { data_[TKey(name)] = std::move(data); }, data);
              }
            } else {
              for (const auto& [name, value] : ts) {
                get_or_create(name, std::vector<std::remove_cvref_t<decltype(value)>>{}).push_back(value);
              }
            }
          }
        }(), ...);
      }, result.data);
      return *this;
    }

    template<class T>
    constexpr auto& operator-=(const utility::result<T>& result) {
      std::apply([&](const auto&... ts) {
        ([&] {
          using type = std::remove_cvref_t<decltype(ts)>;
          if constexpr (requires { typename type::key_type; typename type::mapped_type; }) {
            if constexpr (not requires { typename std::variant_size<typename type::mapped_type>::type; }) {
              for (const auto& [name, value] : ts) {
                auto&& data = get<std::vector<std::remove_cvref_t<decltype(value)>>>(name);
                verify(not data.empty());
                data.back() = std::max(
                  typename std::remove_cvref_t<decltype(data)>::value_type{},
                  data.back() - value
                );
              }
            }
          }
        }(), ...);
      }, result.data);
      return *this;
    }

    [[nodiscard]] constexpr auto contains(const TKey& key) const {
      return data_.contains(key);
    }

    template<class T> requires requires (T t) { t.name(); }
    [[nodiscard]] constexpr const auto& operator[](const T& t) const { return data_.at(TKey(t.name())); }
    [[nodiscard]] constexpr auto& operator[](const TKey& key) { return data_[key]; }
    [[nodiscard]] constexpr decltype(auto) at(const TKey& key) { return data_.at(key); }

   private:
    template<class T>
    [[nodiscard]] constexpr auto& get(const auto& name) {
      verify(data_.contains(TKey(name)));
      auto&& data = data_[TKey(name)];
      verify(data.has_value());
      return std::any_cast<T&>(data);
    }

    template<class T>
    [[nodiscard]] constexpr auto& get_or_create(const auto& name, T&& t) {
      auto&& data = data_[TKey(name)];
      if (not data.has_value()) {
        data = std::move(t);
      }
      return std::any_cast<std::remove_cvref_t<T>&>(data);
    }

    std::unordered_map<TKey, TMapped> data_{};
  };

  /**
   * warmup runs
   * hot cache
   */
  struct warm {
  };

  /**
   * no warmup
   * cache evition
   */
  struct cold {
  };

  /**
   * Strict sequential execution via data dependency chain # no out of order, no speculative
   * @code
   * constexpr auto latency = [](auto&& fn, auto&&... ts) {
   *   auto checksum = 0u;
   *   perf::code::align(64u); for (auto i = 0u; i < iterations; ++i) {
   *     checksum ^= fn(checksum ^ ts...); // data dependency
   *     perf::memory::fence(); // required if there is a memory write
   *   }
   *   perf::compiler::prevent_elision(checksum);
   * };
   * @endcode
   */
  struct latency {
    std::optional<double> confidence_level = .95;

    template<mp::type_list Ops>
    [[nodiscard]] constexpr auto operator()(auto&& fn, auto&&... ts) const {
      dataset data{};

      auto&& timer = std::apply(
        [](const auto&... ts) { return perf::time::timer{ts...}; },
        mp::filter<[]<class T> { return perf::time::time_like<T>; }, std::tuple>(Ops)
      );
      #if PERF_LINUX == 1
      auto&& counter = std::apply(
        [](const auto&... ts) { return perf::stat::counter{ts...}; },
        mp::filter<[]<class T> { return perf::stat::event_like<T>; }, std::tuple>(Ops)
      );
      auto&& sampler = std::apply(
          // todo should be automated
        [](const auto&... ts) { return perf::record::sampler{/*perf::record::memory::aux::loads, */ts...}; },
        mp::filter<[]<class T> { return perf::record::event_like<T>; }, std::tuple>(Ops)
      );
      #if PERF_INTEL == 1
      auto&& tracer = std::apply(
        [](const auto&... ts) { return perf::trace::tracer{ts...}; },
        mp::filter<[]<class T> { return perf::trace::trace_like<T>; }, std::tuple>(Ops)
      );
      #endif // PERF_INTEL
      #endif // PERF_LINUX

      auto&& t = std::forward_as_tuple(sampler, counter, timer);

#if 0
      const auto iterations = [&] {
        if (this->iterations.time_budget and this->iterations.min) {
          const auto& time = bench(*this->iterations.min, time::timer{time::steady_clock}, fn, ts...).data[time::steady_clock.name()] / *this->iterations.min;
          const auto n = std::size_t((std::chrono::duration_cast<std::chrono::nanoseconds>(*this->iterations.time_budget) / time));
          return std::max(*this->iterations.min, n);
        } else {
          verify(this->iterations.max);
          return *this->iterations.max;
        }
      }();

      const auto samples = [&] {
        if (this->samples.confidence_level and this->samples.min and this->samples.max) {
          for (auto i = 0u; i < *this->samples.min; ++i) {
            data += bench<false>(iterations, t, fn, ts...);
            //data -= bench(iterations, t, utility::overhead(fn), ts...);
            data -= bench<true>(iterations, t, fn, ts...);
          }

          constexpr auto fp = []<class T>(const std::vector<T>& t) -> decltype(auto) {// todo
            if constexpr (requires { typename T::rep; }) {
              std::vector<typename T::rep> v(t.size());
              std::transform(t.begin(), t.end(), v.begin(), [](const auto& t) { return t.count(); });
              return v;
            } else {
              return t;
            }
          };

          const auto& time = fp(std::any_cast<const std::vector<decltype(*time::steady_clock)>&>(data[time::steady_clock]));
          const auto degrees_of_freedom = *this->samples.min - 1u;
          const auto t_score = stat::t_score(*this->samples.confidence_level, degrees_of_freedom);
          const auto margin_of_error = (1. - *this->samples.confidence_level) * stat::mean(time);
          return std::min(std::size_t(std::ceil(std::pow((t_score * stat::std_dev(time)) / margin_of_error, 2.)) - *this->samples.min), *this->samples.max);
        } else {
          verify(this->samples.max);
          return *this->samples.max;
        }
      }();
#endif
      const auto iterations = 10000;
      const auto samples = 100;

      for (auto i = 0u; i < samples; ++i) {
        data += bench<false>(iterations, t, fn, ts...);
        //data -= bench(iterations, t, overhead(fn), ts...);
        data -= bench<true>(iterations, t, fn, ts...);
      }

      // todo max ts.size instead of 1000
      #if PERF_INTEL == 1
      data += bench<false>(1000, std::forward_as_tuple(tracer), fn, ts...);
      #endif

      //if constexpr (requires { debug(fn); }) {
        //bench(iterations, std::tuple{}, debug(fn), ts...);
      //}

      return data;
    }

    template<auto ovh = false>
    [[nodiscard]] constexpr auto bench(const std::size_t iterations, auto&& t, auto&& fn, auto&&... ts) const {
      using fn_t = std::remove_cvref_t<decltype(fn)>;
      return [&]<class... TArgs>(mp::type_list<TArgs...>) {
        if constexpr(ovh) {
        return utility::result<decltype(*utility::composite{t})>{
          .iterations = iterations,
          .data = std::move(
            bench<fn_t, TArgs...>(
              iterations,
              &latency::invoke2<fn_t, TArgs...>,
              utility::composite{t},
              fn,
              utility::make<TArgs>(ts, iterations)...
            )),
        };
        } else {
        return utility::result<decltype(*utility::composite{t})>{
          .iterations = iterations,
          .fn = { .entry = std::uint64_t(mp::type_id<fn_t, utility::fn_entry_tag>),
                  .exit  = std::uint64_t(mp::type_id<fn_t, utility::fn_exit_tag>) },
          .data = std::move(
            bench<fn_t, TArgs...>(
              iterations,
              &latency::invoke<fn_t, TArgs...>,
              utility::composite{t},
              fn,
              utility::make<TArgs>(ts, iterations)...
            )),
        };
        }
      }(typename mp::function_traits<fn_t>::args_type{});
    }

    [[nodiscard]] static constexpr auto name() { return "latency"; }

   private:
    /**
     * @code
     * section: latency
     *  invoke<add>:
     *   add %esi,%edi
     *   xor %edi,0x1e994(%rip) /// data dependency (edi), memory write
     *   ret
     *
     *  invoke<overhead<add>>:
     *   xor %edi,0x1e994(%rip)
     *   ret
     *
     * section: .text
     *  bench:
     *   ...        // loop (aligned: 64u)
     *   call *%r12 // call via function pointer (invoke<add> or invoke<overhead<add>>)
     *   mfence     // wait for the expected memory write
     *   ...        // loop continues
     * @endcode
     */
    template<class Fn, class... TArgs>
    #if PERF_GNU == 1
    [[gnu::section("latency")]] [[gnu::noinline]] [[gnu::aligned(64u)]]
    #endif // PERF_GNU
    [[nodiscard]] constexpr auto bench(
      const std::size_t iterations, void (*invoke)(Fn, TArgs...), auto&& t, auto&& fn, auto&&... ts) const
      requires requires (std::size_t i) { t.start(); t.stop(); *t; invoke(fn, ts[i]...); } {
      mp::unroll<5u>([&] { t.start(); t.stop(); });

      t.start();
      compiler::prevent_reorder(std::memory_order_seq_cst);
      code::align<64u>(); for (auto i = 0u; i < iterations; ++i) {
        #if defined(__clang__)
        memory::fence<std::memory_order_seq_cst>();
        invoke(fn, ts[i]...);
        #else
        invoke(fn, ts[i]...);
        memory::fence<std::memory_order_seq_cst>();
        #endif
      }
      compiler::prevent_reorder(std::memory_order_seq_cst);
      t.stop();

      return *t;
    }

    template<class Fn, class T, class... Ts>
    #if PERF_GNU == 1
    [[gnu::section("latency")]] [[gnu::noinline]] [[gnu::aligned(64u)]]
    #endif // PERF_GNU
    static constexpr void invoke2(Fn fn, T t, Ts... ts) {
      static int checksum;
      checksum ^= t;
    }

    /**
     * invokes original function with parameters
     * - parameters are passed with the same qualifiers as original function
     * - `[[noinline]]` is required to keep same ip's for branches
     * - applies `entry/exit` function labels for tracing/disassembly
     */
    template<class Fn, class... Ts>
    #if PERF_GNU == 1
    [[gnu::section("latency")]] [[gnu::noinline]] [[gnu::aligned(64u)]]
    #endif // PERF_GNU
    static constexpr void invoke(Fn fn, Ts... ts) {
      constexpr auto fn_entry = mp::type_id<Fn, utility::fn_entry_tag>;
      constexpr auto fn_exit = mp::type_id<Fn, utility::fn_exit_tag>;
      static int checksum; /// not-initialized
      static int ret; /// not-initialized
      if constexpr (std::is_void_v<decltype(fn(ts...))>) {
        code::label<fn_entry>();
        fn(ts...);
        code::label<fn_exit>();
      } else if constexpr (requires { checksum ^= fn(ts...); }) {
        code::label<fn_entry>();
        auto&& ret = fn(ts...);
        compiler::prevent_elision(
          ret
        );
        code::label<fn_exit>();
        checksum ^= ret;
      } else {
        code::label<fn_entry>();
        compiler::prevent_elision(
          fn(ts...)
        );
        code::label<fn_exit>();
        checksum ^= ret;
      }
    }
  };

  /**
   * Out of order, speculative execution
   * @code
   * constexpr auto throughput = [](auto&& fn, auto&&... ts) {
   *   perf::code::align(64u); for (auto i = 0u; i < iterations; ++i) {
   *     perf::compiler::prevent_elision(fn(ts...));
   *   } // allows out of order execution of the following iterations
   * };
   * @endcode
   */
  struct throughput {
    std::size_t threads{};

    struct {
      std::optional<std::size_t> min = 1000u;
      /// time budget per single run to estimate required iterations count
      std::optional<std::chrono::duration<double, std::milli>> time_budget = std::chrono::duration<double, std::milli>(1.);
      std::optional<std::size_t> max = 1'000'000u;
    } iterations{};

    struct {
      std::optional<std::size_t> min = 100u;
      /// confidence level to estimate required number of samples, ex. `.95` equals 95% confidence
      std::optional<double> confidence_level = .95;
      std::optional<std::size_t> max = 1000u;
    } samples{};

    template<mp::type_list Ops>
    [[nodiscard]] constexpr auto operator()(auto&& fn, auto&&... ts) const {
      dataset data{};

      auto&& timer = std::apply(
        [](const auto&... ts) { return perf::time::timer{ts...}; },
        mp::filter<[]<class T> { return perf::time::time_like<T>; }, std::tuple>(Ops)
      );
      #if PERF_LINUX == 1
      auto&& counter = std::apply(
        [](const auto&... ts) { return perf::stat::counter{ts...}; },
        mp::filter<[]<class T> { return perf::stat::event_like<T>; }, std::tuple>(Ops)
      );
      auto&& sampler = std::apply(
          // todo should be automated
        [](const auto&... ts) { return perf::record::sampler{/*perf::record::memory::aux::loads, */ts...}; },
        mp::filter<[]<class T> { return perf::record::event_like<T>; }, std::tuple>(Ops)
      );
      #if PERF_INTEL == 1
      auto&& tracer = std::apply(
        [](const auto&... ts) { return perf::trace::tracer{ts...}; },
        mp::filter<[]<class T> { return perf::trace::trace_like<T>; }, std::tuple>(Ops)
      );
      #endif // PERF_INTEL
      #endif // PERF_LINUX

      auto&& t = std::forward_as_tuple(sampler, counter, timer);

      //const auto iterations = [&] {
        //if (this->iterations.time_budget and this->iterations.min) {
          //const auto& time = bench(*this->iterations.min, time::timer{time::steady_clock}, fn, ts...).data[time::steady_clock.name()] / *this->iterations.min;
          //const auto n = std::size_t((std::chrono::duration_cast<std::chrono::nanoseconds>(*this->iterations.time_budget) / time));
          //return std::max(*this->iterations.min, n);
        //} else {
          //verify(this->iterations.max);
          //return *this->iterations.max;
        //}
      //}();

      //const auto samples = [&] {
        //if (this->samples.confidence_level and this->samples.min and this->samples.max) {
          //for (auto i = 0u; i < *this->samples.min; ++i) {
            //data += bench(iterations, t, fn, ts...);
            ////data -= bench<true, 10>(iterations, t, fn, ts...);
          //}

          //constexpr auto fp = []<class T>(const std::vector<T>& t) -> decltype(auto) {// todo
            //if constexpr (requires { typename T::rep; }) {
              //std::vector<typename T::rep> v(t.size());
              //std::transform(t.begin(), t.end(), v.begin(), [](const auto& t) { return t.count(); });
              //return v;
            //} else {
              //return t;
            //}
          //};

          //const auto& time = fp(std::any_cast<const std::vector<decltype(*time::steady_clock)>&>(data[time::steady_clock]));
          //const auto degrees_of_freedom = *this->samples.min - 1u;
          //const auto t_score = stat::t_score(*this->samples.confidence_level, degrees_of_freedom);
          //const auto margin_of_error = (1. - *this->samples.confidence_level) * stat::mean(time);
          //return std::min(std::size_t(std::ceil(std::pow((t_score * stat::std_dev(time)) / margin_of_error, 2.)) - *this->samples.min), *this->samples.max);
        //} else {
          //verify(this->samples.max);
          //return *this->samples.max;
        //}
      //}();

      const auto iterations = 10000;
      const auto samples = 100;

      for (auto i = 0u; i < samples; ++i) {
        data += bench<100, 100>(iterations, t, fn, ts...);
        data -= bench<50, 100>(iterations, t, fn, ts...);
      }

      // todo max ts.size instead of 1000
      //#if PERF_INTEL == 1
      //data += bench(1000, std::forward_as_tuple(tracer), fn, ts...);
      //#endif

      //if constexpr (requires { debug(fn); }) {
        //bench(iterations, std::tuple{}, debug(fn), ts...);
      //}

      return data;
    }

    template<auto N, auto M>
    [[nodiscard]] constexpr auto bench(const std::size_t iterations, auto&& t, auto&& fn, auto&&... ts) const {
      using fn_t = std::remove_cvref_t<decltype(fn)>;

      return [&]<class... TArgs>(mp::type_list<TArgs...>) {
        using invoke_t = void (*)(fn_t, TArgs...);
        std::vector<invoke_t> invoke{};
        invoke.push_back(&throughput::invoke<N, fn_t, TArgs...>);

        return utility::result<decltype(*utility::composite{t})>{
          .iterations = iterations,
          .fn = { .entry = std::uint64_t(mp::type_id<fn_t, utility::fn_entry_tag>),
                  .exit  = std::uint64_t(mp::type_id<fn_t, utility::fn_exit_tag>)},
          .data = std::move(
            bench<N, fn_t, TArgs...>(
              invoke,
              iterations,
              utility::composite{t},
              fn,
              utility::make<TArgs>(ts, iterations).data()...
            )),
        };
      }(typename mp::function_traits<fn_t>::args_type{});
    }

    [[nodiscard]] auto name() const {
      return std::format("throughput{{threads={}}}", threads);
    }

   private:
    template<auto M, class Fn, class... TArgs>
    #if PERF_GNU == 1
    [[gnu::section("dupa")]] [[gnu::noinline]] [[gnu::aligned(64u)]]
    #endif // PERF_GNU
    [[nodiscard]] constexpr auto bench(auto& invoke,
      const std::size_t iterations, auto&& t, auto&& fn, auto&&... ts) const
      requires requires (std::size_t i) { t.start(); t.stop(); *t; } {
      mp::unroll<5u>([&] { t.start(); t.stop(); });

      asm volatile("mfence");
      t.start();
      compiler::prevent_reorder(std::memory_order_seq_cst);

      code::align<64u>(); for (auto i = 0u; i < iterations; i+=50) {
        //invoke[0](fn, ts[i]...);
        fn(ts[i]...);
      }
      compiler::prevent_reorder(std::memory_order_seq_cst);
      t.stop();

      return *t;
    }

    template<auto N, class Fn, class... TArgs>
    #if PERF_GNU == 1
    [[gnu::section("throughput")]] [[gnu::noinline]] [[gnu::aligned(64u)]]
    #endif // PERF_GNU
    static constexpr void invoke(Fn fn, TArgs... ts) {
      constexpr auto fn_entry = mp::type_id<Fn, utility::fn_entry_tag>; // for latency and throughput
      constexpr auto fn_exit = mp::type_id<Fn, utility::fn_exit_tag>;

      mp::unroll<N>([] {
        asm volatile("add   %esi,%edi");
        //asm volatile("pext   %esi,%edi,%eax");
      });
      //[=]<auto...Ns>(std::index_sequence<Ns...>) {
        //([=] {
          //if constexpr (Ns < N) {
            //compiler::prevent_elision(fn(std::get<0>(std::get<Ns>(args)), std::get<1>(std::get<Ns>(args))));
          //} else {
            //compiler::prevent_elision(std::get<Ns>(args));
          //}
        //}(), ...);
      //}(std::make_index_sequence<Y>{});
    }
  };

  struct config {
    /**
     * benchmark name formatter
     */
    struct name {
      /**
       * name format # ex. `{name}({args})`, `max_ipc({args})`, `max_ipc`
       * - {name} - deduced bench name, ex. 'max_ipc'
       * - {args} - name of args..., ex. `1`, `uniform{0, 10}`
       */
      std::string fmt = "{name}({args})";

      [[nodiscard]] constexpr auto operator()(auto&& fn, auto&&... ts) const {
        constexpr std::string_view name_tag = "{name}";
        constexpr std::string_view args_tag = "{args}";

        const auto& fn_name = [&] {
          if constexpr (requires { fn.name(); }) {
            return std::string(fn.name());
          } else {
            const auto& fn_name = [&] -> std::string {
              #if PERF_LLVM == 1
              const auto& qualified_name = info::bin::addr_to_fn_name(
                info::proc::self::name(),
                std::uint64_t(&fn) - info::proc::self::base_address()
              );
              verify(qualified_name.has_value());
              return qualified_name->contains("(")
                ? qualified_name->substr(0u, qualified_name->find("("))
                : *qualified_name;
              #else
              return typeid(fn).name();
              #endif
            }();

            return fn_name.contains(":")
              ? fn_name.substr(fn_name.find_last_of(":") + 1u)
              : fn_name;
          }
        }();

        auto name = this->fmt;

        if (auto name_pos = name.find(name_tag); name_pos != std::string::npos) {
          name.replace(name_pos, name_tag.size(), fn_name);
        }

        if (auto args_pos = name.find(args_tag); args_pos != std::string::npos) {
          std::ostringstream args{};
          std::string comma{};
          ([&] {
            if constexpr (requires { args << ts.name(); }) {
              args << std::exchange(comma, ",") << ts.name();
            } else if constexpr (requires { std::format("{}", ts); }) {
              args << std::exchange(comma, ",") << std::format("{}", ts);
            } else if constexpr (requires { args << ts; }) {
              args << std::exchange(comma, ",") << ts;
            } else {
              static_assert([]<class = decltype(ts)>{ return false; }(),
                "[ERROR] name: argument can't be formatted!"
              );
            }
          }(), ...);

          name.replace(args_pos, args_tag.size(), args.str());
        }

        return name;
      }
    } name{};

    /**
     * computation engine
     * - maps internal representation of the results to expected output (executed by `bench[...]`)
     * - handles user counters
     */
    struct engine {
      /// dsl # ex. `cycles / instructions`
      [[nodiscard]] constexpr auto operator()(auto&& data, auto&& t) const
        requires requires { typename std::remove_cvref_t<decltype(t)>::element_type; } {
        /// compute required dependencies first
        [&]<class... TOps>(mp::type_list<TOps...>) {
          ([&] {
            if (const auto& name = std::string(std::remove_cvref_t<TOps>::name()); not data.contains(name)) {
              data[name] = (*this)(data, TOps{});
            }
          }(), ...);
        }(typename std::remove_cvref_t<decltype(t)>::element_type{});

        return [&]<class... TOps>(mp::type_list<TOps...>) {
          return t(std::any_cast<const decltype((*this)(data, TOps{}))&>(
            data.at(std::string(std::remove_cvref_t<TOps>::name())))...);
        }(typename std::remove_cvref_t<decltype(t)>::element_type{});
      }

      /// counters # iterations, samples
      [[nodiscard]] constexpr auto operator()(auto&& data, auto&& t) const requires requires { t.value; } {
        const auto& name = std::string(t.name());
        verify(data.contains(name), std::format("[ERROR] suite: '{}' not found in the dataset!", name));
        return std::any_cast<std::vector<std::remove_cvref_t<decltype(t.value)>>&>(data.at(name));
      }

      /// time
      [[nodiscard]] constexpr auto operator()(auto&& data, auto&& t) const requires requires { (*t).count(); } {
        const auto& name = std::string(t.name());
        verify(data.contains(name), std::format("[ERROR] suite: '{}' not found in the dataset!", name));
        return std::any_cast<std::vector<std::remove_cvref_t<decltype(*t)>>>(data.at(name));
      }

      #if PERF_LINUX == 1
      /// stat
      [[nodiscard]] constexpr const auto& operator()(auto&& data, perf::stat::event_like auto&& t) const {
        const auto& name = std::string(t.name());
        verify(data.contains(name), std::format("[ERROR] suite: '{}' not found in the dataset!", name));
        return std::any_cast<const std::vector<double>&>(data.at(name));
      }

      #if PERF_LLVM == 1
      /// record
      [[nodiscard]] constexpr auto operator()(auto&& data, perf::record::event_like auto&& t) {
        struct event { double value{}; double total{}; };
        using type = std::remove_cvref_t<decltype(t)>;
        const auto& instructions = (*this)(data, std::vector<mc::instruction>{});
        if constexpr (type{}.kind == record::event::kind::event) {
          const auto& name = std::string(t.name());
          verify(data.contains(name), std::format("[ERROR] suite: '{}' not found in the dataset!", name));
          using event_t = std::unordered_map<std::uint64_t, std::vector<double>>;
          const auto& records = std::any_cast<const event_t&>(data.at(name));
          std::vector<event> events(instructions.size());
          double total{};
          std::size_t i{};
          for (const auto& instruction : instructions) {
            if (records.contains(instruction.ip)) {
              const auto& v = records.at(instruction.ip);
              events[i].value = std::accumulate(v.begin(), v.end(), 0.);
              total += events[i].value;
            }
            ++i;
          }
          for (auto& r : events) {
            r.total = total;
          }
          return events;
        } else if constexpr (type{}.kind == record::event::kind::memory) {
          struct memory { };
          //using memory_t = std::unordered_map<std::uint64_t, std::vector<record::aux::memory>>;
          //const auto& memory = std::any_cast<const memory_t&>(data.at("record.memory"));
          //return std::vector<record>(instructions.size());
          //return std::vector<struct record>(instructions.size());
          //return std::vector<struct record>(instructions.size());
          //todo
        } else if constexpr (type{}.kind == record::event::kind::branch) {
          //return std::vector<struct record>(instructions.size());
        }
      }

      /// trace
      [[nodiscard]] constexpr decltype(auto) operator()(auto&& data, auto&& t)
        requires mc::instruction_like<decltype(t[std::size_t()])> {
        constexpr auto name = "mc.instructions";
        auto&& instructions = data[name];
        if (not data.contains(name) or not data.at(name).has_value()) {
          #if PERF_INTEL == 1
          if (data.contains("trace.traces")) {
            instructions = std::move(mc::disassemble(
              code::labels[std::any_cast<std::uint64_t>(data.at("fn.entry"))],
              code::labels[std::any_cast<std::uint64_t>(data.at("fn.exit"))],
              std::any_cast<const std::vector<struct trace::trace>&>(data.at("trace.traces")),
              llvm)
            );
          } else
          #else
          instructions = std::move(mc::disassemble(
            code::labels[std::any_cast<std::uint64_t>(data.at("fn.entry"))],
            code::labels[std::any_cast<std::uint64_t>(data.at("fn.exit"))],
            llvm)
          )
          #endif // PERF_INTEL
          ;
        }
        verify(data.contains(name));
        return std::any_cast<const std::vector<mc::instruction>&>(instructions);
      }

      /// mc
      [[nodiscard]] constexpr auto operator()(auto&& data, auto&& t)
        requires requires (const mc::instruction& instruction, llvm<arch>& llvm) { t(instruction, llvm); } {
        const auto& instructions = (*this)(data, std::vector<mc::instruction>{});
        std::vector<decltype(t(instructions[std::size_t()], llvm))> v(instructions.size());
        std::size_t i{};
        for (const auto& instruction : instructions) {
          v[i++] = std::move(t(instruction, llvm));
        }
        return v;
      }

      /// mca
      [[nodiscard]] constexpr auto operator()(auto&& data, auto&& t)
        requires requires (const std::vector<mc::instruction>& instructions, llvm<arch>& llvm) { t(instructions, llvm); } {
        verify(data.contains("bench.iterations"));
        const auto iterations = 1u;//std::any_cast<std::uint64_t>(data.at("bench.iteration.real"));
        return std::remove_cvref_t<decltype(t)>{iterations}((*this)(data, std::vector<mc::instruction>{}), llvm);
      }

      backend::arch arch{};
      backend::llvm<decltype(arch)> llvm{arch};
      #endif // PERF_LLVM
      #endif // PERF_LINUX
    } engine{};
  };

  /**
   * runs benchmark for given function and parameters
   * - stores results per benchmark and returns data for the most recently executed
   */
  template<auto Tag = []{}, class TConfig = config, class... TRunners>
    requires requires (TConfig config) { config.name([]{}); config.engine; } and
    (requires (TRunners runner) { runner.name(); } and ...)
  class runner {
    template<class TData> struct result {
      std::string name{};
      std::string runner{};
      TData data{};
    };

    template<class T>
    using result_of_t = std::remove_cvref_t<
      decltype(std::declval<TConfig&&>().engine(
        std::declval<dataset<>&&>(), std::declval<T&&>())
      )
    >;

    template<class... Ts>
    [[nodiscard]] static constexpr auto ops() {
      return ([] {
        if constexpr (requires { typename Ts::element_type; }) { /// compound operator
          []<class... TArgs>(mp::type_list<TArgs...>) {
            return (ops<TArgs>() and  ...);
          }(typename Ts::element_type{});
        } else {
          mp::meta<Ts, mp::tick<Tag>(), Tag>;
        }
        return true;
      }() and ...);
    }

   public:
    constexpr runner(runner&&) = delete;
    constexpr runner(const runner&) = delete;
    //todo
    constexpr explicit runner(TRunners&... runners) : config_{}, runners_{runners...} { }
    constexpr explicit runner(TConfig&& config, TRunners&... runners) : config_{std::move(config)}, runners_{runners...} { }
    constexpr explicit runner(TRunners&&... runners) : config_{}, runners_{std::move(runners)...} { }
    constexpr explicit runner(TConfig&& config, TRunners&&... runners) : config_{std::move(config)}, runners_{std::move(runners)...} { }

    template<class Fn, class... Ts>
      /// non-constexpr is needed for lazy population of `ops` [optional] # run-time performance optimization
      requires (sizeof...(Ts) == perf::mp::size<typename mp::function_traits<std::remove_cvref_t<Fn>>::args_type>)
    auto operator()(Fn&& fn, Ts&&... ts) -> const runner& {
      constexpr auto ops = mp::apply<
        mp::type_list,
        []<std::size_t... Ns>(std::index_sequence<Ns...>) {
          return std::array{Ns...};
        }(std::make_index_sequence<mp::tick<Tag>()>{}),
        Tag
      >();

      std::apply([&](auto&&... runners) {
        ([&] {
          const auto& name =  config_.name(fn, ts...);
          verify(std::find_if(results_.begin(), results_.end(),
            [&](const auto& result) { return result.name == name and result.runner == runners.name(); }) == results_.end(),
            std::format("[ERROR] bench: name '{}' already exist!", name)
          );

          results_.push_back({name, runners.name()});

          if constexpr (requires { runners.template operator()<ops>(fn, ts...); }) {
            results_.back().data = std::move(runners.template operator()<ops>(fn, ts...));
          } else {
            results_.back().data = std::move(runners(fn, ts...));
          }
        }(), ...);
      }, runners_);

      return *this;
    }

    template<class... Ts> requires (ops<Ts...>()) /// `ops` register used operators [optional]
    [[nodiscard]] constexpr auto operator[](const std::tuple<Ts...>& t) { /// non-const as `suite` may add/cache computed results
      std::vector<result<std::tuple<fixed_named<Ts::name(), result_of_t<Ts>>...>>> results(results_.size());
      std::size_t i{};
      for (auto&& [name, runner, data] : results_) {
        results[i].name = name;
        results[i].runner = runner;
        std::apply([&]<class... TArgs>(const TArgs&... args) {
          ([&] {
            auto&& result = std::get<fixed_named<TArgs::name(), result_of_t<TArgs>>>(results[i].data);
            result = fixed_named<TArgs::name(), result_of_t<TArgs>>(config_.engine(data, args));
          }(), ...);
        }, t);
        ++i;
      }
      return results;
    }

    #if __cpp_multidimensional_subscript >= 202110L
    template<class... Ts> [[nodiscard]] constexpr auto operator[](const Ts&... ts)
      -> decltype(this->template operator[](std::tuple{ts...})) {
      return this->template operator[](std::tuple{ts...});
    }
    #endif // __cpp_multidimensional_subscript

    //[[nodiscard]] constexpr auto operator|(const auto& fn) [>const<] {
      //return results_;
    //}

   private:
    TConfig config_{};
    std::tuple<TRunners...> runners_{};
    std::vector<result<dataset<>>> results_{};
  };

  struct iterations { std::size_t value{}; };
  inline constexpr auto iterations = fixed_named<"bench.iterations", struct iterations>{};

  struct samples { std::size_t value{}; };
  inline constexpr auto samples = fixed_named<"bench.samples", struct samples>{};

  namespace misc {
    template<class T, T Begin = {}, T End = {}>
    struct index {
      static constexpr T begin = Begin;
      static constexpr T end = End;
      T value{};
    };

    struct legend {
      std::string_view label{};
    };

    template<class TOs>
    struct split {
      constexpr explicit split(TOs& os, const auto rows)
        : os_{os}, rows_{rows}
      { }

      constexpr auto& operator[](std::size_t i) {
        return streams[i];
      }

      constexpr ~split() {
        constexpr auto is_sep = [](const auto& buffer) { return buffer.empty(); };

        std::vector<std::vector<std::string>> buffers(streams.size());
        std::vector<std::size_t> cols(streams.size());

        std::size_t i{};
        for (const auto& stream : streams) {
          std::istringstream istream{stream.str()};
          std::size_t max_len{};
          std::string line{};
          while (std::getline(istream, line)) {
            max_len = std::max(max_len, line.size());
            buffers[i].push_back(line);
          }
          cols[i] = max_len;
          ++i;
        }

        const auto size = std::max_element(buffers.begin(), buffers.end(),
          [](const auto& lhs, const auto& rhs) { return lhs.size() < rhs.size(); }
        )->size();

        for (auto& buffer : buffers) {
          if (buffer.size() >= size) continue;
          if (auto sep = std::find_if(buffer.rbegin(), buffer.rend(), is_sep); sep != buffer.rend()) {
            buffer.insert(sep.base() - 1u, size - buffer.size(), std::string{});
          }
        }

        for (auto i = 0u; i < size; ++i) {
          for (auto n = 0u; n < buffers.size(); ++n) {
            log(os_, "{}{:{}s}", buffers[n][i], "", cols[n] - buffers[n][i].size() + sizeof(" "));
          }
          log(os_, "\n");
        }
        log(os_, "\n");
      }

     private:
      TOs& os_;
      std::size_t rows_{};
      std::vector<std::ostringstream> streams = std::vector<std::ostringstream>(rows_);
    };
  } // namespace misc

  inline namespace layout {
    enum split { vsplit, hsplit }; /// vertical, horizontal
  } // namespace layout

  /**
   * shows report # per benchmark / per operation
   * @param os ostream # std::cout, std::clog, std::cerr
   * @param results `bench[...]` # perf::time, perf::stat, perf::bench
   * @param stats... statistics # perf::bench::stat::min, perf::bench::stat::max, perf::bench::stat::median, ...
   */
  template<enum layout::split split = layout::hsplit, class TResults> requires (split == layout::hsplit)
  inline constexpr void report(auto& os, const TResults& results_, const auto&... stats) requires (sizeof...(stats) > 0u) {
    if (results_.empty()) {
      return;
    }

    std::map<std::string, TResults> map;
    for (const auto& result : results_) {
      map[result.runner].push_back(result);
    }

    struct rep {
      //todo tuple of types
      std::string runner{};
      std::vector<std::string> stats{};
      std::vector<std::string_view> labels{};
      std::vector<std::vector<std::string>> buffers{};
    };

    std::vector<rep> reports{};
    for (const auto& [runner, results] : map) {
      const auto labels = std::apply([](const auto&... ts) {
        return std::vector{std::string_view(ts.name())...};
      }, results.begin()->data);
      reports.push_back({runner, std::vector{std::string(stats.name())...}, labels, std::vector<std::vector<std::string>>(results.size())});
    }

    std::size_t x{};
    for (const auto& [runner, results] : map) {
      if (results.empty()) {
        continue;
      }

      constexpr auto fp = []<class T>(const std::vector<T>& t) -> decltype(auto) {
        if constexpr (requires { typename T::rep; }) {
          std::vector<typename T::rep> v(t.size());
          std::transform(t.begin(), t.end(), v.begin(), [](const auto& t) { return t.count(); });
          return v;
        } else {
          return t;
        }
      };

      std::unordered_map<std::string_view, std::unordered_map<std::string_view, std::vector<double>>> ratio{};
      ([&](const auto& stat) {
        for (const auto& [name, _, data] : results) {
          std::apply([&](const auto&... ts) {
            (ratio[ts.name()][stat.name()].push_back(stat(fp(ts))), ...);
          }, data);
        }
      }(stats), ...);
      std::size_t n{};
      for (const auto& [name, _, data] : results) {
        reports[x].buffers[n].push_back(std::format(" + {}", name));
        std::apply([&]<class... Ts>(const Ts&... ts) {
          ([&]<class T>(const T& t) {
            struct rep {//todo
              typename T::value_type value{};
              struct {
                typename T::value_type min{};
                typename T::value_type max{};
              } limits{};
            };
            ([&](const auto& stat) {
              const auto value = stat(fp(t));
              reports[x].buffers[n].push_back(std::format("{}", rep{
                .value = typename T::value_type(value), .limits = {
                  .min = typename T::value_type(stat::min(ratio[t.name()][stat.name()])),
                  .max = typename T::value_type(stat::max(ratio[t.name()][stat.name()])),
                }
              }));
            }(stats), ...);
          }(ts), ...);
        }, data);
        ++n;
      }

      ++x;
    }

    log(os, "{}", reports);
  }

  template<enum layout::split split = layout::hsplit> requires (split == layout::hsplit)
  inline constexpr void report(auto& os, const auto& results) {
    report<split>(os, results, stat::median);
  }

  template<enum layout::split split = layout::hsplit> requires (split == layout::hsplit)
  inline constexpr void report(const auto& results, const auto&... stats) {
    report<split>(std::clog, results, stats...);
  }

  template<enum layout::split split = layout::hsplit, class TResults> requires (split == layout::vsplit)
  inline constexpr void report(auto& os, const TResults& results, const auto&... stats) {
    misc::split layout{os, results.size()};
    std::size_t i{};
    for (const auto& result : results) {
      report<layout::hsplit>(layout[i++], TResults(1u, result), stats...);
    }
  }

  template<enum layout::split split = layout::hsplit> requires (split == layout::vsplit)
  inline constexpr void report(const auto& results, const auto&... stats) {
     report<layout::vsplit>(std::clog, results, stats...);
  }

  /**
   * shows annotation # per benchmark / per instruction
   * @param os ostream # std::cout, std::clog, std::cerr
   * @param results bench[...] # perf::mc, perf::mca, perf::record, perf::trace
   */
  template<enum layout::split split = layout::hsplit> requires (split == layout::hsplit)
  inline constexpr void annotate(auto& os, const auto& results) {
    if (results.empty()) {
      return;
    }

    // todo tuple of buffers
    struct annotate {
      decltype(results) results;
    };

    log(os, "{}", annotate{results});
  }

  template<enum layout::split split = layout::hsplit> requires (split == layout::hsplit)
  inline constexpr void annotate(const auto& results) {
    annotate<split>(std::clog, results);
  }

  template<enum layout::split split = layout::hsplit, class TResults> requires (split == layout::vsplit)
  inline constexpr void annotate(auto& os, const TResults& results) {
    misc::split layout{os, results.size()};
    std::size_t i{};
    for (const auto& result : results) {
      annotate<layout::hsplit>(layout[i++], TResults(1u, result));
    }
  }

  template<enum layout::split split = layout::hsplit> requires (split == layout::vsplit)
  inline constexpr void annotate(const auto& results, const auto&... ts) {
    annotate<layout::vsplit>(std::clog, results);
  }

  /**
   * writes results to json (chrome:tracing, perfetto.dev compatible)
   */
  inline constexpr auto json(const auto& path, const auto& results) {
    std::ofstream file{path};
    for (const auto& [name, _, data]: results) {
      std::apply([&](const auto&... ts) {
        ([&] {
          if constexpr (requires { ts.size(); }) {
            log(file, "{}:{}\n", name, ts.size());
          }
        }(), ...);
      }, data);
    }
  }

  /**
   * plotting # `apt get install gnuplot`
   * - http://www.gnuplot.info # gnuplot
   * - https://en.wikipedia.org/wiki/Sixel / https://www.arewesixelyet.com # sixel
   *
   * `gnuplot` is ditrubuted under `gnuplot license`
   * - https://sourceforge.net/p/gnuplot/gnuplot-main/ci/master/tree/Copyright
   */
  namespace plot {
    namespace math {
      inline constexpr auto lin_space =
        []<class T> requires std::is_arithmetic_v<T>(const T start, const T end, const std::size_t size = {}) {
          const auto n = size ? size : static_cast<std::size_t>(end - start + 1u);
          verify(n > 1u);
          std::vector<T> r(n);
          const auto step = (end - start) / static_cast<T>(n - 1u);
          for (auto i = 0u; i < n; ++i) {
            r[i] = start + i * step;
          }
          return r;
        };

      inline constexpr auto bin =
        []<std::ranges::range T>(const T& v, const typename T::value_type start, const typename T::value_type end, const std::size_t size) {
          std::vector<typename T::value_type> r(size);
          verify(size > 1u);
          const auto step = (end - start) / static_cast<typename T::value_type>(size - 1u);
          for (const auto& val : v) {
            ++r[static_cast<std::size_t>((val - start) / step)];
          }
          return r;
        };
    } // namespace math

    /**
     * ENV: `PERF_PLOT_TERM`  - gnuplot terminal (see `set terminal` to supported options in gnuplot)
     * ENV: `PERF_PLOT_STYLE` - light/dark
     */
    struct config {
      std::string cmd = "gnuplot --persist"; /// `persist` is required if popup-based term is used, ex. 'wxt'
      std::size_t width = 1500;
      std::size_t height = 500;
      std::string theme = std::getenv("PERF_PLOT_STYLE") ?  std::getenv("PERF_PLOT_STYLE") : "dark"; /// light/dark [default]
      /**
       * gnuplot: `set terminal`
       * @code
       * sixel                  # console image # default - https://www.arewesixelyet.com
       * wxt                    # popup window
       * dumb size 150,25 ansi  # console with colors
       * dumb size 80,25        # console
       * @endcode
       */
      std::string term = std::getenv("PERF_PLOT_TERM") ?  std::getenv("PERF_PLOT_TERM") : "sixel";
      std::string style = std::format(R"(
        set key outside below vertical left Left reverse nobox spacing 1.5 samplen 4 textcolor rgb '{0}'
        set title textcolor '{0}'
        set xtics textcolor '{0}'
        set ytics textcolor '{0}'
        set xlabel textcolor '{0}'
        set ylabel textcolor '{0}'
        set xrange [*:*]
        set yrange [*:*]
        set style fill transparent solid 0.7
        set style line 1 lt 1 lw 2 pt 7 pi -1 ps 1.0
        set noborder
        set style boxplot nooutliers
        {1}
      )", theme == "dark" ? "white" : "black", term.contains("dumb") ? "" : "set grid");
    };

    /**
     * http://www.gnuplot.info/documentation.html
     * http://www.gnuplot.info/demo
     * https://gnuplot.io
     */
    template<class TConfig = config>
    struct gnuplot {
      constexpr explicit gnuplot(const TConfig& config = {})
        : config_{config}, subprocess_{config_.cmd.c_str(), "w"} {
        send("set term {} noenhanced {} {}\n{}",
          config_.term,
          config_.term.contains("size") ?
            "" :
            std::format(" size {}, {}", config_.width, config_.height),
          config_.term.contains("background") or config.term.contains("dumb") ?  "" :
            std::format(" background '{}'",
              config_.theme == "light" ? "white" : "black",
              config_.width,
              config_.height
            ),
          config_.style
        );
      }

      enum class plot {
        point, /// https://gnuplot.sourceforge.net/demo/circles.html
        line,  /// https://gnuplot.sourceforge.net/demo/simple.html
        bar,   /// https://gnuplot.sourceforge.net/demo/histograms.html
        box,   /// https://gnuplot.sourceforge.net/demo/boxplot.html
        err,   /// https://gnuplot.sourceforge.net/demo/errorbars.html
      };

      constexpr void plot(const auto& x, const auto& y, const auto& name, const std::same_as<plot> auto&... plots) {
        verify(x.size() == y.size(), std::format("[ERROR] plot: x:{} vs y:{}", x.size(), y.size()));
        std::size_t i{};
        const auto value = id(name);
        (series_.push_back({i++, name, plots, std::format("#{}", value * value)}), ...);
        send("$db_{} << EOD", value);
        for (auto i = 0u; i < y.size(); ++i) {
          if constexpr (requires { y[i].begin(); y[i].end(); }) {
            std::stringstream str{};
            for (const auto& y : y[i]) {
              log(str, "{} ", y);
            }
            send("{} {}", x[i], str.str());
          } else {
            send("{} {}", x[i], y[i]);
          }
        }
        send("""EOD");
      }

      constexpr void show() {
        const auto plot_name = [](const auto& value) {
          switch (value) {
            case plot::point: return "points";     /// https://gnuplot.sourceforge.net/demo/circles.html
            case plot::line:  return "line";       /// https://gnuplot.sourceforge.net/demo/simple.html
            case plot::bar:   return "boxes";      /// https://gnuplot.sourceforge.net/demo/histograms.html
            case plot::box:   return "boxplot";    /// https://gnuplot.sourceforge.net/demo/boxplot.html
            case plot::err:   return "errorbars";  /// https://gnuplot.sourceforge.net/demo/errorbars.html
          }
          return "unknown";
        };
        std::string plot = "plot";
        std::size_t i = 1u;
        for (auto&& [id, name, type, color] : series_) {
          send("{} $db_{} {} with {} ls 1 lc rgb '{}' {},\\",
            std::exchange(plot, ""),
            this->id(name),
            type == plot::bar ? "u 1:2" : "",
            plot_name(type),
            color,
            id ? "notitle" : std::format("title '{}'", name)
          );
        }
        send("\n");

        subprocess_.flush();
        series_ = {};
      }

      constexpr void title(const auto& title) { send("set title '{}'", title); }
      constexpr void x_tics() { send("unset xtics"); }
      constexpr void x_tics(const auto& tics) {
        std::ostringstream str{};
        std::string comma{};
        for (const auto& [id, name] : tics) {
          str << std::exchange(comma, ",") << "'" << name << "' " << id;
        }
        send("set xtics ({})", str.str());
      }
      constexpr void x_label(const auto& label) { send("set xlabel '{}'", label); }
      constexpr void y_label(const auto& label) { send("set ylabel '{}'", label); }
      constexpr void x_range(const auto min, const auto max) { send("set xrange [{}:{}]", min, max); }
      constexpr void y_range(const auto min, const auto max) { send("set yrange [{}:{}]", min, max); }
      constexpr void log_scale() { send("set logscale x"); send("set logscale y"); }
      constexpr void width(const auto width) { send("set boxwidth {}", width); }
      constexpr void legend(const bool status) { send("set key {}", std::array{"off", "on"}[status]); }
      constexpr void layout(const std::size_t rows = {}, const std::size_t cols = {}) {
        if (rows and cols) {
          send("set multiplot layout {},{}", rows, cols);
        } else {
          send("unset multiplot");
        }
      }

      template<class... Ts> /// gnuplot specific
      constexpr auto send(std::format_string<Ts...> fmt, Ts&&... ts) -> gnuplot& {
        subprocess_.write((std::format(fmt, std::forward<Ts>(ts)...) + '\n').c_str());
        return *this;
      }

     private:
      struct subprocess {
        constexpr subprocess(const subprocess&) = delete;
        constexpr subprocess(subprocess&& other) : pipe_{std::move(other.pipe_)} { other.pipe_ = {}; }
        constexpr subprocess(const char* cmd, const char* perm) : pipe_{popen(cmd, perm)} { verify(bool(pipe_)); }
        constexpr ~subprocess() noexcept { if (pipe_) { pclose(pipe_); } }
        constexpr void write(const char* cmd) const { fprintf(pipe_, "%s", cmd); }
        constexpr void flush() const { fflush(pipe_); }

       private:
        FILE* pipe_{};
      };

      struct series {
        std::size_t id{};
        std::string name{};
        enum plot plot{};
        std::string color{};
      };

      [[nodiscard]] static constexpr auto id(const auto& str) -> std::uint32_t
        requires requires(std::size_t i) { str.size(); str[i]; } {
        std::uint32_t hash = 0x811C9DC5u;
        for (auto i = 0u; i < str.size(); ++i) {
          hash ^= str[i];
          hash *= 0x01000193u;
        }
        return hash;
      }

      TConfig config_{};
      subprocess subprocess_;
      std::vector<series> series_{};
    };

    namespace detail {
      inline constexpr auto layout = [](auto&& plt, const auto rows, const auto cols) {
        return perf::utility::scoped{
          .on_entry = [&] { plt.layout(rows, cols); },
          .on_exit  = [&] { plt.layout(); },
        };
      };
    } // namespace detail

    /**
     * https://en.wikipedia.org/wiki/Histogram
     */
    template<enum layout::split split = layout::hsplit, class TPlt> requires (split == layout::hsplit)
    inline constexpr auto hist(TPlt& plt, const auto& results) {
      constexpr auto bucket_size = 100u;
      if (results.empty()) {
        return;
      }

      const auto labels = std::apply([](const auto&... ts) {
        return std::array{std::string_view(ts.name())...};
      }, results.begin()->data);

      for (const auto& label : labels) {
        auto min = std::numeric_limits<double>::max();
        auto max = std::numeric_limits<double>::min();

        for (const auto& [name, _, data] : results) {
          std::apply([&](const auto&... ts) {
            ([&] {
              min = std::min(min, stat::min(ts));
              max = std::max(max, stat::max(ts));
            }(), ...);
          }, data);
        }

        plt.title(label);
        plt.x_range(min - min * .1, max + max * .1);
        plt.x_label(misc::legend{label});
        plt.y_label("frequency");
        const auto& x = math::lin_space(min, max, bucket_size);
        for (const auto& [name, _, data] : results) {
          std::apply([&](const auto&... ts) {
            ([&] {
              if (label != std::string_view(ts.name())) return;
              plt.plot(x, math::bin(ts, min, max, bucket_size), name, TPlt::plot::bar);
            }(), ...);
          }, data);
        }
        plt.show();
      }
    }

    template<enum layout::split split = layout::hsplit> requires (split == layout::hsplit)
    inline constexpr auto hist(const auto& results) {
      gnuplot plt{};
      const auto size = std::apply([](const auto&... ts) { return sizeof...(ts); }, results.begin()->data);
      auto _ = detail::layout(plt, 1u, size);
      hist<split>(plt, results);
    }

    template<enum layout::split split = layout::hsplit, class TResults> requires (split == layout::vsplit)
    inline constexpr auto hist(const TResults& results) {
      gnuplot plt{};
      const auto size = std::apply([](const auto&... ts) { return sizeof...(ts); }, results.begin()->data);
      auto _ = detail::layout(plt, 1u, results.size() * size);
      for (const auto& result : results) {
        hist<layout::hsplit>(plt, TResults(1u, result));
      }
    }

    /**
     * https://en.wikipedia.org/wiki/Bar_chart
     */
    template<class TPlt>
    constexpr auto bar(TPlt& plt, const auto& results) {
      if (results.empty()) {
        return;
      }

      constexpr auto error = [](const auto& data) {
        const auto mean = stat::mean(data);
        std::vector<double> gtm{};
        std::vector<double> ltm{};
        std::copy_if(data.begin(), data.end(), std::back_inserter(gtm), [&](const auto& value) {
          return value > mean;
        });
        std::copy_if(data.begin(), data.end(), std::back_inserter(ltm), [&](const auto& value) {
          return value < mean;
        });
        return std::array{mean, mean + stat::std_dev(gtm), mean - stat::std_dev(ltm)};
      };

      const auto labels = std::apply([](const auto&... ts) {
        return std::array{std::string_view(ts.name())...};
      }, results.begin()->data);

      plt.x_tics();

      auto _ = detail::layout(plt, 1u, labels.size());
      plt.width(.75);
      for (const auto& label : labels) {
        plt.title(label);
        plt.y_label(std::format("{} {}", label, misc::legend{label}));
        auto i = 1ul;
        for (const auto& [name, _, data] : results) {
          std::apply([&](const auto&... ts) {
            ([&] {
              if (label != std::string_view(ts.name())) return;
              plt.plot(std::vector{i++}, std::vector{error(ts)}, name, TPlt::plot::bar, TPlt::plot::err);
            }(), ...);
          }, data);
        }
        plt.show();
      }
    }

    constexpr auto bar(const auto& results) {
      gnuplot plt{};
      bar(plt, results);
      gnuplot save{{.term = "svg"}};
      save.send("set output 'output_01.svg'");
      bar(save, results);
    }

    /**
     * https://en.wikipedia.org/wiki/Box_plot
     */
    template<enum layout::split split = layout::hsplit, class TPlt> requires (split == layout::hsplit)
    constexpr auto box(TPlt& plt, const auto& results) {
      if (results.empty()) {
        return;
      }

      const auto labels = std::apply([](const auto&... ts) {
        return std::array{std::string_view(ts.name())...};
      }, results.begin()->data);

      plt.x_tics();

      for (const auto& label : labels) {
        plt.title(label);
        plt.y_label(std::format("{} {}", label, misc::legend{label}));
        auto i = 1ul;
        for (const auto& [name, _, data] : results) {
          std::apply([&](const auto&... ts) {
            ([&] {
              if (label != std::string_view(ts.name())) return;
              plt.plot(math::lin_space(i, i, ts.size()), ts, name, TPlt::plot::box);
              ++i;
            }(), ...);
          }, data);
        }
        plt.show();
      }
    }

    template<enum layout::split split = layout::hsplit> requires (split == layout::hsplit)
    constexpr auto box(const auto& results) {
      gnuplot plt{};
      auto _ = detail::layout(plt, 1u,
        std::apply([](const auto&... ts) { return sizeof...(ts); }, results.begin()->data)
      );
      box<split>(plt, results);
    }

    template<enum layout::split split = layout::hsplit, class TResults> requires (split == layout::vsplit)
    constexpr auto box(const TResults& results) {
      gnuplot plt{};
      const auto size = std::apply([](const auto&... ts) { return sizeof...(ts); }, results.begin()->data);
      auto _ = detail::layout(plt, 1u, results.size() * size);
      for (const auto& result : results) {
        box<layout::hsplit>(plt, TResults(1u, result));
      }
    }

    /**
     * https://en.wikipedia.org/wiki/Line_chart
     * https://en.wikipedia.org/wiki/Error_bar
     */
    template<class TPlt>
    constexpr auto line(TPlt& plt, const auto& results) {
      if (results.empty()) {
        return;
      }

      constexpr auto fn = [](const auto& x, auto fn) {
        std::remove_cvref_t<decltype(x)> y(x.size());
        for (auto i = 0u; i < x.size(); ++i) {
          y[i] = fn(x[i]);
        }
        return y;
      };

      constexpr auto error = [](const auto& data) {
        const auto mean = stat::mean(data);
        std::vector<double> gtm{};
        std::vector<double> ltm{};
        std::copy_if(data.begin(), data.end(), std::back_inserter(gtm), [&](const auto& value) {
          return value > mean;
        });
        std::copy_if(data.begin(), data.end(), std::back_inserter(ltm), [&](const auto& value) {
          return value < mean;
        });
        return std::array{mean, mean + stat::std_dev(gtm), mean - stat::std_dev(ltm)};
      };

      const auto labels = std::apply([](const auto&... ts) {
        return std::array{std::string_view(ts.name())...};
      }, results.begin()->data);

      auto _ = detail::layout(plt, 1u, labels.size());
      for (const auto& label : labels) {
        plt.title(label);
        plt.y_label(misc::legend{label});
        std::map<std::string, std::vector<std::array<double, 3u>>> v{};
        std::map<std::size_t, std::string> xtics{};
        std::size_t i = 1u;

        for (const auto& [name, _, data] : results) {
          std::apply([&](const auto&... ts) {
            ([&] {
              v[name].push_back(error(ts));
              xtics[i] = std::format("{}", i);
              ++i;
            }(), ...);
          }, data);
        }

        plt.x_tics(xtics);

        auto xmax = 0ul;
        auto ymax = 0.;
        for (const auto& [name, data]: v) {
          plt.plot(math::lin_space(1ul, data.size()), data, name, TPlt::plot::line, TPlt::plot::err);
          xmax = std::max(xmax, data.size());
          ymax = std::max(ymax, (*std::max_element(data.begin(), data.end(),
            [](const auto& lhs, const auto& rhs) { return lhs[0] < rhs[0]; }))[0]
          );
        }
        plt.show();
      }
    }

    constexpr auto line(const auto& results) {
      gnuplot plt{};
      line(plt, results);
    }

    /**
     * https://en.wikipedia.org/wiki/Empirical_distribution_function
     */
    template<class TPlt>
    constexpr auto ecdf(TPlt& plt, const auto& results) {
      if (results.empty()) {
        return;
      }

      constexpr auto ecdf = [](auto x) {
        std::sort(x.begin(), x.end());
        std::vector<double> y(x.size());
        for (auto i = 0u; i < y.size(); ++i) {
          y[i] = double(i + 1u) / y.size();
        }
        return std::tuple{x, y};
      };

      const auto labels = std::apply([](const auto&... ts) {
        return std::array{std::string_view(ts.name())...};
      }, results.begin()->data);

      auto _ = detail::layout(plt, 1u, labels.size());
      for (const auto& label : labels) {
        plt.title(label);
        plt.x_label(misc::legend{label});
        plt.y_label("cumulative probability");
        for (const auto& [name, _, data] : results) {
          std::apply([&](const auto&... ts) {
            ([&] {
              if (label != std::string_view(ts.name())) return;
              const auto& [x, y] = ecdf(ts);
              plt.plot(x, y, name, TPlt::plot::point);
            }(), ...);
          }, data);
        }
        plt.show();
      }
    }

    constexpr auto ecdf(const auto& results) {
      gnuplot plt{};
      ecdf(plt, results);
    }
  } // namespace plot
} // namespace bench
} // namespace perf

/**
 * API # bench shortcut
 */
namespace perf {
  using bench::runner;
  using bench::split;
  using bench::report;
  using bench::annotate;

  namespace data {
    using bench::data::repeat;
    using bench::data::range;
    using bench::data::choice;
    using bench::data::uniform;
    using bench::data::normal;
    using bench::data::take;
  } // namespace data

  namespace plot {
    using bench::plot::hist;
    using bench::plot::bar;
    using bench::plot::box;
    using bench::plot::ecdf;
    using bench::plot::line;
  } // namespace plot
} // namespace perf

template<class T>
  requires requires (T t) { t.major; t.minor; t.patch; }
struct std::formatter<T> {
  [[nodiscard]] constexpr auto parse(auto& ctx) { return ctx.begin(); }
  [[nodiscard]] constexpr auto format(const auto& v, auto& ctx) const {
    return std::format_to(ctx.out(), "{}.{}.{}", v.major, v.minor, v.patch);
  }
};

template<>
struct std::formatter<perf::bench::misc::legend> {
  [[nodiscard]] constexpr auto parse(auto& ctx) { return ctx.begin(); }
  [[nodiscard]] constexpr auto format(const auto& t, auto& ctx) const {
    return std::format_to(ctx.out(), "{}", legend[t.label]);
  }
  static inline std::unordered_map<
    decltype(std::declval<perf::bench::misc::legend>().label), std::string
  > legend{}; /// [N]: legend
};

template<class T>
  requires requires (T t) { T::begin; T::end; t.value; }
struct std::formatter<T> {
  [[nodiscard]] constexpr auto parse(auto& ctx) { return ctx.begin(); }
  [[nodiscard]] constexpr auto format(const auto& t, auto& ctx) const {
    std::formatter<perf::bench::misc::legend>::legend["index"] = T::end ?
      std::format("[{}-{}]", T::begin, T::end) :
      std::format("[{}-N]", T::begin);

    return std::format_to(ctx.out(), "{}.", t.value);
  }
};

template<class T>
struct std::formatter<perf::bench::data::repeat<T>> {
  [[nodiscard]] constexpr auto parse(auto& ctx) { return ctx.begin(); }
  [[nodiscard]] constexpr auto format(const auto& t, auto& ctx) const {
    std::format_to(ctx.out(), "repeat{{");
    std::string comma{};
    for (const auto& v : t.values) {
      std::format_to(ctx.out(), "{}{}", std::exchange(comma, ","), v);
    }
    return std::format_to(ctx.out(), "}}");
  }
};

template<class T>
struct std::formatter<perf::bench::data::range<T>> {
  [[nodiscard]] constexpr auto parse(auto& ctx) { return ctx.begin(); }
  [[nodiscard]] constexpr auto format(const auto& t, auto& ctx) const {
    return std::format_to(ctx.out(), "range{{{},{},{}}}", t.start, t.stop, t.step);
  }
};

template<class T, class... Ts>
struct std::formatter<perf::bench::data::uniform<T, Ts...>> {
  [[nodiscard]] constexpr auto parse(auto& ctx) { return ctx.begin(); }
  [[nodiscard]] constexpr auto format(const auto& t, auto& ctx) const {
    return std::format_to(ctx.out(), "uniform{{{},{},{}}}", t.min, t.max, t.seed);
  }
};

template<class T, class... Ts>
struct std::formatter<perf::bench::data::normal<T, Ts...>> {
  [[nodiscard]] constexpr auto parse(auto& ctx) { return ctx.begin(); }
  [[nodiscard]] constexpr auto format(const auto& t, auto& ctx) const {
    return std::format_to(ctx.out(), "normal{{{},{},{}}}", t.mean, t.std_dev, t.seed);
  }
};

template<class T, class... Ts>
struct std::formatter<perf::bench::data::choice<T, Ts...>> {
  [[nodiscard]] constexpr auto parse(auto& ctx) { return ctx.begin(); }
  [[nodiscard]] constexpr auto format(const auto& t, auto& ctx) const {
    std::format_to(ctx.out(), "{{");
    std::string comma{};
    for (const auto& v : t.values) {
      std::format_to(ctx.out(), "{}{}", std::exchange(comma, ","), v);
    }
    std::format_to(ctx.out(), "}},{{");
    comma = {};
    for (const auto& p : t.probabilities) {
      std::format_to(ctx.out(), "{}{}", std::exchange(comma, ","), p);
    }
    return std::format_to(ctx.out(), "}}");
  }
};

template<class T>
  requires requires (T t) { t.family; t.model; t.stepping; }
struct std::formatter<T> {
  [[nodiscard]] constexpr auto parse(auto& ctx) { return ctx.begin(); }
  [[nodiscard]] constexpr auto format(const auto& v, auto& ctx) const {
    return std::format_to(ctx.out(),
      "{}.{}.{}",
      v.family,
      v.model,
      v.stepping
    );
  }
};

template<class T>
  requires requires (T t) { t.size; t.line_size; t.assoc; }
struct std::formatter<T> {
  [[nodiscard]] constexpr auto parse(auto& ctx) { return ctx.begin(); }
  [[nodiscard]] constexpr auto format(const auto& cache, auto& ctx) const {
    constexpr auto size = [](const std::size_t value) {
      if (value >= 1024u * 1024u) {
        return std::format("{}Mb", value / (1024u * 1024u));
      } else if (value >= 1024u) {
        return std::format("{}Kb", value / 1024u);
      } else {
        return std::format("{}b", value);
      }
    };

    return std::format_to(ctx.out(),
      "{}: {} ({}b)",
      size(cache.size),
      cache.cache_line_size
    );
  }
};

#if PERF_LLVM == 1
template<>
struct std::formatter<perf::mc::assembly::value_type> {
  [[nodiscard]] constexpr auto parse(auto& ctx) { return ctx.begin(); }
  [[nodiscard]] constexpr auto format(const auto& t, auto& ctx) const {
    std::formatter<perf::bench::misc::legend>::legend["mc.assembly"] = std::format("# {}",
      t.syntax == decltype(t.syntax)::intel ? "intel" : "at&t"
    );
    std::string str{};
    auto i = 0u;
    while (t.str[i] == '\t' or t.str[i] == ' ') ++i;
    for (; i < t.str.size(); ++i) {
      str += t.str[i] == '\t' ? ' ' : t.str[i];
    }
    return std::format_to(ctx.out(), "{}", str);
  }
};

template<>
struct std::formatter<perf::mc::address::value_type> {
  [[nodiscard]] constexpr auto parse(auto& ctx) { return ctx.begin(); }
  [[nodiscard]] constexpr auto format(const auto& t, auto& ctx) const {
    const auto hex = [&](const std::uint64_t ip, const std::size_t size) {
      std::stringstream str{};
      const auto* data = reinterpret_cast<const std::uint8_t*>(ip);
      str << std::format("{:04x}", (ip - base_address) & 0xFFFF);
      return str.str();
    };
    return std::format_to(ctx.out(), "{}", hex(t.ip, t.size));
  }

 private:
  inline static const auto base_address = perf::info::proc::self::base_address();
};

template<>
struct std::formatter<perf::mc::encoding::value_type> {
  [[nodiscard]] constexpr auto parse(auto& ctx) { return ctx.begin(); }
  [[nodiscard]] constexpr auto format(const auto& t, auto& ctx) const {
    const auto hex = [&](const std::uint64_t ip, const std::size_t size) {
      std::stringstream str{};
      const auto* data = reinterpret_cast<const std::uint8_t*>(ip);
      for (auto i = 0u; i < size; ++i) {
        str << std::format("{:02x} ", data[i]);
      }
      return str.str();
    };
    return std::format_to(ctx.out(), "{}", hex(t.ip, t.size));
  }

 private:
  inline static const auto base_address = perf::info::proc::self::base_address();
};

template<>
struct std::formatter<perf::mc::size::value_type> {
  [[nodiscard]] constexpr auto parse(auto& ctx) { return ctx.begin(); }
  [[nodiscard]] constexpr auto format(const auto& t, auto& ctx) const {
    return std::format_to(ctx.out(), "{}b", t.value);
  }
};

template<>
struct std::formatter<perf::mc::uops::value_type> {
  [[nodiscard]] constexpr auto parse(auto& ctx) { return ctx.begin(); }
  [[nodiscard]] constexpr auto format(const auto& t, auto& ctx) const {
    return std::format_to(ctx.out(), "{}", t.value);
  }
};

template<>
struct std::formatter<perf::mc::latency::value_type> {
  [[nodiscard]] constexpr auto parse(auto& ctx) { return ctx.begin(); }
  [[nodiscard]] constexpr auto format(const auto& t, auto& ctx) const {
    return std::format_to(ctx.out(), "{}", t.value);
  }
};

template<>
struct std::formatter<perf::mc::rthroughput::value_type> {
  [[nodiscard]] constexpr auto parse(auto& ctx) { return ctx.begin(); }
  [[nodiscard]] constexpr auto format(const auto& t, auto& ctx) const {
    return std::format_to(ctx.out(), "{:.1f}", t.value);
  }
};

template<>
struct std::formatter<perf::mc::may_load::value_type> {
  [[nodiscard]] constexpr auto parse(auto& ctx) { return ctx.begin(); }
  [[nodiscard]] constexpr auto format(const auto& t, auto& ctx) const {
    std::formatter<perf::bench::misc::legend>::legend["mc.may_load"] = "{'*':yes, '':no}";
    return std::format_to(ctx.out(), "{}", t.value ? "*" : "");
  }
};

template<>
struct std::formatter<perf::mc::may_store::value_type> {
  [[nodiscard]] constexpr auto parse(auto& ctx) { return ctx.begin(); }
  [[nodiscard]] constexpr auto format(const auto& t, auto& ctx) const {
    std::formatter<perf::bench::misc::legend>::legend["mc.may_store"] = "{'*':yes, '':no}";
    return std::format_to(ctx.out(), "{}", t.value ? "*" : "");
  }
};

template<>
struct std::formatter<perf::mc::has_side_effects::value_type> {
  [[nodiscard]] constexpr auto parse(auto& ctx) { return ctx.begin(); }
  [[nodiscard]] constexpr auto format(const auto& t, auto& ctx) const {
    std::formatter<perf::bench::misc::legend>::legend["mc.has_side_effects"] = "{'U':yes, '':no}";
    return std::format_to(ctx.out(), "{}", t.value ? "U" : "");
  }
};

template<>
struct std::formatter<perf::mc::branch::is_conditional::value_type> {
  [[nodiscard]] constexpr auto parse(auto& ctx) { return ctx.begin(); }
  [[nodiscard]] constexpr auto format(const auto& t, auto& ctx) const {
    std::formatter<perf::bench::misc::legend>::legend["mc.branch.is_conditional"] = "{'*':yes, '':no}";
    return std::format_to(ctx.out(), "{}", t.value ? "*" : "");
  }
};

template<>
struct std::formatter<perf::mc::branch::is_unconditional::value_type> {
  [[nodiscard]] constexpr auto parse(auto& ctx) { return ctx.begin(); }
  [[nodiscard]] constexpr auto format(const auto& t, auto& ctx) const {
    std::formatter<perf::bench::misc::legend>::legend["mc.branch.is_unconditional"] = "{'*':yes, '':no}";
    return std::format_to(ctx.out(), "{}", t.value ? "*" : "");
  }
};

template<>
struct std::formatter<perf::mc::branch::is_indirect::value_type> {
  [[nodiscard]] constexpr auto parse(auto& ctx) { return ctx.begin(); }
  [[nodiscard]] constexpr auto format(const auto& t, auto& ctx) const {
    std::formatter<perf::bench::misc::legend>::legend["mc.branch.is_indirect"] = "{'*':yes, '':no}";
    return std::format_to(ctx.out(), "{}", t.value ? "*" : "");
  }
};

template<>
struct std::formatter<perf::mc::branch::is_call::value_type> {
  [[nodiscard]] constexpr auto parse(auto& ctx) { return ctx.begin(); }
  [[nodiscard]] constexpr auto format(const auto& t, auto& ctx) const {
    std::formatter<perf::bench::misc::legend>::legend["mc.branch.is_call"] = "{'*':yes, '':no}";
    return std::format_to(ctx.out(), "{}", t.value ? "*" : "");
  }
};

template<>
struct std::formatter<perf::mc::branch::is_ret::value_type> {
  [[nodiscard]] constexpr auto parse(auto& ctx) { return ctx.begin(); }
  [[nodiscard]] constexpr auto format(const auto& t, auto& ctx) const {
    std::formatter<perf::bench::misc::legend>::legend["mc.branch.is_ret"] = "{'*':yes, '':no}";
    return std::format_to(ctx.out(), "{}", t.value ? "*" : "");
  }
};

template<>
struct std::formatter<perf::mc::source::value_type> {
  [[nodiscard]] constexpr auto parse(auto& ctx) { return ctx.begin(); }
  [[nodiscard]] constexpr auto format(const auto& t, auto& ctx) const {
    std::formatter<perf::bench::misc::legend>::legend["mc.source"] = "# requires (-g)";
    return std::format_to(ctx.out(), "{}", t.value);
  }
};

template<class T>
  requires requires (T t, std::size_t i) {
    t.units[i].name;
    t.units[i].pressure;
  }
struct std::formatter<T> {
  [[nodiscard]] constexpr auto parse(auto& ctx) { return ctx.begin(); }
  [[nodiscard]] constexpr auto format(const auto& t, auto& ctx) const {
    auto&& legend = std::formatter<perf::bench::misc::legend>::legend;
    auto&& out = ctx.out();

    legend["mca.resource_pressure"] = {};
    std::size_t i{};
    for (const auto& [name, pressure] : t.units) {
      legend["mca.resource_pressure"] += std::format("\n\t- {}: {}" , i++, name);
      if (pressure) {
        std::format_to(out, "{:>6.2f}", pressure);
      } else {
        std::format_to(out, "{:>6}", "-");
      }
    }
    return out;
  }
};

template<class T>
  requires requires (T t) {
    *t.cycle_dispatched;
    t.cycle_ready;
    t.cycle_issued;
    t.cycle_executed;
    t.cycle_retired;
    t.dispatch_width;
    t.last_cycle;
  }
struct std::formatter<T> {
  static constexpr auto dispatched = 'D';
  static constexpr auto executed = 'E';
  static constexpr auto retired = 'R';
  static constexpr auto waiting = '=';
  static constexpr auto executing = 'e';
  static constexpr auto retire_lag = '-';

  [[nodiscard]] constexpr auto parse(auto& ctx) { return ctx.begin(); }
  [[nodiscard]] constexpr auto format(const auto& entry, auto& ctx) const {
    std::formatter<perf::bench::misc::legend>::legend["mca.timeline"] = std::format(
      "\n  - 'D': instruction dispatched"
      "\n  - 'e': instruction executing"
      "\n  - 'E': instruction executed"
      "\n  - 'R': instruction retired"
      "\n  - '=': instruction waiting"
      "\n  - '-': instruction executed"
    );

    perf::verify(entry.cycle_dispatched.has_value());

    auto&& out = ctx.out();

    for (auto i = 0u; i < *entry.cycle_dispatched; ++i) {
      out = std::format_to(out, "{}", i % entry.dispatch_width ? ' ' : '.');
    }
    out = std::format_to(out, "{}", dispatched);

    if (*entry.cycle_dispatched != entry.cycle_executed) {
      for (auto i = *entry.cycle_dispatched + 1u; i < entry.cycle_issued; ++i) {
        out = std::format_to(out, "{}", waiting);
      }

      if (entry.cycle_issued == entry.cycle_executed) {
        out = std::format_to(out, "{}", executed);
      } else {
        if (*entry.cycle_dispatched != entry.cycle_issued) {
          out = std::format_to(out, "{}", executing);
        }

        for (auto i = entry.cycle_issued + 1u; i < entry.cycle_executed; ++i) {
          out = std::format_to(out, "{}", executing);
        }

        out = std::format_to(out, "{}", executed);
      }
    }

    for (auto i = entry.cycle_executed + 1u; i < entry.cycle_retired; ++i) {
      out = std::format_to(out, "{}", retire_lag);
    }

    if (entry.cycle_executed < entry.cycle_retired) {
      out = std::format_to(out, "{}", retired);
    }

    for (auto i = entry.cycle_retired + 1u; i <= entry.last_cycle; ++i) {
      out = std::format_to(out, "{}", i % entry.dispatch_width ? ' ' : '.');
    }

    return out;
  }
};

template<class T>
  requires requires (T t) {
    t.dependency;
    *t.name;
    *t.freq;
    *t.cost;
  }
struct std::formatter<T> {
  [[nodiscard]] constexpr auto parse(auto& ctx) { return ctx.begin(); }
  [[nodiscard]] constexpr auto format(const auto& t, auto& ctx) const {
    std::formatter<perf::bench::misc::legend>::legend["mca.bottleneck"] = std::format(
      "\n  - <register name>"
      "\n  - [memory access]"
      "\n  - {{resource name:probability}}"
    );
    switch (t.dependency) {
      default: {
        return std::format_to(ctx.out(), "");
      }
      case decltype(t.dependency)::reg: {
        perf::verify(t.name.has_value());
        return std::format_to(ctx.out(), "<{}>", *t.name);
      }
      case decltype(t.dependency)::mem: {
        return std::format_to(ctx.out(), "[loads/stores]");
      }
      case decltype(t.dependency)::res: {
        perf::verify(t.name.has_value() and t.freq.has_value());
        return std::format_to(ctx.out(), "{{{}:{}%}}", *t.name, *t.freq);
      }
    }
  }
};
#endif // PERF_LLVM

template<class T>
  requires requires (T t) { t.value; t.total; }
struct std::formatter<T> {
  [[nodiscard]] constexpr auto parse(auto& ctx) { return ctx.begin(); }
  [[nodiscard]] constexpr auto format(const auto& t, auto& ctx) const {
    return std::format_to(ctx.out(), "{:.2f}%", 100. * (t.value / t.total));
  }
};

template<class T>
  requires requires (T t) { t.value; t.limits.min; t.limits.max; }
struct std::formatter<T> {
  [[nodiscard]] constexpr auto parse(auto& ctx) { return ctx.begin(); }
  [[nodiscard]] constexpr auto format(const auto& t, auto& ctx) const {
    if constexpr (std::floating_point<decltype(t.value)>) {
      const auto max_len = std::format("{:.2f}", t.limits.max).size();
      return std::format_to(ctx.out(), " ({:.2f}x) {:>{}}",
        t.value ? t.limits.min / t.value : 0., std::format("{:.2f}", t.value), max_len
      );
    } else if constexpr (requires { t.value.count(); t.limits.min.count(); t.limits.max.count(); } ) {
      const auto max_len = std::format("{:.2f}", t.limits.max.count()).size();
      const auto unit = [](const auto value) {
        if (value >= 1e9) {
          return std::format("{:.2f}s", value / 1e9);
        }
        if (value >= 1e6) {
          return std::format("{:.2f}ms", value / 1e6);
        }
        if (value >= 1e3) {
          return std::format("{:.2f}µs", value / 1e3);
        }
        return std::format("{:.2f}ns", value);
      };
      return std::format_to(ctx.out(), " ({:.2f}x) {:>{}}",
        t.value.count() ? double(t.limits.min.count()) / double(t.value.count()) : 0., unit(t.value.count()), max_len
      );
    } else {
      const auto max_len = std::format("{}", t.limits.max).size();
      return std::format_to(ctx.out(), " ({:.2f}x) {:>{}}",
        t.value ? double(t.limits.min) / double(t.value) : 0., std::format("{}", t.value), max_len
      );
    }
  }
};

template<class T>
  requires requires (T t, std::size_t i) { t[i].runner; t[i].buffers; t[i].labels; t[i].stats; }
struct std::formatter<T> {
  static constexpr std::string_view title = "benchmark";

  [[nodiscard]] constexpr auto parse(auto& ctx) { return ctx.begin(); }
  [[nodiscard]] constexpr auto format(const auto& t, auto& ctx) const {
    auto&& out = ctx.out();

    std::vector<std::size_t> len(t[0].buffers[0].size());
    for (const auto& ts : t) {
      auto&& buffers = ts.buffers;
      perf::verify(not buffers.empty());
      len[0] = 1u + title.size();
      for (const auto& buffer : buffers) {
        perf::verify(not buffer.empty());
        for (auto i = 0u; i < buffer.size(); ++i) {
          len[i] = std::max(len[i], buffer[i].size());
        }
      }
    }

    out = std::format_to(out, "{:<{}}{}", title, len[0u], ' ');
    for (auto i = 1u; i < len.size(); ++i) {
      out = std::format_to(out, "{:>{}}{}", std::format("[{}]", i), len[i], ' ');
    }
    out = std::format_to(out, "\n");
    for (auto i = 0u; i < len.size(); ++i) {
      out = std::format_to(out, "{:->{}}{}", "", len[i], ' ');
    }

    for (const auto& ts : t) {
      out = std::format_to(out, "\n{}\n", ts.runner);
      for (const auto& buffer : ts.buffers) {
        perf::verify(not buffer.empty());
        out = std::format_to(out, "{:<{}}{}", buffer[0u], len[0u], ' ');
        for (auto i = 1u; i < buffer.size(); ++i) {
          out = std::format_to(out, "{:>{}}{}", buffer[i], len[i], ' ');
        }
        out = std::format_to(out, "\n");
      }
    }

    out = std::format_to(out, "\n");
    out = std::format_to(out, "(x) relative # ratio\n");
    std::size_t id = 1u;
    for (const auto& label : t[0].labels) {
      for (const auto& stat : t[0].stats) {
        out = std::format_to(out, "[{}] {}({}) {}\n",
          id++,
          stat, label, perf::bench::misc::legend{label}
        );
      }
    }
    return std::format_to(out, "\n");
  }
};

template<class T>
  requires requires (T t) { t.results; }
struct std::formatter<T> {
  [[nodiscard]] constexpr auto parse(auto& ctx) { return ctx.begin(); }
  [[nodiscard]] constexpr auto format(const auto& t, auto& ctx) const {
    auto&& out = ctx.out();

    std::map<std::string, std::remove_cvref_t<decltype(t.results)>> map{};
    for (const auto& result : t.results) {
      map[result.runner].push_back(result);
    }

    for (const auto& [runner, results] : map) {
      using index_t = perf::bench::misc::index<std::size_t, 1u>;
      std::vector<std::map<std::string_view, std::vector<std::string>>> buffers(results.size());
      std::size_t n{};
      for (const auto& [name, _, data] : results) {
        std::apply([&](const auto&... ts) {
          ([&] {
            auto&& buffer = buffers[n][std::string_view(ts.name())];
            index_t index{1u};
            for (const auto& t : ts) {
              if (auto& v = buffers[n]["."]; v.size() < index.value) {
                v.push_back(std::format("{}", index));
              }
              buffer.push_back(std::format("{}", t));
              index.value++;
            }
          }(), ...);
        }, data);
        ++n;
      }

      //todo
      auto labels = std::apply([](const auto&... ts) {
        return std::vector{std::string_view{"."}, std::string_view{ts.name()}...};
      }, results.begin()->data);

      std::vector<std::size_t> len(labels.size(), sizeof(" "));
      for (auto i = 0u; i < labels.size(); ++i) {
        for (auto&& buffer : buffers) {
          perf::verify(buffer.contains(labels[i]) and not buffer[labels[i]].empty(),
            std::format("[ERROR] annotate: '{}' can't be handled!", labels[i])
          );
          const auto size = std::max_element(buffer[labels[i]].begin(), buffer[labels[i]].end(),
            [](const auto& lhs, const auto& rhs) { return lhs.size() < rhs.size(); }
          )->size();
          len[i] = std::max(len[i], size);
        }
      }

      constexpr auto is_num = [](const auto& str) {
        return std::all_of(str.begin(), str.end(), [](const auto c) {
          return std::isdigit(c) or c == '%' or c == '.' or c == '-';
        });
      };

      n = {};
      //out = std::format_to(out, "{}\n", results[0].runner);
      for (const auto& [name, runner, _] : results) {
        out = std::format_to(out, "{}:", name);
        out = std::format_to(out, "\n{}", ' ');
        for (auto i = 0u; i < len.size(); ++i) {
          out = std::format_to(out, "{:<{}}{}", std::format("[{}]", i), len[i] + 1u, ' ');
        }
        out = std::format_to(out, "\n{}", ' ');
        for (auto i = 0u; i < len.size(); ++i) {
          out = std::format_to(out, "{:->{}}{}", "", len[i] + 1u, ' ');
        }
        out = std::format_to(out, "\n");

        perf::verify(not len.empty() and buffers[n].size() == len.size());

        for (auto i = 0u; i < buffers[n].begin()->second.size(); ++i) {
          out = std::format_to(out, "{}", ' ');
          for (auto l = 0u; l < labels.size(); ++l) {
            perf::verify(buffers[n].contains(labels[l]));
            const auto& str = buffers[n][labels[l]][i];
            if (is_num(str)) {
              out = std::format_to(out, "{:>{}} {}", str, len[l], ' ');
            } else {
              out = std::format_to(out, "{:<{}} {}", str, len[l], ' ');
            }
          }
          out = std::format_to(out, "\n");
        }
        ++n;
        out = std::format_to(out, "\n");
      }

      for (auto i = 0u; i < labels.size(); ++i) {
        out = std::format_to(out, "{}[{}] {} {}\n", ' ', i, labels[i], perf::bench::misc::legend{labels[i]});
      }
    }

    return out;
  }
};

/**
 * `-DNTEST` #default: not defined
 * - disables compile-time tests
 * - disables run-time tests (`perf::test::run`)
 */
#ifndef NTEST
namespace perf::test {
  /**
   * test runner
   * - tests marked `consteval` run only at compile-time
   * - tests marked `constexpr` run at compile-time and run-time
   * - tests marked `mutable` run only at run-time
   */
  template<fixed_string Name>
  struct test {
    constexpr void operator=(auto fn) const {
      if constexpr (constexpr auto non_mutable =
        requires { []<class R, class B, class... Ts>(R (B::*)(Ts...) const) { }
          (&decltype(fn)::operator()); }; non_mutable) {
        if constexpr (requires { fn(); }) {
          static_assert((fn(), true)); /// run at compile-time unless fn is marked `mutable`
        }
      }
      if constexpr (requires { fn(); }) {
        if (test<"*">::verbose) {
          log(std::cout, "  {} ", std::string_view(Name));
        }
        fn(); /// run at run-time unless fn is marked `consteval`
        if (test<"*">::verbose) {
          log(std::cout, "✔️\n", std::string_view(Name));
        }
      }
    }
    static inline bool verbose{};
  };

  template<fixed_string Name>
  inline constexpr auto operator""_test() { return test<Name>{}; }

  template<fixed_string Name>
  struct suite {
    constexpr void operator=(auto fn) const {
      if constexpr (requires { fn(); }) {
        if (suite<"*">::verbose) {
          log(std::cout, "{}\n", std::string_view(Name));
        }
        fn();
      }
    }
    static inline bool verbose{};
  };

  template<fixed_string Name>
  inline constexpr auto operator""_suite() { return suite<Name>{}; }

  template<fixed_string>
  struct skip { };

  inline constexpr auto expect =
    []<auto abort = [] { std::abort(); }, class TMsg = std::string_view>(
      const std::same_as<bool> auto cond, const TMsg& msg = {},
      const std::source_location location = std::source_location::current()
    ) requires requires { log(msg); } {
    if consteval {
      if (not cond) {
        void failed(); failed();
      }
    } else {
      if (not cond) {
        log(std::cout, "{}:{}:FAILED:{}\n", location.file_name(), location.line(), msg);
        abort();
      }
    }
  };

  namespace fake {
    inline const auto args_0 = []{};
    inline const auto args_1 = [](int){};

    template<fixed_string Name, auto Time>
    struct time {
      constexpr time() {
        start_calls = {};
        stop_calls = {};
      }

      constexpr void start() { ++start_calls; }
      constexpr void stop() { ++stop_calls; }

      [[nodiscard]] constexpr auto operator*() const {
        return std::chrono::duration<double, std::nano>(Time);
      }

      [[nodiscard]] static constexpr auto name() {
        return std::string_view(Name);
      }

      static inline std::size_t start_calls{};
      static inline std::size_t stop_calls{};
    };

    template<std::array Vs>
    struct random_suite {
      using result_type = decltype(Vs)::value_type;
      constexpr explicit random_suite(auto&&...) { }
      [[nodiscard]] constexpr auto operator()() const {
        verify(i < Vs.size());
        return Vs[i++];
      }
      [[nodiscard]] static constexpr auto min() {
        return *std::min_element(Vs.begin(), Vs.end());
      }
      [[nodiscard]] static constexpr auto max() {
        return *std::max_element(Vs.begin(), Vs.end());
      }
     private:
      mutable std::size_t i{};
    };

    template<class T>
    struct distribution {
      using result_type = T;
      constexpr explicit distribution(auto&&...) { }
      [[nodiscard]] constexpr auto operator()(const auto& suite) const {
        return suite();
      }
    };
  } // namespace fake

  struct config {
    bool verbose = false;
  };

  inline void run(const config& config = {}) {
    scoped _{
      .on_entry = [&] {
        if (config.verbose) {
          log(std::cout, "test\n{:->{}}\n", "", 16u);
          "*"_suite.verbose = true;
          "*"_test.verbose = true;
        }
      },
      .on_exit = [&] {
        if (config.verbose) {
          log(std::cout, "\n");
        }
      },
    };

    "utility"_suite = [] {
      "verify"_test = [] consteval {
        utility::verify(true);
        utility::verify(true, "[ERROR]");
      };

      "fixed_string"_test = [] consteval {
        using std::literals::string_view_literals::operator""sv;
        expect(0u == fixed_string{""}.size());
        expect(sizeof("perf") - 1u == fixed_string{"perf"}.size());
        expect('p' == fixed_string{"perf"}[0]);
        expect('e' == fixed_string{"perf"}[1]);
        expect('r' == fixed_string{"perf"}[2]);
        expect('f' == fixed_string{"perf"}[3]);
        expect(fixed_string{"perf"} == fixed_string{"perf"});
        expect(fixed_string{"perf"} != fixed_string{"PERF"});
        expect("perf"sv == std::string_view{fixed_string{"perf"}});
      };

      "named"_test = [] consteval {
        using std::literals::string_view_literals::operator""sv;
        struct named { int i{}; };
        expect("name"sv == utility::named{"name", named{}}.name());
        expect(42 == utility::named{"name", named{.i = 42}}.i);
      };

      "scoped"_test = [] constexpr {
        auto on_entry = false;
        auto on_exit = false;

        {
          scoped _ {
            .on_entry = [&] { on_entry = true; },
            .on_exit  = [&] { on_exit = true; },
          };

          expect(on_entry);
          expect(not on_exit);
        }

        expect(on_entry);
        expect(on_exit);
      };
    };

    "mp"_suite = [] {
      "type_id"_test = [] consteval {
        expect(mp::type_id<void> == mp::type_id<void>);
        expect(mp::type_id<int> != mp::type_id<void>);
        expect(mp::type_id<int> == mp::type_id<int>);
        expect(mp::type_id<int> != mp::type_id<const int&>);
        expect(mp::type_id<int, void> == mp::type_id<int, void>);
        expect(mp::type_id<int, void> != mp::type_id<void, int>);
      };

      "size"_test = [] consteval {
        expect(0u == mp::size<void>);
        expect(0u == mp::size<mp::type_list<>>);
        expect(1u == mp::size<mp::type_list<void>>);
        expect(2u == mp::size<mp::type_list<int, void>>);
        expect(2u == mp::size<std::tuple<int, float>>);
        expect(2u == mp::size<std::variant<int, float>>);
      };

      "unroll"_test = [] consteval {
        {
          auto calls = 0u;
          mp::unroll<0>([&]{ ++calls; });
          expect(not calls);
        }

        {
          auto calls = 0u;
          mp::unroll<1>([&]{ ++calls; });
          expect(calls == 1u);
        }

        {
          auto calls = 0u;
          mp::unroll<2>([&]{ ++calls; });
          expect(calls == 2u);
        }

        {
          auto calls = 0u;
          mp::unroll<3>([&]<auto N>{ calls += N; });
          expect(calls == 0u + 1u + 2u);
        }
      };

      #if not __cpp_reflection
      "meta"_test = [] consteval {
        expect(typeid(mp::meta<void>) == typeid(mp::meta<int>));
        expect(mp::meta<void> != mp::meta<int>);
        expect(mp::meta<void> == mp::meta<void>);
        expect(mp::meta<int> == mp::meta<int>);
      };

      "info"_test = [] consteval {
        expect(2u == std::vector<mp::info>{
          mp::meta<int>,
          mp::meta<void>
        }.size());
        expect(typeid(mp::info) == typeid(mp::meta<void>));
        expect(typeid(mp::info) == typeid(mp::meta<int>));
      };

      "apply"_test = [] consteval {
        expect(
          typeid(mp::type_list<>) ==
          typeid(mp::apply<mp::type_list, std::array<mp::info, 0u>{}>())
        );

        expect(
          typeid(mp::type_list<void>) ==
          typeid(mp::apply<mp::type_list, std::array{mp::meta<void>}>())
        );

        expect(
          typeid(mp::type_list<int, void>) ==
          typeid(mp::apply<mp::type_list,
            std::array{
              mp::meta<int>,
              mp::meta<void>
            }>())
        );

        expect(
          typeid(mp::type_list<const int&, const void*, void*>) ==
          typeid(mp::apply<
            mp::type_list,
            std::array{
              mp::meta<const int&>,
              mp::meta<const void*>,
              mp::meta<void*>
            }>()
          )
        );
      };

      "filter"_test = [] consteval {
        expect(
          typeid(mp::type_list<>) ==
          typeid(mp::filter<[]<class> { return true; }, mp::type_list>(
            mp::type_list<>{}
          ))
        );

        expect(
          typeid(mp::type_list<>) ==
          typeid(mp::filter<[]<class> { return false; }, mp::type_list>(
            mp::type_list<void>{}
          ))
        );

        expect(
          typeid(mp::type_list<void>) ==
          typeid(mp::filter<[]<class> { return true; }, mp::type_list>(
            mp::type_list<void>{}
          ))
        );

        expect(
          typeid(mp::type_list<int, void>) ==
          typeid(mp::filter<[]<class> { return true; }, mp::type_list>(
            mp::type_list<int, void>{}
          ))
        );

        expect(
          typeid(mp::type_list<>) ==
          typeid(mp::filter<[]<class> { return false; }, mp::type_list>(
            mp::type_list<int, void>{}
          ))
        );

        expect(
          typeid(mp::type_list<void>) ==
          typeid(mp::filter<[]<class T> { return std::is_void_v<T>; }, mp::type_list>(
            mp::type_list<int, void>{}
          ))
        );

        expect(
          typeid(std::tuple<int>) ==
          typeid(mp::filter<[]<class T> { return not std::is_void_v<T>; }, std::tuple>(
            mp::type_list<int, void>{}
          ))
        );
      };

      "tick"_test = [](skip<"TU issue">) consteval {
        constexpr auto tag1 = []{};
        constexpr auto tag2 = []{};

        expect(0u == mp::tick());
        expect(1u == mp::tick());
        expect(0u == mp::tick<tag1>());
        expect(1u == mp::tick<tag1>());
        expect(0u == mp::tick<tag2>());
        expect(2u == mp::tick());
        expect(1u == mp::tick<tag2>());
      };
      #endif // __cpp_reflection

      "overload"_test = [] consteval {
        expect(1 == mp::overload{[] { return 1; }}());
        expect(1 == mp::overload{[] { return 1; }, [](auto i) { return i; }}());
        expect(2 == mp::overload{[] { return 1; }, [](auto i) { return i; }}(2));
        expect(2 == mp::overload{[](auto i) { return i; }}(2));
        expect(2 == mp::overload{[](auto i) { return i; }, [] { return 1; }}(2));
      };

      "function_traits"_test = [] consteval {
        expect(
          typeid(mp::type_list<>) ==
          typeid(mp::function_traits<decltype([]{})>::args_type)
        );
        expect(
          typeid(void) ==
          typeid(mp::function_traits<decltype([]{})>::result_type)
        );

        expect(
          typeid(mp::type_list<int>) ==
          typeid(mp::function_traits<decltype([](int) mutable {})>::args_type)
        );
        expect(
          typeid(void) ==
          typeid(mp::function_traits<decltype([](int) mutable {})>::result_type)
        );

        expect(
          typeid(mp::type_list<>) ==
          typeid(mp::function_traits<decltype([]() noexcept -> int { return {}; })>::args_type)
        );
        expect(
          typeid(int) ==
          typeid(mp::function_traits<decltype([]() noexcept -> int { return {}; })>::result_type)
        );

        expect(
          typeid(mp::type_list<const int&, const float*>) ==
          typeid(mp::function_traits<decltype([](const int&, const float*) noexcept {})>::args_type)
        );
        expect(
          typeid(void) ==
          typeid(mp::function_traits<decltype([](const int&, const float*) noexcept {})>::result_type)
        );

        expect(
          typeid(mp::type_list<>) ==
          typeid(mp::function_traits<void (*)()>::args_type)
        );
        expect(
          typeid(void) ==
          typeid(mp::function_traits<void (*)()>::result_type)
        );

        expect(
          typeid(mp::type_list<int>) ==
          typeid(mp::function_traits<void (*)(int)>::args_type)
        );
        expect(
          typeid(int) ==
          typeid(mp::function_traits<int (*)(int)>::result_type)
        );

        expect(
          typeid(mp::type_list<int, float, short*>) ==
          typeid(mp::function_traits<void (*)(int, float, short*)>::args_type)
        );
        expect(
          typeid(const float&) ==
          typeid(mp::function_traits<auto (*)(int, float, short*) noexcept -> const float&>::result_type)
        );
      };
    };

    "info"_suite = [] {
      "config"_test = [] consteval {
        expect(PERF_GNU >= 0);
        expect(PERF_LINUX >= 0);
        expect(PERF_LLVM >= 0);
        expect(PERF_INTEL >= 0);
      };

      "version"_test = [] consteval {
        constexpr auto version = [](const auto major, const auto minor, const auto patch) {
          return major * 100 + minor * 10 + patch;
        };
        expect(version PERF >= 1'0'0);
        expect(version PERF == version(info::version().major, info::version().minor, info::version().patch));
      };

      "compiler"_suite = [] {
        "name"_test = [] consteval {
            expect(not info::compiler::name().empty());
        };
      };

      "cpu"_suite = [] {
        "name"_test = [] mutable {
          expect(not info::cpu::name().empty());
        };

        #if PERF_GNU == 1 and defined(__x86_64__)
        "version"_test = [] mutable {
          "version"_test = [] mutable {
            auto&& version = info::cpu::version();
            expect((version.family + version.model + version.stepping) > 0);
          };
        };
        #endif // PERF_GNU and __x86_64__

        #if PERF_LLVM == 1
        "code_name"_test = [] mutable {
          expect(not info::cpu::code_name().empty());
        };

        "dispatch_width"_test = [] mutable {
          llvm llvm{};
          expect(info::cpu::dispatch_width(llvm) > 0u);
        };

        "features"_test = [] mutable {
          auto&& features = info::cpu::features();
          expect(not features.empty());
          for (const auto& feature : features) {
            expect(not feature.empty());
          }
        };
        #endif // PERF_LLVM
      };

      #if PERF_LINUX == 1
      "memory"_suite = [] {
        "icache"_test = [] mutable {
          for (const auto& [cache, data] : info::memory::icache()) {
            expect(cache == decltype(cache)::L1);
            expect(data.size > 0u);
            expect(data.line_size > 0u);
            expect(data.assoc > 0u);
          }
        };

        "dcache"_test = [] mutable {
          for (const auto& [cache, data] : info::memory::dcache()) {
            expect(
              cache == decltype(cache)::L1 or
              cache == decltype(cache)::L2 or
              cache == decltype(cache)::L3
            );
            expect(data.size > 0u);
            expect(data.line_size > 0u);
            expect(data.assoc > 0u);
          }
        };
      };
      #endif // PERF_LINUX

      "sys"_suite = [] {
        "name"_test = [] mutable {
          expect(not info::sys::name().empty());
        };

        #if PERF_LLVM == 1
        "triple"_test = [] mutable {
          expect(not info::sys::triple().empty());
        };
        #endif // PERF_LLVM
      };

      "proc"_suite = [] {
        "name"_test = [] mutable {
          expect(not info::proc::self::name().empty());
        };

        "base_address"_test = [] mutable {
          const auto value = 0;
          const auto base_address = info::proc::self::base_address();
          expect(std::uint64_t(std::addressof(value)) >= base_address);
        };
      };

      #if PERF_LLVM == 1
      "bin"_suite = [] {
        "addr_to_line"_test = [] mutable {
          expect(not info::bin::addr_to_line(info::proc::self::name(), 0ul).has_value());
        };

        "addr_to_name"_test = [] mutable {
          expect(info::bin::addr_to_name(info::proc::self::name(), 0ul).has_value());
        };

        "addr_to_fn_name"_test = [] mutable {
          expect(info::bin::addr_to_fn_name(info::proc::self::name(), 0ul).has_value());
        };
      };
      #endif // PERF_LLVM
    };

    "core"_suite = [] {
      "code"_suite = [] {
        "align"_test = [] consteval {
          expect(not []<std::size_t Alignment> { return requires { code::align<Alignment>(); }; }.template operator()<0u>());
          expect(not []<std::size_t Alignment> { return requires { code::align<Alignment>(); }; }.template operator()<5u>());
          expect(not []<std::size_t Alignment> { return requires { code::align<Alignment>(); }; }.template operator()<15u>());
          expect(not []<std::size_t Alignment> { return requires { code::align<Alignment>(); }; }.template operator()<127u>());
          expect([]<std::size_t Alignment> { return requires { code::align<Alignment>(); }; }.template operator()<16u>());
          expect([]<std::size_t Alignment> { return requires { code::align<Alignment>(); }; }.template operator()<32u>());
          expect([]<std::size_t Alignment> { return requires { code::align<Alignment>(); }; }.template operator()<64u>());
          expect([]<std::size_t Alignment> { return requires { code::align<Alignment>(); }; }.template operator()<128u>());
        };

        "align"_test = [] mutable {
          static constexpr auto begin = "align.begin";
          static constexpr auto end = "align.end";
          code::label<&begin>();
          code::align<64u>();
          code::label<&end>();
          expect(code::labels[&begin] != code::labels[&end]);
        };

        "label"_test = [] mutable {
          static constexpr auto begin = "label.begin";
          static constexpr auto end = "label.end";

          code::label<&begin>();
          code::label<&end>();
          #if __OPTIMIZE__
          expect(code::labels[&begin] == code::labels[&end]);
          #else
          expect(code::labels[&begin] != code::labels[&end]);
          #endif
        };

        "labels"_test = [] consteval {
          static constexpr auto label = "";
          expect(requires { code::labels["label"]; });
          expect(requires { code::labels[0ul]; });
          expect(requires { code::labels[&label]; });
        };
      };

      "compiler"_suite = [] {
        #if __OPTIMIZE__
        "prevent_elision/is_elided"_test = [] mutable {
          int i{}; int* ptr{};

          expect(compiler::is_elided([] { }));
          expect(compiler::is_elided([] { [[maybe_unused]] auto value = 4 + 2; }));
          expect(compiler::is_elided([] { [[maybe_unused]] int i{}; i++; }));

          expect(not compiler::is_elided([&] { i++; }));
          expect(not compiler::is_elided([] { [[maybe_unused]] static int i{}; i++; }));
          expect(not compiler::is_elided([=] {
            int i{};
            compiler::prevent_elision(i++);
          }));
          expect(not compiler::is_elided([] {
            auto value = 4 + 2;
            compiler::prevent_elision(value);
          }));

          expect(42 == compiler::prevent_elision(42));
          expect(ptr == compiler::prevent_elision(ptr));
        };
        #endif // __OPTIMIZE__
      };

      "cpu"_suite = [] {
        "pipeline"_test = [] mutable {
          cpu::pipeline::flush();
        };
      };

      "memory"_suite = [] {
        "align/is_aligned"_test = [] mutable {
          constexpr auto max_alignment = alignof(std::max_align_t);
          alignas(64u) std::byte buffer[256u]{};

          {
            const std::byte* null = nullptr;
            const auto aligned = memory::align(null, 8u);
            expect(aligned == nullptr);
            expect(memory::is_aligned(aligned, 8u));
          }

          {
            const auto* ptr = buffer + 13u;
            expect(memory::is_aligned(ptr, 1u));
            const auto aligned = memory::align(ptr, 1u);
            expect(aligned == ptr);
          }

          {
            const auto* ptr = buffer + 64u;
            expect(memory::is_aligned(ptr, 64u));
            auto aligned = memory::align(ptr, 64u);
            expect(aligned == ptr);
          }

          {
            const auto* ptr = buffer + 3u;
            expect(not memory::is_aligned(ptr, 4u));
            const auto aligned = memory::align(ptr, 4u);
            expect(memory::is_aligned(aligned, 4u));
            expect(reinterpret_cast<std::uintptr_t>(aligned) >= reinterpret_cast<std::uintptr_t>(ptr));
          }

          {
            const auto* ptr = buffer + 5u;
            const auto aligned = memory::align(ptr, 16u, 4u);
            expect(reinterpret_cast<std::uintptr_t>(aligned) % 16u == 4u);
          }

          {
            const auto* ptr = buffer + 7u;
            const auto aligned = memory::align(ptr, max_alignment);
            expect(memory::is_aligned(aligned, max_alignment));
          }

          {
            const auto* ptr = buffer + 2u;
            const auto aligned = memory::align(ptr, 8u, 12u);
            expect(reinterpret_cast<std::uintptr_t>(aligned) % 8u == 4u);
          }
        };

        "prefetch"_test = [&] mutable {
          int i{};
          auto addr = &i;

          memory::prefetch<memory::operation::read, memory::locality::none>(addr);
          memory::prefetch<memory::operation::read, memory::locality::low>(addr);
          memory::prefetch<memory::operation::read, memory::locality::moderate>(addr);
          memory::prefetch<memory::operation::read, memory::locality::high>(addr);

          memory::prefetch<memory::operation::write, memory::locality::none>(addr);
          memory::prefetch<memory::operation::write, memory::locality::low>(addr);
          memory::prefetch<memory::operation::write, memory::locality::moderate>(addr);
          memory::prefetch<memory::operation::write, memory::locality::high>(addr);
        };

        #if PERF_LINUX == 1
        "protect"_test = [] mutable {
          constexpr auto invoke = []<class TFn>(std::span<const std::uint8_t> code, auto&&... ts) {
            return reinterpret_cast<TFn*>(code.data())(std::forward<decltype(ts)>(ts)...);
          };

          {
            const std::array<const std::uint8_t, 6u> add{
              0x89, 0xf8, /// mov eax, edi
              0x01, 0xf0, /// add eax, esi
              0xc3        /// ret
            };
            expect(memory::protect(add, memory::prot::read | memory::prot::write | memory::prot::exec));
            expect(1 + 2 == invoke.template operator()<int(int, int)>(add, 1, 2));
            expect(2 + 3 == invoke.template operator()<int(int, int)>(add, 2, 3));
          }

          {
            const std::array<const std::uint8_t, 6u> mult{
              0x89, 0xf8,       /// mov eax, edi
              0x0f, 0xaf, 0xc6, /// imul eax, esi
              0xc3              /// ret
            };
            expect(memory::protect(mult, memory::prot::read | memory::prot::write | memory::prot::exec));
            expect(1 * 2 == invoke.template operator()<int(int, int)>(mult, 1, 2));
            expect(2 * 3 == invoke.template operator()<int(int, int)>(mult, 2, 3));
          }
        };
        #endif // PERF_LINUX

        "cache"_suite = [] {
          "flush"_test = [] mutable {
            int i{};
            //todo
            //memory::cache::flush(std::span<const std::uint8_t>{&i, sizeof(i)});
          };
        };

        "heap"_suite = [] {
          "pollute"_test = [] mutable {
            memory::heap::pollute(1024u);
          };
        };
      };

      "sys"_suite = [] {
        #if PERF_LINUX == 1
        "thread"_suite = [] {
          "thread"_test = [] mutable {
            expect(pthread_self() == thread::self);
            expect(pthread_self() == sys::thread::self);
          };

          "affinity"_test = [] mutable {
            {
              const auto before = thread::affinity::get(thread::self);
              expect(thread::affinity::set(thread::self, before));
              expect(before == thread::affinity::get(thread::self));
            }

            if (std::thread::hardware_concurrency() >= 2u) {
              const auto affinity = thread::affinity::get(thread::self);
              expect(thread::affinity::set(thread::self, std::array{1ul, 2ul}));
              expect(std::vector{1ul, 2ul} == thread::affinity::get(thread::self));
              expect(thread::affinity::set(thread::self, affinity));
            }
          };

          "priority"_test = [](skip<"not implemented">) mutable {
          };
        };
        #endif // PERF_LINUX
      };
    };

    #if PERF_LLVM == 1
    "backend"_suite = [] {
      "arch_like"_test = [] consteval {
        expect(not arch_like<void>);

        struct empty {};
        expect(not arch_like<empty>);

        struct arch_missing {
          std::string triple{};
          std::vector<std::string> features{};
          enum perf::arch::syntax syntax{};
        };
        expect(not arch_like<arch_missing>);

        struct arch {
          std::string triple{};
          std::string cpu{};
          std::vector<std::string> features{};
          enum perf::arch::syntax syntax{};
        };
        expect(arch_like<arch>);

        expect(arch_like<backend::arch>);
      };

      "llvm"_test = [] mutable {
        {
          llvm llvm{};
          expect(llvm.arch.triple == ::llvm::Triple::normalize(::llvm::sys::getDefaultTargetTriple()));
          expect(llvm.arch.cpu == ::llvm::sys::getHostCPUName().str());
          expect(bool(llvm.register_info.get()));
          expect(bool(llvm.assembler_info.get()));
          expect(bool(llvm.instruction_info.get()));
          expect(bool(llvm.subtarget_info.get()));
          expect(bool(llvm.instrument_manager.get()));
          expect(bool(llvm.instruction_analysis.get()));
          expect(bool(llvm.instruction_builder.get()));
          expect(bool(llvm.instruction_printer.get()));
          expect(bool(llvm.context.get()));
          expect(bool(llvm.disassembler.get()));
        }

        {
          llvm llvm{arch{}};
          expect(llvm.arch.triple == ::llvm::Triple::normalize(::llvm::sys::getDefaultTargetTriple()));
          expect(llvm.arch.cpu == ::llvm::sys::getHostCPUName().str());
          expect(bool(llvm.register_info.get()));
          expect(bool(llvm.assembler_info.get()));
          expect(bool(llvm.instruction_info.get()));
          expect(bool(llvm.subtarget_info.get()));
          expect(bool(llvm.instrument_manager.get()));
          expect(bool(llvm.instruction_analysis.get()));
          expect(bool(llvm.instruction_builder.get()));
          expect(bool(llvm.instruction_printer.get()));
          expect(bool(llvm.context.get()));
          expect(bool(llvm.disassembler.get()));
        }
      };

      "mc"_suite = [] {
        "instruction_like"_test = [] consteval {
          expect(not mc::instruction_like<void>);

          struct empty{};
          expect(not mc::instruction_like<empty>);

          struct instruction_no_opcode {
            std::uint64_t ip{};
            std::uint64_t size{};
          };
          expect(not mc::instruction_like<instruction_no_opcode>);

          struct instruction_no_ip {
            std::uint64_t opcode{};
            std::uint64_t size{};
          };
          expect(not mc::instruction_like<instruction_no_ip>);

          expect(mc::instruction_like<mc::instruction>);
        };

        "disassemble"_test = [] mutable {
          llvm llvm{};

          {
            static constexpr auto begin = "label.begin";
            static constexpr auto end = "label.end";

            int a{}, b{};
            code::label<&begin>();
            compiler::prevent_elision(a + b);
            code::label<&end>();
            const auto& instructions = mc::disassemble(
              code::labels[&begin],
              code::labels[&end],
              llvm
            );

            #if __OPTIMIZE__
            expect(1u == instructions.size());
            #else
            expect(instructions.size() >= 1u);
            #endif // __OPTIMIZE__
          }

          auto add = [](int a, int b) {
            return a + b;
          };

          {
            const auto& instructions = mc::disassemble(
              [&] { return add(1, 2); },
              llvm
            );

            #if __OPTIMIZE__
            expect(1u == instructions.size());
            #else
            expect(instructions.size() >= 1u);
            #endif // __OPTIMIZE__
          }

          #if PERF_INTEL == 1
          {
            #if __OPTIMIZE__
            trace::tracer tracer{};
            const auto& traces = trace::trace(
              [&] { return add(1, 2); },
              tracer
            );

            expect(1u == traces.size());
            for (const auto& trace : traces) {
              const auto* ip = reinterpret_cast<const std::uint8_t*>(trace.ip);
              auto&& dissasm = mc::disassemble(ip, ip + trace.size, llvm);
              expect(1u == dissasm.size());
            }
            #endif // __OPTIMIZE__
          }

          // todo trace vs disassmeelb with branch

          // todo dissamble traces
   /*       {*/
            //int a{}, b{};
            //code::label("disassemble.trace.begin");
            //compiler::prevent_elision(a + b);
            //code::label("disassemble.trace.end");


            //const auto& instructions = mc::disassemble(
              //code::labels["disassemble.trace.begin"],
              //code::labels["disassemble.trace.end"],
              //llvm
            //);
          /*}*/

          #endif // PERF_INTEL
        };

        #if defined(__x86_64__)
        "info"_suite = [] {
          using std::literals::string_view_literals::operator""sv;
          llvm llvm{};

          auto&& mov = [&] {
            const auto& instructions = mc::disassemble(
              [] { return 1; /*0: xor, N: mov*/ },
              llvm
            );
            expect(1u == instructions.size());
            return instructions[0];
          }();

          "assembly"_test = [&] mutable {
            expect("mc.assembly"sv == mc::assembly.name());
            const auto& [str, syntax] = mc::assembly(mov, llvm);
            expect(str.contains("mov"));
            expect(syntax == llvm.arch.syntax);
            expect(std::format("{}", mc::assembly(mov, llvm)).contains("mov"));
          };

          "address"_test = [&] mutable {
            expect("mc.address"sv == mc::address.name());
            const auto& [ip, size] = mc::address(mov, llvm);
            expect(ip == mov.ip);
            expect(size == mov.size);
            expect(not std::format("{}", mc::address(mov, llvm)).empty());
          };

          "encoding"_test = [&] mutable {
            expect("mc.encoding"sv == mc::encoding.name());
            const auto& [ip, size] = mc::encoding(mov, llvm);
            expect(ip == mov.ip);
            expect(size == mov.size);
            expect(not std::format("{}", mc::encoding(mov, llvm)).empty());
          };

          "size"_test = [&] mutable {
            expect("mc.size"sv == mc::size.name());
            const auto& [size] = mc::size(mov, llvm);
            expect(size == mov.size);
            expect(not std::format("{}", mc::size(mov, llvm)).empty());
          };

          "uops"_test = [&] mutable {
            expect("mc.uops"sv == mc::uops.name());
            const auto& [uops] = mc::uops(mov, llvm);
            expect(uops >= 1u);
            expect(not std::format("{}", mc::uops(mov, llvm)).empty());
          };

          "latency"_test = [&] mutable {
            expect("mc.latency"sv == mc::latency.name());
            const auto& [latency] = mc::latency(mov, llvm);
            expect(latency >= 1u);
            expect(not std::format("{}", mc::latency(mov, llvm)).empty());
          };

          "rthroughput"_test = [&] mutable {
            expect("mc.rthroughput"sv == mc::rthroughput.name());
            const auto& [rthroughput] = mc::rthroughput(mov, llvm);
            expect(rthroughput > 0.);
            expect(not std::format("{}", mc::rthroughput(mov, llvm)).empty());
          };

          "may_load"_test = [&] mutable {
            expect("mc.may_load"sv == mc::may_load.name());
            const auto& [may_load] = mc::may_load(mov, llvm);
            expect(not may_load); /// reg->reg
            expect("" == std::format("{}", mc::may_load(mov, llvm)));
          };

          "may_store"_test = [&] mutable {
            expect("mc.may_store"sv == mc::may_store.name());
            const auto& [may_store] = mc::may_store(mov, llvm);
            expect(not may_store); /// reg->reg
            expect("" == std::format("{}", mc::may_load(mov, llvm)));
          };

          "has_side_effects"_test = [&] mutable {
            expect("mc.has_side_effects"sv == mc::has_side_effects.name());
            const auto& [has_side_effects] = mc::has_side_effects(mov, llvm);
            expect(not has_side_effects);
            expect("" == std::format("{}", mc::has_side_effects(mov, llvm)));
          };

          "branch"_suite = [&] {
            "is_conditional"_test = [&] mutable {
              expect("mc.branch.is_conditional"sv == mc::branch::is_conditional.name());
              const auto& [is_conditional] = mc::branch::is_conditional(mov, llvm);
              expect(not is_conditional);
              expect("" == std::format("{}", mc::branch::is_conditional(mov, llvm)));
            };

            "is_unconditional"_test = [&] mutable {
              expect("mc.branch.is_unconditional"sv == mc::branch::is_unconditional.name());
              const auto& [is_unconditional] = mc::branch::is_unconditional(mov, llvm);
              expect(not is_unconditional);
              expect("" == std::format("{}", mc::branch::is_unconditional(mov, llvm)));
            };

            "is_indirect"_test = [&] mutable {
              expect("mc.branch.is_indirect"sv == mc::branch::is_indirect.name());
              const auto& [is_indirect] = mc::branch::is_indirect(mov, llvm);
              expect(not is_indirect);
              expect("" == std::format("{}", mc::branch::is_indirect(mov, llvm)));
            };

            "is_call"_test = [&] mutable {
              expect("mc.branch.is_call"sv == mc::branch::is_call.name());
              const auto& [is_call] = mc::branch::is_call(mov, llvm);
              expect(not is_call);
              expect("" == std::format("{}", mc::branch::is_call(mov, llvm)));
            };

            "is_ret"_test = [&] mutable {
              expect("mc.branch.is_ret"sv == mc::branch::is_ret.name());
              const auto& [is_ret] = mc::branch::is_ret(mov, llvm);
              expect(not is_ret);
              expect("" == std::format("{}", mc::branch::is_ret(mov, llvm)));
            };
          };

          #if PERF_LINUX == 1
          "source"_test = [] mutable {
          };
          #endif // PERF_LINUX
        };
        #endif // _x86_64__
      };

      //todo struct with iterations
      //todo verify output
      "mca"_suite = [] {
        "timeline"_test = [] mutable {
          using std::literals::string_view_literals::operator""sv;
          llvm llvm{};
          expect("mca.timeline"sv == mca::timeline.name());
          auto&& instructions = mc::disassemble([&] { return 1 + 2; }, llvm);
          auto&& data = mca::timeline(instructions, llvm);
          expect(1u == data.size());
        };

        "resource_pressure"_test = [] mutable {
          using std::literals::string_view_literals::operator""sv;
          llvm llvm{};
          expect("mca.resource_pressure"sv == mca::resource_pressure.name());
          auto&& instructions = mc::disassemble([&] { return 1 + 2; }, llvm);
          auto&& data = mca::resource_pressure(instructions, llvm);
          expect(1u == data.size());
        };

        "bottleneck"_test = [] mutable {
          using std::literals::string_view_literals::operator""sv;
          llvm llvm{};
          expect("mca.bottleneck"sv == mca::bottleneck.name());
          auto&& instructions = mc::disassemble([&] { return 1 + 2; }, llvm);
          auto&& data = mca::bottleneck(instructions, llvm);
          expect(1u == data.size());
        };
      };
    };
    #endif // PERF_LLVM

    "prof"_suite = [] {
      "dsl"_suite = [] {
        "concat"_test = [] consteval {
          expect(fixed_string{""} == dsl::detail::concat());
          expect(fixed_string{"perf"} == dsl::detail::concat(fixed_string{"perf"}));
          expect(
            fixed_string{"x86-64"} ==
            dsl::detail::concat(
              fixed_string{"x86"},
              fixed_string{"-"},
              fixed_string{"64"}
            )
          );
        };

        "op"_test = [] consteval {
          using dsl::operator+;
          using dsl::operator-;
          using dsl::operator*;
          using dsl::operator/;
        };
      };

      "profiler_like"_test = [] consteval {
        expect(not prof::profiler_like<void>);
        struct empty{};
        expect(not prof::profiler_like<empty>);

        struct missing_start {
          void stop();
        };

        expect(not prof::profiler_like<missing_start>);
        struct missing_stop {
          void start();
        };
        expect(not prof::profiler_like<missing_stop>);

        struct none {
          void start();
          void stop();
        };
        expect(prof::profiler_like<none>);
      };

      "time"_suite = [] {
        "time_like"_test = [] consteval {
          expect(not time::time_like<void>);

          struct empty{};
          expect(not time::time_like<empty>);

          struct t_constexpr {
            constexpr void start();
            constexpr void stop();
            constexpr auto operator*() const -> std::chrono::duration<double, std::nano>;
          };
          expect(time::time_like<t_constexpr>);

          struct t_no_start {
            void stop();
            auto operator*() const -> std::chrono::duration<double, std::nano>;
          };
          expect(not time::time_like<t_no_start>);

          struct t_no_start_no_stop {
            auto operator*() const -> std::chrono::duration<double, std::nano>;
          };
          expect(not time::time_like<t_no_start_no_stop>);

          struct t_wrong_ret {
            void start();
            void stop();
            auto operator*() const -> double;
          };
          expect(not time::time_like<t_wrong_ret>);

          struct t {
            void start();
            void stop();
            auto operator*() const -> std::chrono::duration<double, std::nano>;
          };
          expect(time::time_like<t>);
        };

        "time"_suite = [] {
          constexpr auto test = [](auto t) {
            t.start();
            t.stop();
            expect((*t).count() > .0);
          };

          "steady_clock"_test = [test] mutable {
            test(time::steady_clock);
          };

          #if PERF_GNU == 1 and defined(__x86_64__)
          "tsc"_test = [test] mutable {
            expect(time::tsc::freq() > .0);
            test(time::tsc);
          };
          #endif

          #if PERF_LINUX == 1
          "cpu"_test = [test] mutable {
            test(time::cpu);
          };

          "thread"_test = [test] mutable {
            test(time::thread);
          };

          "real"_test = [test] mutable {
            test(time::real);
          };

          "monotonic"_test = [test] mutable {
            test(time::monotonic);
          };
          #endif
        };

        "timer"_test = [] mutable {
          using result_type = std::unordered_map<
            std::string_view,
            std::chrono::duration<double, std::nano>
          >;

          {
            expect([](const auto&... ts) { return requires { time::timer{ts...}; }; }(fake::time<"", 0.>{}));
            expect([](const auto&... ts) { return requires { time::timer{ts...}; }; }(time::steady_clock));
            expect([](const auto&... ts) { return requires { time::timer{ts...}; }; }(fake::time<"", 0.>{}, time::steady_clock));
            expect(not [](const auto&... ts) { return requires { time::timer{ts...}; }; }(0u));
            expect(not [](const auto&... ts) { return requires { time::timer{ts...}; }; }(stat::cycles));
            expect(not [](const auto&... ts) { return requires { time::timer{ts...}; }; }(record::cycles));
            expect(not [](const auto&... ts) { return requires { time::timer{ts...}; }; }(stat::cycles, record::cycles));
          }

          {
            auto&& t = time::timer{};
            t.start();
            t.stop();
            expect(result_type{} == *t);
          }

          using fake_time_1 = fake::time<"fake_time_1", 1.>;
          using fake_time_2 = fake::time<"fake_time_2", 2.>;

          {
            auto&& t = time::timer{fake_time_1{}};
            expect(0u == fake_time_1::start_calls);
            expect(0u == fake_time_1::stop_calls);

            t.start();
            t.stop();

            expect(1u == fake_time_1::start_calls);
            expect(1u == fake_time_1::stop_calls);

            auto&& data = *t;
            expect(1u == data.size());
            expect(data.contains(fake_time_1::name()));
            expect(*fake_time_1{} == data[fake_time_1::name()]);
          }

          {
            auto&& t = time::timer{
              fake_time_1{},
              fake_time_2{},
            };
            expect(0u == fake_time_1::start_calls);
            expect(0u == fake_time_1::stop_calls);
            expect(0u == fake_time_2::start_calls);
            expect(0u == fake_time_2::stop_calls);

            t.start();
            t.stop();

            expect(1u == fake_time_1::start_calls);
            expect(1u == fake_time_1::stop_calls);
            expect(1u == fake_time_2::start_calls);
            expect(1u == fake_time_2::stop_calls);

            auto&& data = *t;
            expect(2u == data.size());
            expect(data.contains(fake_time_1::name()));
            expect(data.contains(fake_time_2::name()));
            expect(*fake_time_1{} == data[fake_time_1::name()]);
            expect(*fake_time_2{} == data[fake_time_2::name()]);
          }
        };
      };

      #if PERF_LINUX == 1
      "stat"_suite = [] {
        "event_like"_test = [] consteval {
          struct empty { };
          expect(not stat::event_like<empty>);

          struct event_missing {
            std::string_view name{};
            std::uint32_t type{};
            std::array<std::uint64_t, 3u> config{};
            std::size_t kind{};
          };
          expect(not stat::event_like<event_missing>);

          struct event {
            std::string_view name{};
            std::uint32_t type{};
            std::array<std::uint64_t, 3u> config{};
            std::size_t priority{};
          };
          expect(stat::event_like<event>);

          expect(stat::event_like<stat::event>);
        };

        "counter"_test = [] mutable {
          {
            expect([](const auto&... ts) { return requires { stat::counter{ts...}; }; }(stat::cycles));
            expect(not [](const auto&... ts) { return requires { stat::counter{ts...}; }; }(0u));
            expect(not [](const auto&... ts) { return requires { stat::counter{ts...}; }; }(record::cycles));
            expect(not [](const auto&... ts) { return requires { stat::counter{ts...}; }; }(time::steady_clock));
            expect(not [](const auto&... ts) { return requires { stat::counter{ts...}; }; }(stat::cycles, record::cycles));
            expect(not [](const auto&... ts) { return requires { stat::counter{ts...}; }; }(stat::cycles, record::cycles));
          }

          {
            stat::counter counter{};
            counter.start();
            counter.stop();
            auto&& data = *counter;
            expect(data.empty());
          }

          {
            stat::counter counter{stat::config{}};
            counter.start();
            counter.stop();
            auto&& data = *counter;
            expect(data.empty());
          }

          {
            stat::counter counter{stat::config{}, stat::cycles};
            counter.start();
            counter.stop();
            auto&& data = *counter;
            expect(1u == data.size());
            expect(data.contains(stat::cycles.name()));
            expect(data[stat::cycles.name()] > 0.);
          }

          {
            stat::counter counter{stat::instructions, stat::cycles};
            counter.start();
            counter.stop();
            auto&& data = *counter;
            expect(2u == data.size());
            expect(data.contains(stat::instructions.name()));
            expect(data.contains(stat::cycles.name()));
            expect(data[stat::instructions.name()] > 0.);
            expect(data[stat::cycles.name()] > 0.);
          }
        };
      };
      #endif // PERF_LINUX

      #if PERF_LINUX == 1
      "record"_suite = [] {
        "event_like"_test = [] consteval {
          struct event {
            std::string_view name{};
            std::uint32_t type{};
            std::array<std::uint64_t, 3u> config{};
            std::size_t kind{};
          };
          expect(record::event_like<event>);

          struct empty { };
          expect(not record::event_like<empty>);

          struct event_missing {
            std::string_view name{};
            std::uint32_t type{};
            std::array<std::uint64_t, 3u> config{};
            std::size_t priority{};
          };
          expect(not record::event_like<event_missing>);

          expect(record::event_like<record::event>);
        };

        "sampler"_test = [] mutable {
          {
            expect([](const auto&... ts) { return requires { record::sampler{ts...}; }; }(record::cycles));
            expect(not [](const auto&... ts) { return requires { record::sampler{ts...}; }; }(0u));
            expect(not [](const auto&... ts) { return requires { record::sampler{ts...}; }; }(stat::cycles));
            expect(not [](const auto&... ts) { return requires { record::sampler{ts...}; }; }(time::steady_clock));
            expect(not [](const auto&... ts) { return requires { record::sampler{ts...}; }; }(record::cycles, stat::cycles));
          }

          {
            record::sampler sampler{};
            sampler.start();
            sampler.stop();
            auto&& data = *sampler;
            expect(data.empty());
          }

          constexpr auto event = mp::overload{
            [](const std::unordered_map<std::uint64_t, std::vector<double>>& data) {
              expect(data.size() > 0u);
              for (const auto& [ip, values] : data) {
                expect(ip > 0u);
                expect(values.size() > 0u);
                for (const auto& v : values) {
                  expect(v > 0.);
                }
              }
            },
            [](const auto& t) {
              verify(false, typeid(t).name());
            },
          };

          record::config config {
            .interval = record::config::period{1} /// every event
          };

          {
            record::sampler sampler{config, record::cycles};
            sampler.start();
            sampler.stop();
            auto&& data = *sampler;
            expect(1u == data.size());
            expect(data.contains(record::cycles.name()));
            std::visit(event, data[record::cycles.name()]);
          }

          {
            record::sampler sampler{config, record::instructions, record::cycles};
            sampler.start();
            sampler.stop();
            auto&& data = *sampler;
            expect(2u == data.size());
            expect(data.contains(record::instructions.name()));
            expect(data.contains(record::cycles.name()));
            std::visit(event, data[record::instructions.name()]);
            std::visit(event, data[record::cycles.name()]);
          }
        };
      };
      #endif // PERF_LINUX

      "trace"_suite = [] {
        #if PERF_LINUX == 1 and PERF_INTEL == 1
        "trace_like"_test = [] consteval {
          expect(not trace::trace_like<void>);

          struct empty {};
          expect(not trace::trace_like<empty>);

          expect(trace::trace_like<trace::event<[](auto...){}>>);
        };

        "tracer"_test = [] mutable {
          {
            trace::tracer tracer{};
            tracer.start();
            tracer.stop();
            expect((*tracer).size() > 0u); /// instructions from start/stop
          }

          // multiple traces
        };

        #if __OPTIMIZE__
        "trace"_test = [] mutable {
          trace::tracer tracer{};

          {
            const auto& traces = trace::trace([] { }, tracer);
            expect(0u == traces.size());
          }

          auto add = [](int a, int b) {
            return a + b;
          };

          {
            const auto& traces = trace::trace(
              [&] { /*return */ add(1, 2);},
              tracer
            );
            expect(0u == traces.size());
          }

          {
            const auto& traces = trace::trace(
              [&] { compiler::prevent_elision(add(1, 2)); },
              tracer
            );
            expect(1u == traces.size());
          }

          {
            const auto& traces = trace::trace(
              [&] { return add(1, 2); },
              tracer
            );
            expect(1u == traces.size());
          }

          {
            const auto& traces = trace::trace(
              [&] { /* optimized */ return add(1, 2) + add(3, 4); },
              tracer
            );
            expect(1u == traces.size());
          }

          //// todo if else branch
          //{
            //bool value{};

            //const auto& traces = trace::trace(
              //[&] { if (value) return add(1, 2); },
              //tracer
            //);
            //expect(1u == traces.size());
          //}
        };
        #endif // __OPTIMIZE__
        #endif // PERF_LINUX and PERF_INTEL
      };

      "tool"_suite = [] consteval {
        #if __has_include(<valgrind/callgrind.h>)
        expect(prof::profiler_like<prof::tool::callgrind>);
        #endif

        #if __has_include(<xray/xray_interface.h>) and \
            __has_include(<xray/xray_log_interface.h>)
        expect(prof::profiler_like<prof::tool::xray>);
        #endif

        #if __has_include(<fcntl.h>) and __has_include(<unistd.h>) and __has_include(<cassert>)
        expect(prof::profiler_like<prof::tool::perf>);
        #endif

        #if __has_include(<gperftools/profiler.h>)
        expect(prof::profiler_like<prof::tool::gperf>);
        #endif

        #if __has_include(<ittnotify.h>)
        expect(prof::profiler_like<prof::tool::vtune>);
        #endif
      };
    };

    "bench"_suite = [] {
      "data"_suite = [] {
        "repeat"_test = [] mutable {
          expect(std::vector{0} == bench::data::repeat<int>({0})(1u));
          expect(std::vector{0, 0} == bench::data::repeat<int>{.values = {0}}(2u));
          expect(std::vector{0} == bench::data::repeat<int>({0, 1})(1u));
          expect(std::vector{0, 1} == bench::data::repeat<int>({0, 1})(2u));
          expect(std::vector{0, 1, 0} == bench::data::repeat<int>({0, 1})(3u));
          expect(std::vector{0, 1, 0, 1} == bench::data::repeat<int>{.values = {0, 1}}(4u));
          expect(std::vector{2, 1} == bench::data::repeat<int>({2, 1, 3})(2u));
          expect(std::vector{2, 1, 3, 2, 1} == bench::data::repeat<int>({2, 1, 3})(5u));
          expect(std::vector{2ul, 1ul, 3ul, 2ul, 1ul} == bench::data::repeat<std::size_t>{.values = {2, 1, 3}}(5u));
        };

        "range"_test = [] mutable {
          expect(std::vector<int>{} == bench::data::range<int>{.start = 0, .stop = 2, .step = 1}(0u));
          expect(std::vector{0} == bench::data::range<int>{.start = 0, .stop = 2, .step = 1}(1u));
          expect(std::vector{1, 11} == bench::data::range<int>{.start = 1, .stop = 100, .step = 10}(2u));
          expect(std::vector{1, 3, 5} == bench::data::range<int>{.start = 1, .stop = 6, .step = 2}(3u));

          expect(std::vector{0} == bench::data::range<int>{.start = 0, .stop = 3, .step = 1}(1u));
          expect(std::vector{0, 1} == bench::data::range<int>{.start = 0, .stop = 3, .step = 1}(2u));
          expect(std::vector{0, 1, 2} == bench::data::range<int>{.start = 0, .stop = 3, .step = 1}(3u));
          expect(std::vector{0, 1, 2, 0} == bench::data::range<int>{.start = 0, .stop = 3, .step = 1}(4u));
          expect(std::vector{0, 1, 2, 0, 1} == bench::data::range<int>{.start = 0, .stop = 3, .step = 1}(5u));
          expect(std::vector{0, 1, 2, 0, 1, 2} == bench::data::range<int>{.start = 0, .stop = 3, .step = 1}(6u));
        };

        "choice"_test = [] mutable {
          expect(std::vector<int>{} == bench::data::choice<int>{.values = {1}, .probabilities = {1.}}(0u));
          expect(std::vector<int>{1} == bench::data::choice<int>{.values = {1}, .probabilities = {1.}}(1u));
          expect(std::vector<int>{2} == bench::data::choice<int>{.values = {1, 2}, .probabilities = {0., 1.}}(1u));
          expect(std::vector<int>{2, 2} == bench::data::choice<int>{.values = {1, 2}, .probabilities = {0., 1.}}(2u));
          expect(std::vector<int>{1, 1} == bench::data::choice<int>{.values = {1, 2}, .probabilities = {1., 0.}}(2u));
        };

        "uniform"_test = [] mutable {
          expect(std::vector<int>{} == bench::data::uniform<int>{.min = 0, .max = 0, .seed = 0}(0u));
          expect(std::vector<int>{0} == bench::data::uniform<int>{.min = 0, .max = 0, .seed = 0}(1u));
          expect(std::vector<int>{0, 0} == bench::data::uniform<int>{.min = 0, .max = 0, .seed = 0}(2u));
          expect(std::vector<int>{1} == bench::data::uniform<
            int, int, fake::random_suite<{1,2,3}>, fake::distribution<int>
          >{.min = 0, .max = 10, .seed = 0}(1u));
          expect(std::vector<int>{1, 2} == bench::data::uniform<
            int, int, fake::random_suite<{1,2,3}>, fake::distribution<int>
          >{.min = 0, .max = 10, .seed = 0}(2u));
          expect(std::vector<int>{1, 2, 3} == bench::data::uniform<
            int, int, fake::random_suite<{1,2,3}>, fake::distribution<int>
          >{.min = 0, .max = 10, .seed = 0}(3u));
        };

        "normal"_test = [] mutable {
          expect(std::vector<float>{} == bench::data::normal<float>{.mean = 0., .std_dev = 0., .seed = 0}(0u));
          expect(std::vector<float>{0.} == bench::data::normal<float>{.mean = 0., .std_dev = 0., .seed = 0}(1u));
          expect(std::vector<float>{1.} == bench::data::normal<
            float, fake::random_suite<{1., 2., 3.}>, fake::distribution<float>
          >{.mean = 1., .std_dev = 1., .seed = 0}(1u));
          expect(std::vector<float>{1., 2.} == bench::data::normal<
            float, fake::random_suite<{1., 2., 3.}>, fake::distribution<float>
          >{.mean = 1., .std_dev = 1., .seed = 0}(2u));
          expect(std::vector<float>{1., 2., 3.} == bench::data::normal<
            float, fake::random_suite<{1., 2., 3.}>, fake::distribution<float>
          >{.mean = 1., .std_dev = 1., .seed = 0}(3u));
        };

        "take"_test = [] mutable {
          expect(std::vector<std::vector<int>>{} == bench::data::take(0u)(bench::data::repeat<int>{{1}})(0u));
          expect(std::vector<std::vector<int>>{std::vector<int>{}} == bench::data::take(0u)(bench::data::repeat<int>{{1}})(1u));
          expect(std::vector<std::vector<int>>{std::vector{1}} == bench::data::take(1u)(bench::data::repeat<int>{{1}})(1u));

          using bench::data::operator|;
          expect(std::vector<std::vector<int>>{} == (bench::data::repeat<int>{{1}} | bench::data::take(0u))(0u));
          expect(std::vector<std::vector<int>>{std::vector<int>{}} == (bench::data::repeat<int>{{1}} | bench::data::take(0u))(1u));
          expect(std::vector<std::vector<int>>{std::vector{1}} == (bench::data::repeat<int>{{1}} | bench::data::take(1u))(1u));
          expect(
            std::vector<std::vector<int>>{std::vector{1,2,3,1,2}, std::vector{1,2,3,1,2}} ==
            (bench::data::repeat<int>{{1,2,3}} | bench::data::take(5u))(2u)
          );
        };
      };

      "utility"_suite = [] {
        "debug_like"_test = [] consteval {
          expect(not bench::utility::debug_like<decltype([]{})>);
          expect(not bench::utility::debug_like<decltype([](int){})>);
          expect(not bench::utility::debug_like<decltype([](int, float){})>);

          expect(bench::utility::debug_like<decltype([]<bool = {}>{})>);
          expect(bench::utility::debug_like<decltype([]<bool = {}>(int){})>);
          expect(bench::utility::debug_like<decltype([]<bool = {}>(int, float){})>);
          expect(bench::utility::debug_like<decltype([]<auto = true>(int, float){})>);
          expect(bench::utility::debug_like<decltype([]<auto = true, auto...>(int, float){})>);
        };

        "debug"_test = [] consteval {
          expect(bench::utility::debug([]<bool Debug = {}> { return Debug; })());
          expect(bench::utility::debug([]<bool Debug = {}>(int) { return Debug; })(int{}));
          expect(bench::utility::debug([]<bool Debug = {}>(int, float) { return Debug; })(int{}, float{}));
        };

        "composite"_test = [] mutable {
          {
            auto&& t = std::tuple{};
            auto&& c = bench::utility::composite{t};
            c.start();
            c.stop();
            expect(std::tuple{} == *c);
          }

          using fake_time_1 = fake::time<"fake_time_1", 1.>;
          using fake_time_2 = fake::time<"fake_time_2", 2.>;

          {
            auto&& t = std::tuple{fake_time_1{}};
            auto&& c = bench::utility::composite{t};
            expect(0u == fake_time_1::start_calls);
            expect(0u == fake_time_1::stop_calls);

            c.start();
            c.stop();

            expect(1u == fake_time_1::start_calls);
            expect(1u == fake_time_1::stop_calls);

            expect(std::tuple{*fake_time_1{}} == *c);
          }

          {
            auto&& t = std::tuple{fake_time_1{}, fake_time_2{}};
            auto&& c = bench::utility::composite{t};
            expect(0u == fake_time_1::start_calls);
            expect(0u == fake_time_1::stop_calls);
            expect(0u == fake_time_2::start_calls);
            expect(0u == fake_time_2::stop_calls);

            c.start();
            c.stop();

            expect(1u == fake_time_1::start_calls);
            expect(1u == fake_time_1::stop_calls);
            expect(1u == fake_time_2::start_calls);
            expect(1u == fake_time_2::stop_calls);

            expect(std::tuple{*fake_time_1{}, *fake_time_2{}} == *c);
          }
        };

        "dataset"_test = [] constexpr {
        };
      };

      "latency"_test = [] mutable {
      };

      "throughput"_test = [] mutable {
      };

      "config"_suite = [] {
        using std::literals::string_view_literals::operator""sv;

        "name"_test = [] mutable {
          {
            bench::config config{}; // "{name}({args})";
            expect("args_0()"sv == config.name(fake::args_0));
            expect("args_1(0)"sv == config.name(fake::args_1, 0));
            expect("args_1(1)"sv == config.name(fake::args_1, 1));
          }

          {
            bench::config config{.name = "{name}/{args}/"};
            expect("args_0//"sv == config.name(fake::args_0));
            expect("args_1/0/"sv == config.name(fake::args_1, 0));
            expect("args_1/1/"sv == config.name(fake::args_1, 1));
          }

          {
            bench::config config{.name = "{args}"};
            expect(""sv == config.name(fake::args_0));
            expect("0"sv == config.name(fake::args_1, 0));
            expect("1"sv == config.name(fake::args_1, 1));
          }

          {
            bench::config config{.name = "perf"};
            expect("perf"sv == config.name(fake::args_0));
            expect("perf"sv == config.name(fake::args_1, 0));
            expect("perf"sv == config.name(fake::args_1, 1));
          }
        };

        "engine"_test = [] mutable {
        };
      };

      "runner"_test = [] mutable {
      };

      "stat"_suite = [] {
        "min"_test = [] constexpr {
          using std::literals::string_view_literals::operator""sv;
          expect("min"sv == bench::stat::min.name());
          expect(0 == bench::stat::min(std::array<int, 0>{}));
          expect(0 == bench::stat::min(std::array{0}));
          expect(1 == bench::stat::min(std::array{1, 1}));
          expect(1 == bench::stat::min(std::array{1, 2, 3}));
          expect(-3 == bench::stat::min(std::array{-3, 2, 3}));
        };

        "math"_test = [] constexpr {
          using std::literals::string_view_literals::operator""sv;
          expect("max"sv == bench::stat::max.name());
          expect(0 == bench::stat::max(std::array{0}));
          expect(1 == bench::stat::max(std::array{1, 1}));
          expect(3 == bench::stat::max(std::array{1, 2, 3}));
          expect(3 == bench::stat::max(std::array{3, 2, -3}));
        };

        "median"_test = [] constexpr {
          using std::literals::string_view_literals::operator""sv;
          expect("median"sv == bench::stat::median.name());
          expect(0 == bench::stat::median(std::vector<int>{}));
          expect(2 == bench::stat::median(std::array{1, 2, 3}));
          expect(2 == bench::stat::median(std::array{1, 4, 3, 2}));
          expect(3 == bench::stat::median(std::array{2, 3, 1, 4, 5}));
        };

        "mean"_test = [] constexpr {
          using std::literals::string_view_literals::operator""sv;
          expect("mean"sv == bench::stat::mean.name());
          expect(0 == bench::stat::mean(std::vector<int>{}));
          expect(2 == bench::stat::mean(std::array{1, 2, 3}));
          expect(0 == bench::stat::mean(std::array{-1, 0, 1}));
        };

        "geometric_mean"_test = [] mutable {
          using std::literals::string_view_literals::operator""sv;
          expect("geometric_mean"sv == bench::stat::geometric_mean.name());
          expect(0 == bench::stat::geometric_mean(std::vector<int>{}));
          expect(2 == bench::stat::geometric_mean(std::array{1, 2, 4}));
        };

        "percentile"_test = [] mutable {
          using std::literals::string_view_literals::operator""sv;
          {
            const auto p50 = bench::stat::percentile(50);
            expect("percentile{50}"sv == p50.name());
            expect(9 == p50(std::array{1, 3, 5, 7, 9, 11, 13, 15, 17, 19}));
            expect(1 == p50(std::array{1,1,1}));
          }

          {
            const auto p90 = bench::stat::percentile(90);
            expect("percentile{90}"sv == p90.name());
            expect(17 == p90(std::array{1, 3, 5, 7, 9, 11, 13, 15, 17, 19}));
            expect(3 == p90(std::array{3, 3, 3, 3, 3}));
          }

          {
            const auto p99 = bench::stat::percentile(99);
            expect("percentile{99}"sv == p99.name());
            expect(17 == p99(std::array{1, 3, 5, 7, 9, 11, 13, 15, 17, 19}));
            expect(3 == p99(std::array{3, 3, 3, 3, 3}));
          }
        };

        "variance"_test = [] mutable {
          using std::literals::string_view_literals::operator""sv;
          expect("variance"sv == bench::stat::variance.name());
          expect(0 == bench::stat::variance(std::vector<int>{}));
          expect(0 == bench::stat::variance(std::array{5, 5, 5}));
          expect(std::fabs(.66 - bench::stat::variance(std::array{1, 2, 3})) <= .1);
        };

        "std_dev"_test = [] mutable {
          using std::literals::string_view_literals::operator""sv;
          expect("std_dev"sv == bench::stat::std_dev.name());
          expect(0 == bench::stat::std_dev(std::vector<int>{}));
          expect(0 == bench::stat::std_dev(std::array{5, 5, 5}));
          expect(std::fabs(.81 - bench::stat::std_dev(std::array{1, 2, 3})) <= .1);
        };

        "std_err"_test = [] mutable {
          using std::literals::string_view_literals::operator""sv;
          expect("std_err"sv == bench::stat::std_err.name());
          expect(0 == bench::stat::std_err(std::vector<int>{}));
          expect(0 == bench::stat::std_err(std::array{1, 1, 1}));
        };

        "median_absolute_err"_test = [] mutable {
          using std::literals::string_view_literals::operator""sv;
          expect("median_absolute_err"sv == bench::stat::median_absolute_err.name());
          expect(0 == bench::stat::median_absolute_err(std::vector<int>{}));
          expect(0 == bench::stat::median_absolute_err(std::array{2, 2, 2}));
        };

        "median_absolute_dev"_test = [] mutable {
          using std::literals::string_view_literals::operator""sv;
          expect("median_absolute_dev"sv == bench::stat::median_absolute_dev.name());
          expect(0 == bench::stat::median_absolute_dev(std::vector<int>{}));
          expect(1 == bench::stat::median_absolute_dev(std::array{1, 2, 3}));
        };

        "coefficient_of_varation"_test = [] mutable {
          using std::literals::string_view_literals::operator""sv;
          expect("coefficient_of_varation"sv == bench::stat::coefficient_of_varation.name());
          expect(0 == bench::stat::coefficient_of_varation(std::array{2, 2, 2}));
          expect(std::fabs(.4 - bench::stat::coefficient_of_varation(std::array{1, 2, 3})) <= .1);
        };

        "z_score"_test = [] mutable {
          using std::literals::string_view_literals::operator""sv;
          expect("z_score"sv == bench::stat::z_score.name());
          const auto z = bench::stat::z_score(std::array{1., 2., 3.});
          expect(z.size() == 3);
          expect(z[1] == 0); /// centered at mean
          expect(z.front() < 0);
          expect(z.back() > 0);
        };

        "t_score"_test = [] mutable {
          using std::literals::string_view_literals::operator""sv;
          expect("t_score"sv == bench::stat::t_score.name());
          const auto t = bench::stat::t_score(std::array{1., 2., 3.});
          expect(t.size() == 3);
          expect(t[1] == 0); /// centered at mean
          expect(t.front() < 0);
          expect(t.back() > 0);
        };
      };

      "report"_test = [] mutable {
        // empty results
      };

      "annotate"_test = [] mutable {
        // empty results
      };

      "json"_test = [] mutable {
      };

      "plot"_suite = [] {
        "math"_suite = [] {
          "lin_space"_test = [] mutable {
            expect(std::vector{1, 2} == bench::plot::math::lin_space(1, 2));
            expect(std::vector{1, 2, 3} == bench::plot::math::lin_space(1, 3));
            expect(std::vector{2, 3, 4, 5, 6, 7} == bench::plot::math::lin_space(2, 7));

            expect(std::vector{1, 2} == bench::plot::math::lin_space(1, 2, 2));
            expect(std::vector{2, 4} == bench::plot::math::lin_space(2, 4, 2));
            expect(std::vector{2, 4, 6, 8, 10} == bench::plot::math::lin_space(2, 10, 5));
            expect(std::vector{5, 10} == bench::plot::math::lin_space(5, 10, 2));
            expect(std::vector{5, 6, 7, 8, 9, 10} == bench::plot::math::lin_space(5, 10, 6));
          };

          "bin"_test = [] mutable {
            expect(std::vector{2, 1} == bench::plot::math::bin(std::vector{1, 2, 3}, 1, 3, 2));
            expect(std::vector{1, 1} == bench::plot::math::bin(std::vector{1, 2, 3}, 2, 3, 2));
            expect(std::vector{1, 1, 1} == bench::plot::math::bin(std::vector{1, 2, 3}, 1, 3, 3));
            expect(std::vector{3, 2} == bench::plot::math::bin(std::vector{1, 2, 3, 4, 5}, 2, 4, 2));
            expect(std::vector{1, 1, 1, 1, 1} == bench::plot::math::bin(std::vector{1, 2, 3, 4, 5}, 1, 5, 5));
          };
        };

        "hist"_test = [] mutable {
        };

        "bar"_test = [] mutable {
        };

        "box"_test = [] mutable {
        };

        "line"_test = [] mutable {
        };

        "ecdf"_test = [] mutable {
        };
      };
    };
  }
} // namespace perf::test
#endif // NTEST
#endif // PERF
